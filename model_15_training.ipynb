{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "## Paper Reading Analysis - Code Implementation\n",
    "### Model 15 Training, Hyperparameter Search and Evaluation\n",
    "### Jonathan Alcineus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 23:50:30.396334: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-30 23:50:30.424025: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756597830.447989   15779 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756597830.455143   15779 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756597830.471680   15779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756597830.471703   15779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756597830.471705   15779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756597830.471707   15779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-30 23:50:30.477075: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# These handle the file locations and importing the dataframe from the saved datafile from the authors files\n",
    "import os\n",
    "import pandas\n",
    "\n",
    "\n",
    "# These handle the image processing, editing, or displaying that needs to be performed\n",
    "import cv2 \n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "\n",
    "# These handle training the convolutional neural network (CNN) model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "import time\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/DNNorDermatologist\n"
     ]
    }
   ],
   "source": [
    "# This changes the home directory\n",
    "home_directory = os.path.expanduser(\"~\")\n",
    "os.chdir(home_directory)\n",
    "\n",
    "# Then goes to the folder where the data lies\n",
    "os.chdir('DNNorDermatologist')\n",
    "\n",
    "# Ensures that we are in the correct folder\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin to build the classifier and the ranges for each model to find the optimal parameters, or searching through hyperparameters\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "\n",
    "space = [Real(1e-6, 0.01, \"log-uniform\", name='learning_rate'),\n",
    "          Real(0.1, 0.8, name='dropout'),\n",
    "          Real(0.8, 1.0, name='momentum'),\n",
    "          Real(0.9, 1.0, name='beta_1'),\n",
    "          Real(0.99, 1.0, name='beta_2'),\n",
    "          Integer(low=5,high=20, name = 'epochs'),\n",
    "          Integer(low=50, high=225, name='num_dense_nodes'),\n",
    "          Categorical(categories=['SGD', 'Adam'],\n",
    "                             name='optimizer_type')\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The first part to implenment is the creation of random models\n",
    "if not os.path.isdir('suite_of_models'):\n",
    "    os.mkdir('suite_of_models')\n",
    "\n",
    "def make_a_model(learning_rate, dropout, momentum, beta_1, beta_2, num_dense_nodes, optimizer_type):\n",
    "    # Like in the paper the base model for the image classifcation will be imagenet\n",
    "    base_model = InceptionV3(weights='imagenet',input_shape=(224, 224, 3), include_top=False)\n",
    "\n",
    "    # Fine tune the model with extra dense layers\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(num_dense_nodes, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = Dropout(rate=dropout)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Selects a type of model optimizer\n",
    "    if optimizer_type == \"Adam\":\n",
    "        optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "    elif optimizer_type == \"SGD\":\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start off with basic parameters and the batch size for the models\n",
    "batch_size = 16\n",
    "best_accuracy = {} \n",
    "for seed in range(15):\n",
    "  best_accuracy[seed] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are currently training on seed: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "learning rate: 1.3e-06\n",
      "num_dense_nodes: 182\n",
      "dropout: 0.11272809400997633\n",
      "optimizer_type: Adam\n",
      "epochs: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756040474.101280   96331 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756040511.425493   97561 service.cc:152] XLA service 0x7fbdec001da0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1756040511.425747   97561 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-08-24 13:01:52.581746: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1756040518.244831   97561 cuda_dnn.cc:529] Loaded cuDNN version 91200\n",
      "2025-08-24 13:02:07.899159: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-08-24 13:02:08.046314: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-08-24 13:02:08.391521: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-08-24 13:02:08.534576: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/52\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:03:23\u001b[0m 75s/step - accuracy: 0.5000 - loss: 0.6876"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756040552.273846   97561 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m51/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.5006 - loss: 0.7066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 13:02:52.058518: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-08-24 13:02:52.204429: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-08-24 13:02:52.516017: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-08-24 13:02:52.658656: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 1s/step - accuracy: 0.5254 - loss: 0.7018 - val_accuracy: 0.5205 - val_loss: 0.7103\n",
      "Epoch 2/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.6123 - loss: 0.6662 - val_accuracy: 0.5930 - val_loss: 0.6737\n",
      "Epoch 3/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.6763 - loss: 0.6334 - val_accuracy: 0.6353 - val_loss: 0.6340\n",
      "Epoch 4/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.6993 - loss: 0.6030 - val_accuracy: 0.6787 - val_loss: 0.6096\n",
      "Epoch 5/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.7379 - loss: 0.5810 - val_accuracy: 0.7331 - val_loss: 0.5784\n",
      "Epoch 6/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.7488 - loss: 0.5657 - val_accuracy: 0.7729 - val_loss: 0.5486\n",
      "Epoch 7/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.7597 - loss: 0.5407 - val_accuracy: 0.7874 - val_loss: 0.5239\n",
      "Epoch 8/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 172ms/step - accuracy: 0.7838 - loss: 0.5101 - val_accuracy: 0.7947 - val_loss: 0.5069\n",
      "Epoch 9/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.7886 - loss: 0.4993 - val_accuracy: 0.8080 - val_loss: 0.4896\n",
      "Epoch 10/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.8019 - loss: 0.4837 - val_accuracy: 0.8116 - val_loss: 0.4748\n",
      "Epoch 11/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.8309 - loss: 0.4606 - val_accuracy: 0.8128 - val_loss: 0.4613\n",
      "Epoch 12/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.8261 - loss: 0.4525 - val_accuracy: 0.8188 - val_loss: 0.4493\n",
      "Epoch 13/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.8357 - loss: 0.4267 - val_accuracy: 0.8285 - val_loss: 0.4387\n",
      "Epoch 14/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.8285 - loss: 0.4209 - val_accuracy: 0.8309 - val_loss: 0.4284\n",
      "Epoch 15/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.8382 - loss: 0.3955 - val_accuracy: 0.8370 - val_loss: 0.4188\n",
      "Epoch 16/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.8635 - loss: 0.3784 - val_accuracy: 0.8321 - val_loss: 0.4094\n",
      "Epoch 17/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 156ms/step - accuracy: 0.8575 - loss: 0.3717 - val_accuracy: 0.8321 - val_loss: 0.4018\n",
      "Epoch 18/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.8611 - loss: 0.3680 - val_accuracy: 0.8333 - val_loss: 0.3957\n",
      "\n",
      "Accuracy: 83.33%\n",
      "\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 279.0816\n",
      "Function value obtained: -0.8333\n",
      "Current minimum: -0.8333\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "learning rate: 2.3e-05\n",
      "num_dense_nodes: 56\n",
      "dropout: 0.5672505413696396\n",
      "optimizer_type: Adam\n",
      "epochs: 13\n",
      "Epoch 1/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 965ms/step - accuracy: 0.5990 - loss: 0.6862 - val_accuracy: 0.6184 - val_loss: 0.6197\n",
      "Epoch 2/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.7560 - loss: 0.5092 - val_accuracy: 0.6522 - val_loss: 0.6288\n",
      "Epoch 3/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.8382 - loss: 0.3820 - val_accuracy: 0.7983 - val_loss: 0.4270\n",
      "Epoch 4/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.8780 - loss: 0.3115 - val_accuracy: 0.8261 - val_loss: 0.3940\n",
      "Epoch 5/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 157ms/step - accuracy: 0.9094 - loss: 0.2463 - val_accuracy: 0.8514 - val_loss: 0.3419\n",
      "Epoch 6/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9529 - loss: 0.1731 - val_accuracy: 0.8671 - val_loss: 0.3226\n",
      "Epoch 7/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9577 - loss: 0.1307 - val_accuracy: 0.8502 - val_loss: 0.3337\n",
      "Epoch 8/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9746 - loss: 0.1011 - val_accuracy: 0.8454 - val_loss: 0.3782\n",
      "Epoch 9/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9783 - loss: 0.0885 - val_accuracy: 0.8708 - val_loss: 0.3704\n",
      "Epoch 10/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9783 - loss: 0.0851 - val_accuracy: 0.8792 - val_loss: 0.3500\n",
      "Epoch 11/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9867 - loss: 0.0492 - val_accuracy: 0.8575 - val_loss: 0.3998\n",
      "Epoch 12/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9867 - loss: 0.0485 - val_accuracy: 0.8659 - val_loss: 0.4260\n",
      "Epoch 13/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9795 - loss: 0.0751 - val_accuracy: 0.8623 - val_loss: 0.4227\n",
      "\n",
      "Accuracy: 86.23%\n",
      "\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 219.1980\n",
      "Function value obtained: -0.8623\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "learning rate: 1.2e-05\n",
      "num_dense_nodes: 201\n",
      "dropout: 0.4203732887405415\n",
      "optimizer_type: Adam\n",
      "epochs: 19\n",
      "Epoch 1/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 968ms/step - accuracy: 0.5604 - loss: 0.6935 - val_accuracy: 0.5797 - val_loss: 0.6838\n",
      "Epoch 2/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7512 - loss: 0.5413 - val_accuracy: 0.6800 - val_loss: 0.5574\n",
      "Epoch 3/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.8080 - loss: 0.4425 - val_accuracy: 0.7645 - val_loss: 0.4644\n",
      "Epoch 4/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.8478 - loss: 0.3683 - val_accuracy: 0.8104 - val_loss: 0.3996\n",
      "Epoch 5/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.8913 - loss: 0.2988 - val_accuracy: 0.8345 - val_loss: 0.3537\n",
      "Epoch 6/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.8901 - loss: 0.2605 - val_accuracy: 0.8527 - val_loss: 0.3313\n",
      "Epoch 7/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9408 - loss: 0.1995 - val_accuracy: 0.8575 - val_loss: 0.3204\n",
      "Epoch 8/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9396 - loss: 0.1697 - val_accuracy: 0.8623 - val_loss: 0.3201\n",
      "Epoch 9/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9517 - loss: 0.1464 - val_accuracy: 0.8623 - val_loss: 0.3174\n",
      "Epoch 10/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9710 - loss: 0.0964 - val_accuracy: 0.8635 - val_loss: 0.3258\n",
      "Epoch 11/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9807 - loss: 0.0961 - val_accuracy: 0.8671 - val_loss: 0.3262\n",
      "Epoch 12/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 173ms/step - accuracy: 0.9783 - loss: 0.0674 - val_accuracy: 0.8623 - val_loss: 0.3468\n",
      "Epoch 13/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9710 - loss: 0.0903 - val_accuracy: 0.8611 - val_loss: 0.3544\n",
      "Epoch 14/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9952 - loss: 0.0362 - val_accuracy: 0.8623 - val_loss: 0.3459\n",
      "Epoch 15/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9940 - loss: 0.0342 - val_accuracy: 0.8647 - val_loss: 0.3540\n",
      "Epoch 16/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9891 - loss: 0.0370 - val_accuracy: 0.8635 - val_loss: 0.3698\n",
      "Epoch 17/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9940 - loss: 0.0265 - val_accuracy: 0.8599 - val_loss: 0.3776\n",
      "Epoch 18/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9952 - loss: 0.0312 - val_accuracy: 0.8587 - val_loss: 0.4007\n",
      "Epoch 19/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9915 - loss: 0.0280 - val_accuracy: 0.8563 - val_loss: 0.4243\n",
      "\n",
      "Accuracy: 85.63%\n",
      "\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 269.9277\n",
      "Function value obtained: -0.8563\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "learning rate: 7.5e-03\n",
      "num_dense_nodes: 214\n",
      "dropout: 0.6735447856191022\n",
      "optimizer_type: SGD\n",
      "epochs: 8\n",
      "Epoch 1/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 849ms/step - accuracy: 0.6920 - loss: 0.5979 - val_accuracy: 0.5000 - val_loss: 3292.1758\n",
      "Epoch 2/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.7379 - loss: 0.5371 - val_accuracy: 0.5000 - val_loss: 569.0084\n",
      "Epoch 3/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.7742 - loss: 0.4771 - val_accuracy: 0.5338 - val_loss: 43.6050\n",
      "Epoch 4/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.7717 - loss: 0.4384 - val_accuracy: 0.5060 - val_loss: 766.6684\n",
      "Epoch 5/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 148ms/step - accuracy: 0.7850 - loss: 0.4516 - val_accuracy: 0.6123 - val_loss: 12.9566\n",
      "Epoch 6/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 148ms/step - accuracy: 0.7862 - loss: 0.5485 - val_accuracy: 0.8430 - val_loss: 0.4893\n",
      "Epoch 7/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.7971 - loss: 0.4363 - val_accuracy: 0.8225 - val_loss: 1.2880\n",
      "Epoch 8/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8019 - loss: 0.4457 - val_accuracy: 0.8116 - val_loss: 0.8758\n",
      "\n",
      "Accuracy: 81.16%\n",
      "\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 154.1860\n",
      "Function value obtained: -0.8116\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "learning rate: 2.0e-04\n",
      "num_dense_nodes: 200\n",
      "dropout: 0.7047085519380893\n",
      "optimizer_type: SGD\n",
      "epochs: 8\n",
      "Epoch 1/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 853ms/step - accuracy: 0.5688 - loss: 0.7371 - val_accuracy: 0.5314 - val_loss: 0.7072\n",
      "Epoch 2/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.6341 - loss: 0.6650 - val_accuracy: 0.6304 - val_loss: 0.6303\n",
      "Epoch 3/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.6932 - loss: 0.6052 - val_accuracy: 0.7307 - val_loss: 0.5567\n",
      "Epoch 4/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.7331 - loss: 0.5481 - val_accuracy: 0.7621 - val_loss: 0.5096\n",
      "Epoch 5/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.7415 - loss: 0.4971 - val_accuracy: 0.7983 - val_loss: 0.4632\n",
      "Epoch 6/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.7766 - loss: 0.4652 - val_accuracy: 0.8152 - val_loss: 0.4205\n",
      "Epoch 7/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8043 - loss: 0.4119 - val_accuracy: 0.8285 - val_loss: 0.4022\n",
      "Epoch 8/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.8261 - loss: 0.4008 - val_accuracy: 0.8406 - val_loss: 0.3772\n",
      "\n",
      "Accuracy: 84.06%\n",
      "\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 150.1687\n",
      "Function value obtained: -0.8406\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 6 started. Searching for the next optimal point.\n",
      "learning rate: 5.0e-04\n",
      "num_dense_nodes: 174\n",
      "dropout: 0.14217438220137169\n",
      "optimizer_type: Adam\n",
      "epochs: 20\n",
      "Epoch 1/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 980ms/step - accuracy: 0.7415 - loss: 0.5504 - val_accuracy: 0.7681 - val_loss: 2.9926\n",
      "Epoch 2/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 172ms/step - accuracy: 0.8478 - loss: 0.3714 - val_accuracy: 0.7911 - val_loss: 1.6978\n",
      "Epoch 3/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.8575 - loss: 0.3048 - val_accuracy: 0.7862 - val_loss: 0.8176\n",
      "Epoch 4/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.8756 - loss: 0.3054 - val_accuracy: 0.8237 - val_loss: 0.7920\n",
      "Epoch 5/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9336 - loss: 0.2052 - val_accuracy: 0.7971 - val_loss: 0.5086\n",
      "Epoch 6/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9106 - loss: 0.2632 - val_accuracy: 0.8527 - val_loss: 0.4624\n",
      "Epoch 7/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 173ms/step - accuracy: 0.9457 - loss: 0.1485 - val_accuracy: 0.8092 - val_loss: 0.4368\n",
      "Epoch 8/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9457 - loss: 0.1552 - val_accuracy: 0.7657 - val_loss: 0.8498\n",
      "Epoch 9/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 158ms/step - accuracy: 0.9046 - loss: 0.2249 - val_accuracy: 0.8249 - val_loss: 0.7790\n",
      "Epoch 10/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9287 - loss: 0.1849 - val_accuracy: 0.7572 - val_loss: 0.8126\n",
      "Epoch 11/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9408 - loss: 0.1314 - val_accuracy: 0.7500 - val_loss: 1.5114\n",
      "Epoch 12/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 173ms/step - accuracy: 0.9577 - loss: 0.1377 - val_accuracy: 0.8140 - val_loss: 0.6210\n",
      "Epoch 13/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9553 - loss: 0.1232 - val_accuracy: 0.8418 - val_loss: 0.4821\n",
      "Epoch 14/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9771 - loss: 0.0700 - val_accuracy: 0.8043 - val_loss: 0.6928\n",
      "Epoch 15/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9807 - loss: 0.0698 - val_accuracy: 0.6377 - val_loss: 1.8781\n",
      "Epoch 16/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9710 - loss: 0.0967 - val_accuracy: 0.8357 - val_loss: 0.5221\n",
      "Epoch 17/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9457 - loss: 0.1293 - val_accuracy: 0.8635 - val_loss: 0.6302\n",
      "Epoch 18/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9565 - loss: 0.1502 - val_accuracy: 0.7295 - val_loss: 1.1761\n",
      "Epoch 19/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9722 - loss: 0.0875 - val_accuracy: 0.8490 - val_loss: 0.4461\n",
      "Epoch 20/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9771 - loss: 0.0804 - val_accuracy: 0.8200 - val_loss: 0.5621\n",
      "\n",
      "Accuracy: 82.00%\n",
      "\n",
      "Iteration No: 6 ended. Search finished for the next optimal point.\n",
      "Time taken: 279.8844\n",
      "Function value obtained: -0.8200\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 7 started. Searching for the next optimal point.\n",
      "learning rate: 1.0e-02\n",
      "num_dense_nodes: 225\n",
      "dropout: 0.8\n",
      "optimizer_type: SGD\n",
      "epochs: 5\n",
      "Epoch 1/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 881ms/step - accuracy: 0.7017 - loss: 0.6001 - val_accuracy: 0.5121 - val_loss: 1.1284\n",
      "Epoch 2/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8128 - loss: 0.4192 - val_accuracy: 0.6510 - val_loss: 0.7343\n",
      "Epoch 3/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8249 - loss: 0.4022 - val_accuracy: 0.7150 - val_loss: 0.6082\n",
      "Epoch 4/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.8623 - loss: 0.3272 - val_accuracy: 0.8357 - val_loss: 0.3495\n",
      "Epoch 5/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8853 - loss: 0.3001 - val_accuracy: 0.7669 - val_loss: 0.4780\n",
      "\n",
      "Accuracy: 76.69%\n",
      "\n",
      "Iteration No: 7 ended. Search finished for the next optimal point.\n",
      "Time taken: 129.8905\n",
      "Function value obtained: -0.7669\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 8 started. Searching for the next optimal point.\n",
      "learning rate: 1.3e-05\n",
      "num_dense_nodes: 207\n",
      "dropout: 0.7780604432007558\n",
      "optimizer_type: Adam\n",
      "epochs: 6\n",
      "Epoch 1/6\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 979ms/step - accuracy: 0.5423 - loss: 0.8627 - val_accuracy: 0.5000 - val_loss: 0.8744\n",
      "Epoch 2/6\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.4928 - loss: 0.9123 - val_accuracy: 0.5000 - val_loss: 0.8344\n",
      "Epoch 3/6\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 158ms/step - accuracy: 0.5217 - loss: 0.8748 - val_accuracy: 0.5000 - val_loss: 0.8180\n",
      "Epoch 4/6\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.5169 - loss: 0.9173 - val_accuracy: 0.5024 - val_loss: 0.8038\n",
      "Epoch 5/6\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.5483 - loss: 0.8900 - val_accuracy: 0.5012 - val_loss: 0.7995\n",
      "Epoch 6/6\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.5193 - loss: 0.8643 - val_accuracy: 0.5024 - val_loss: 0.7948\n",
      "\n",
      "Accuracy: 50.24%\n",
      "\n",
      "Iteration No: 8 ended. Search finished for the next optimal point.\n",
      "Time taken: 163.2632\n",
      "Function value obtained: -0.5024\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 9 started. Searching for the next optimal point.\n",
      "learning rate: 2.6e-05\n",
      "num_dense_nodes: 124\n",
      "dropout: 0.6704920945823173\n",
      "optimizer_type: SGD\n",
      "epochs: 10\n",
      "Epoch 1/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 849ms/step - accuracy: 0.4577 - loss: 0.8191 - val_accuracy: 0.4601 - val_loss: 0.7436\n",
      "Epoch 2/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5000 - loss: 0.7811 - val_accuracy: 0.4807 - val_loss: 0.7251\n",
      "Epoch 3/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5193 - loss: 0.7687 - val_accuracy: 0.5386 - val_loss: 0.7008\n",
      "Epoch 4/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5338 - loss: 0.7470 - val_accuracy: 0.5519 - val_loss: 0.6860\n",
      "Epoch 5/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5205 - loss: 0.7440 - val_accuracy: 0.5531 - val_loss: 0.6750\n",
      "Epoch 6/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5713 - loss: 0.7048 - val_accuracy: 0.5930 - val_loss: 0.6600\n",
      "Epoch 7/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.5676 - loss: 0.7099 - val_accuracy: 0.6341 - val_loss: 0.6484\n",
      "Epoch 8/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.5785 - loss: 0.6811 - val_accuracy: 0.6413 - val_loss: 0.6384\n",
      "Epoch 9/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 150ms/step - accuracy: 0.6051 - loss: 0.6626 - val_accuracy: 0.6558 - val_loss: 0.6290\n",
      "Epoch 10/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 150ms/step - accuracy: 0.6075 - loss: 0.6613 - val_accuracy: 0.6727 - val_loss: 0.6189\n",
      "\n",
      "Accuracy: 67.27%\n",
      "\n",
      "Iteration No: 9 ended. Search finished for the next optimal point.\n",
      "Time taken: 172.2279\n",
      "Function value obtained: -0.6727\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 10 started. Searching for the next optimal point.\n",
      "learning rate: 5.0e-06\n",
      "num_dense_nodes: 50\n",
      "dropout: 0.700663783144861\n",
      "optimizer_type: Adam\n",
      "epochs: 20\n",
      "Epoch 1/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 983ms/step - accuracy: 0.5133 - loss: 0.7301 - val_accuracy: 0.5580 - val_loss: 0.6878\n",
      "Epoch 2/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 158ms/step - accuracy: 0.5749 - loss: 0.6684 - val_accuracy: 0.6486 - val_loss: 0.6429\n",
      "Epoch 3/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.6486 - loss: 0.6227 - val_accuracy: 0.7331 - val_loss: 0.6028\n",
      "Epoch 4/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.6534 - loss: 0.6130 - val_accuracy: 0.7886 - val_loss: 0.5728\n",
      "Epoch 5/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.6993 - loss: 0.5792 - val_accuracy: 0.8140 - val_loss: 0.5431\n",
      "Epoch 6/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 174ms/step - accuracy: 0.7693 - loss: 0.5291 - val_accuracy: 0.8249 - val_loss: 0.5197\n",
      "Epoch 7/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.7729 - loss: 0.5200 - val_accuracy: 0.8370 - val_loss: 0.4933\n",
      "Epoch 8/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.7621 - loss: 0.5007 - val_accuracy: 0.8382 - val_loss: 0.4707\n",
      "Epoch 9/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 159ms/step - accuracy: 0.7911 - loss: 0.4723 - val_accuracy: 0.8490 - val_loss: 0.4490\n",
      "Epoch 10/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.7959 - loss: 0.4685 - val_accuracy: 0.8502 - val_loss: 0.4305\n",
      "Epoch 11/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 175ms/step - accuracy: 0.8188 - loss: 0.4415 - val_accuracy: 0.8514 - val_loss: 0.4148\n",
      "Epoch 12/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.8345 - loss: 0.4204 - val_accuracy: 0.8551 - val_loss: 0.4009\n",
      "Epoch 13/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8297 - loss: 0.4045 - val_accuracy: 0.8551 - val_loss: 0.3890\n",
      "Epoch 14/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 159ms/step - accuracy: 0.8684 - loss: 0.3763 - val_accuracy: 0.8563 - val_loss: 0.3782\n",
      "Epoch 15/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8490 - loss: 0.3683 - val_accuracy: 0.8551 - val_loss: 0.3697\n",
      "Epoch 16/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8696 - loss: 0.3365 - val_accuracy: 0.8539 - val_loss: 0.3627\n",
      "Epoch 17/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8780 - loss: 0.3229 - val_accuracy: 0.8551 - val_loss: 0.3568\n",
      "Epoch 18/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8744 - loss: 0.3239 - val_accuracy: 0.8539 - val_loss: 0.3512\n",
      "Epoch 19/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9143 - loss: 0.2707 - val_accuracy: 0.8539 - val_loss: 0.3481\n",
      "Epoch 20/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9070 - loss: 0.2798 - val_accuracy: 0.8539 - val_loss: 0.3448\n",
      "\n",
      "Accuracy: 85.39%\n",
      "\n",
      "Iteration No: 10 ended. Search finished for the next optimal point.\n",
      "Time taken: 285.1320\n",
      "Function value obtained: -0.8539\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "learning rate: 2.2e-04\n",
      "num_dense_nodes: 99\n",
      "dropout: 0.8\n",
      "optimizer_type: Adam\n",
      "epochs: 15\n",
      "Epoch 1/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 969ms/step - accuracy: 0.7295 - loss: 0.5579 - val_accuracy: 0.7645 - val_loss: 0.4698\n",
      "Epoch 2/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.8406 - loss: 0.4063 - val_accuracy: 0.8454 - val_loss: 0.4426\n",
      "Epoch 3/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 159ms/step - accuracy: 0.8659 - loss: 0.3350 - val_accuracy: 0.7923 - val_loss: 0.6667\n",
      "Epoch 4/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8877 - loss: 0.2942 - val_accuracy: 0.6812 - val_loss: 3.4684\n",
      "Epoch 5/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 159ms/step - accuracy: 0.9191 - loss: 0.2294 - val_accuracy: 0.8188 - val_loss: 0.7688\n",
      "Epoch 6/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9626 - loss: 0.1315 - val_accuracy: 0.8502 - val_loss: 0.6711\n",
      "Epoch 7/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9227 - loss: 0.2638 - val_accuracy: 0.7645 - val_loss: 2.6632\n",
      "Epoch 8/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 175ms/step - accuracy: 0.9529 - loss: 0.1585 - val_accuracy: 0.8382 - val_loss: 0.5658\n",
      "Epoch 9/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9638 - loss: 0.1018 - val_accuracy: 0.8635 - val_loss: 0.6777\n",
      "Epoch 10/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9638 - loss: 0.1192 - val_accuracy: 0.8490 - val_loss: 0.6311\n",
      "Epoch 11/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9541 - loss: 0.1402 - val_accuracy: 0.8575 - val_loss: 0.6309\n",
      "Epoch 12/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9771 - loss: 0.0876 - val_accuracy: 0.8768 - val_loss: 0.7414\n",
      "Epoch 13/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9831 - loss: 0.0604 - val_accuracy: 0.8188 - val_loss: 0.6728\n",
      "Epoch 14/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9686 - loss: 0.0897 - val_accuracy: 0.8696 - val_loss: 0.7441\n",
      "Epoch 15/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9891 - loss: 0.0466 - val_accuracy: 0.8575 - val_loss: 0.4973\n",
      "\n",
      "Accuracy: 85.75%\n",
      "\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 242.1087\n",
      "Function value obtained: -0.8575\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "learning rate: 3.7e-06\n",
      "num_dense_nodes: 98\n",
      "dropout: 0.8\n",
      "optimizer_type: Adam\n",
      "epochs: 20\n",
      "Epoch 1/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 982ms/step - accuracy: 0.5133 - loss: 0.8345 - val_accuracy: 0.5000 - val_loss: 0.8047\n",
      "Epoch 2/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.5157 - loss: 0.7874 - val_accuracy: 0.5193 - val_loss: 0.7216\n",
      "Epoch 3/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.5495 - loss: 0.7444 - val_accuracy: 0.5870 - val_loss: 0.6598\n",
      "Epoch 4/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.5592 - loss: 0.7315 - val_accuracy: 0.6582 - val_loss: 0.6232\n",
      "Epoch 5/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.6075 - loss: 0.6695 - val_accuracy: 0.7126 - val_loss: 0.5957\n",
      "Epoch 6/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.5725 - loss: 0.6908 - val_accuracy: 0.7609 - val_loss: 0.5712\n",
      "Epoch 7/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.6304 - loss: 0.6300 - val_accuracy: 0.7874 - val_loss: 0.5548\n",
      "Epoch 8/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.6667 - loss: 0.6007 - val_accuracy: 0.8007 - val_loss: 0.5369\n",
      "Epoch 9/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.6800 - loss: 0.5934 - val_accuracy: 0.8116 - val_loss: 0.5198\n",
      "Epoch 10/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7222 - loss: 0.5424 - val_accuracy: 0.8200 - val_loss: 0.5033\n",
      "Epoch 11/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7283 - loss: 0.5372 - val_accuracy: 0.8273 - val_loss: 0.4879\n",
      "Epoch 12/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7452 - loss: 0.5209 - val_accuracy: 0.8357 - val_loss: 0.4762\n",
      "Epoch 13/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7355 - loss: 0.5268 - val_accuracy: 0.8345 - val_loss: 0.4628\n",
      "Epoch 14/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7488 - loss: 0.5138 - val_accuracy: 0.8345 - val_loss: 0.4509\n",
      "Epoch 15/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7705 - loss: 0.4895 - val_accuracy: 0.8394 - val_loss: 0.4396\n",
      "Epoch 16/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7705 - loss: 0.4821 - val_accuracy: 0.8418 - val_loss: 0.4272\n",
      "Epoch 17/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7874 - loss: 0.4501 - val_accuracy: 0.8430 - val_loss: 0.4163\n",
      "Epoch 18/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8043 - loss: 0.4537 - val_accuracy: 0.8442 - val_loss: 0.4077\n",
      "Epoch 19/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 158ms/step - accuracy: 0.8285 - loss: 0.4202 - val_accuracy: 0.8454 - val_loss: 0.3991\n",
      "Epoch 20/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.8285 - loss: 0.4142 - val_accuracy: 0.8502 - val_loss: 0.3888\n",
      "\n",
      "Accuracy: 85.02%\n",
      "\n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 279.1068\n",
      "Function value obtained: -0.8502\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "learning rate: 1.0e-06\n",
      "num_dense_nodes: 50\n",
      "dropout: 0.3847823703407012\n",
      "optimizer_type: SGD\n",
      "epochs: 7\n",
      "Epoch 1/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 787ms/step - accuracy: 0.4879 - loss: 0.7576 - val_accuracy: 0.5797 - val_loss: 0.6807\n",
      "Epoch 2/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.4940 - loss: 0.7342 - val_accuracy: 0.5628 - val_loss: 0.6902\n",
      "Epoch 3/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5229 - loss: 0.7310 - val_accuracy: 0.5568 - val_loss: 0.6976\n",
      "Epoch 4/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.5133 - loss: 0.7401 - val_accuracy: 0.5519 - val_loss: 0.6946\n",
      "Epoch 5/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.5242 - loss: 0.7221 - val_accuracy: 0.5447 - val_loss: 0.6962\n",
      "Epoch 6/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.5350 - loss: 0.7202 - val_accuracy: 0.5531 - val_loss: 0.6961\n",
      "Epoch 7/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4928 - loss: 0.7353 - val_accuracy: 0.5483 - val_loss: 0.6949\n",
      "\n",
      "Accuracy: 54.83%\n",
      "\n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 139.7238\n",
      "Function value obtained: -0.5483\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "learning rate: 1.0e-06\n",
      "num_dense_nodes: 225\n",
      "dropout: 0.19917452021136267\n",
      "optimizer_type: SGD\n",
      "epochs: 20\n",
      "Epoch 1/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 897ms/step - accuracy: 0.4638 - loss: 0.7836 - val_accuracy: 0.4903 - val_loss: 0.9106\n",
      "Epoch 2/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4964 - loss: 0.7694 - val_accuracy: 0.4879 - val_loss: 0.8543\n",
      "Epoch 3/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.4952 - loss: 0.7798 - val_accuracy: 0.4734 - val_loss: 0.8196\n",
      "Epoch 4/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.5024 - loss: 0.7811 - val_accuracy: 0.4915 - val_loss: 0.7929\n",
      "Epoch 5/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.4626 - loss: 0.7923 - val_accuracy: 0.4771 - val_loss: 0.7795\n",
      "Epoch 6/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4879 - loss: 0.7781 - val_accuracy: 0.4867 - val_loss: 0.7763\n",
      "Epoch 7/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4915 - loss: 0.7611 - val_accuracy: 0.4807 - val_loss: 0.7760\n",
      "Epoch 8/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4915 - loss: 0.7855 - val_accuracy: 0.4783 - val_loss: 0.7780\n",
      "Epoch 9/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4783 - loss: 0.7761 - val_accuracy: 0.4710 - val_loss: 0.7785\n",
      "Epoch 10/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4988 - loss: 0.7653 - val_accuracy: 0.4831 - val_loss: 0.7777\n",
      "Epoch 11/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4831 - loss: 0.7739 - val_accuracy: 0.4831 - val_loss: 0.7754\n",
      "Epoch 12/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4662 - loss: 0.7697 - val_accuracy: 0.4819 - val_loss: 0.7744\n",
      "Epoch 13/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4879 - loss: 0.7722 - val_accuracy: 0.4831 - val_loss: 0.7733\n",
      "Epoch 14/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4698 - loss: 0.7728 - val_accuracy: 0.4843 - val_loss: 0.7713\n",
      "Epoch 15/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4795 - loss: 0.7644 - val_accuracy: 0.4831 - val_loss: 0.7704\n",
      "Epoch 16/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4795 - loss: 0.7635 - val_accuracy: 0.4843 - val_loss: 0.7685\n",
      "Epoch 17/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4758 - loss: 0.7620 - val_accuracy: 0.4843 - val_loss: 0.7665\n",
      "Epoch 18/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4940 - loss: 0.7577 - val_accuracy: 0.4819 - val_loss: 0.7657\n",
      "Epoch 19/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4964 - loss: 0.7672 - val_accuracy: 0.4855 - val_loss: 0.7640\n",
      "Epoch 20/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4831 - loss: 0.7721 - val_accuracy: 0.4843 - val_loss: 0.7628\n",
      "\n",
      "Accuracy: 48.43%\n",
      "\n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 248.6040\n",
      "Function value obtained: -0.4843\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "learning rate: 2.5e-05\n",
      "num_dense_nodes: 50\n",
      "dropout: 0.5818496379584241\n",
      "optimizer_type: Adam\n",
      "epochs: 15\n",
      "Epoch 1/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 1s/step - accuracy: 0.5036 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 2/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 3/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 4/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 5/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 6/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 7/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 8/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 9/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 10/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 11/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 12/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 13/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 14/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 152ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 15/15\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 152ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "\n",
      "Accuracy: 50.00%\n",
      "\n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 236.0725\n",
      "Function value obtained: -0.5000\n",
      "Current minimum: -0.8623\n",
      "Seed:  14\n",
      "BEST ACCURACY:  {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.8623188138008118}\n",
      "hyper_params  [2.302088642714976e-05, 0.5672505413696396, 0.8035744783088253, 0.9157539746422093, 0.9932312130401708, np.int64(13), np.int64(56), 'Adam']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training on seed 0 for this cell\n",
    "\n",
    "seed = 14\n",
    "\n",
    "print('We are currently training on seed:', seed) \n",
    "# for each iteration of the hyperparameter search, return a set of parameters\n",
    "# and feed them into the relevant parts\n",
    "# run training of the model for this seed, save with seed num\n",
    "X_train = np.load(f'paper_reading_small_data/trial_{seed}_X_train.npy', allow_pickle=True)\n",
    "y_train = np.load(f'paper_reading_small_data/trial_{seed}_y_train.npy', allow_pickle=True)\n",
    "X_test = np.load(f'paper_reading_small_data/trial_{seed}_X_test.npy', allow_pickle=True)\n",
    "y_test = np.load(f'paper_reading_small_data/trial_{seed}_y_test.npy', allow_pickle=True)\n",
    "\n",
    "path_best_model = 'inception_saved_trial_{}.keras'.format(seed)\n",
    "  \n",
    "@use_named_args(dimensions=space)\n",
    "def fitness(learning_rate, dropout, momentum, beta_1, beta_2,\n",
    "              num_dense_nodes, optimizer_type, epochs):\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('dropout:', dropout)\n",
    "    print('optimizer_type:', optimizer_type)\n",
    "    print('epochs:', epochs)\n",
    "\n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = make_a_model(learning_rate=learning_rate, \n",
    "                         dropout=dropout, \n",
    "                         momentum=momentum, \n",
    "                         beta_1=beta_1, beta_2=beta_2,\n",
    "                         num_dense_nodes=num_dense_nodes, \n",
    "                         optimizer_type=optimizer_type)\n",
    "\n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(x=X_train,\n",
    "                          y=y_train,\n",
    "                          epochs=epochs,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data= (X_test,y_test))\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    # auc_val = history.history['val_auc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_accuracy\n",
    "\n",
    "    if accuracy > best_accuracy[seed]:\n",
    "      # Save the new model to harddisk in the recommended Keras format\n",
    "      model_path = os.path.join('DataSplitted', path_best_model)\n",
    "      model.save(model_path)\n",
    "    \n",
    "\n",
    "      # Update the classification accuracy.\n",
    "      best_accuracy[seed] = accuracy\n",
    "      # best_auc = auc_val\n",
    "          \n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "\n",
    "    import gc\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    try:\n",
    "      tf.config.experimental.reset_memory_stats(\"GPU:0\")\n",
    "    except:\n",
    "      pass  # in case older TF version\n",
    "    return -accuracy\n",
    "\n",
    "  \n",
    "#This conducts the hyperparameter search over each data split for details see: https://scikit-optimize.github.io/#skopt.gp_minimize\n",
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=space,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=15,\n",
    "\t\t\t    n_random_starts = 5,\n",
    "                            verbose = True)\n",
    "print('Seed: ',seed)\n",
    "print(\"BEST ACCURACY: \", best_accuracy)\n",
    "print('hyper_params ', search_result.x)\n",
    "\n",
    "del X_train, y_train, X_test, y_test \n",
    "\n",
    "import gc\n",
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# GradCAM and Kernel SHAP Experiments\n",
    "os.chdir(home_directory)\n",
    "os.chdir('paper-reading-analysis')\n",
    "\n",
    "# Library with the methods that I needed\n",
    "import gradcam_shap\n",
    "import scipy\n",
    "\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "from tf_keras_vis.utils.scores import CategoricalScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756597854.258156   15779 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "os.chdir(home_directory)\n",
    "os.chdir('DNNorDermatologist')\n",
    "os.chdir('DataSplitted')\n",
    "seed = 14\n",
    "model = load_model(f'inception_saved_trial_{seed}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<InputLayer name=input_layer, built=True>, <Conv2D name=conv2d, built=True>, <BatchNormalization name=batch_normalization, built=True>, <Activation name=activation, built=True>, <Conv2D name=conv2d_1, built=True>, <BatchNormalization name=batch_normalization_1, built=True>, <Activation name=activation_1, built=True>, <Conv2D name=conv2d_2, built=True>, <BatchNormalization name=batch_normalization_2, built=True>, <Activation name=activation_2, built=True>, <MaxPooling2D name=max_pooling2d, built=True>, <Conv2D name=conv2d_3, built=True>, <BatchNormalization name=batch_normalization_3, built=True>, <Activation name=activation_3, built=True>, <Conv2D name=conv2d_4, built=True>, <BatchNormalization name=batch_normalization_4, built=True>, <Activation name=activation_4, built=True>, <MaxPooling2D name=max_pooling2d_1, built=True>, <Conv2D name=conv2d_8, built=True>, <BatchNormalization name=batch_normalization_8, built=True>, <Activation name=activation_8, built=True>, <Conv2D name=conv2d_6, built=True>, <Conv2D name=conv2d_9, built=True>, <BatchNormalization name=batch_normalization_6, built=True>, <BatchNormalization name=batch_normalization_9, built=True>, <Activation name=activation_6, built=True>, <Activation name=activation_9, built=True>, <AveragePooling2D name=average_pooling2d, built=True>, <Conv2D name=conv2d_5, built=True>, <Conv2D name=conv2d_7, built=True>, <Conv2D name=conv2d_10, built=True>, <Conv2D name=conv2d_11, built=True>, <BatchNormalization name=batch_normalization_5, built=True>, <BatchNormalization name=batch_normalization_7, built=True>, <BatchNormalization name=batch_normalization_10, built=True>, <BatchNormalization name=batch_normalization_11, built=True>, <Activation name=activation_5, built=True>, <Activation name=activation_7, built=True>, <Activation name=activation_10, built=True>, <Activation name=activation_11, built=True>, <Concatenate name=mixed0, built=True>, <Conv2D name=conv2d_15, built=True>, <BatchNormalization name=batch_normalization_15, built=True>, <Activation name=activation_15, built=True>, <Conv2D name=conv2d_13, built=True>, <Conv2D name=conv2d_16, built=True>, <BatchNormalization name=batch_normalization_13, built=True>, <BatchNormalization name=batch_normalization_16, built=True>, <Activation name=activation_13, built=True>, <Activation name=activation_16, built=True>, <AveragePooling2D name=average_pooling2d_1, built=True>, <Conv2D name=conv2d_12, built=True>, <Conv2D name=conv2d_14, built=True>, <Conv2D name=conv2d_17, built=True>, <Conv2D name=conv2d_18, built=True>, <BatchNormalization name=batch_normalization_12, built=True>, <BatchNormalization name=batch_normalization_14, built=True>, <BatchNormalization name=batch_normalization_17, built=True>, <BatchNormalization name=batch_normalization_18, built=True>, <Activation name=activation_12, built=True>, <Activation name=activation_14, built=True>, <Activation name=activation_17, built=True>, <Activation name=activation_18, built=True>, <Concatenate name=mixed1, built=True>, <Conv2D name=conv2d_22, built=True>, <BatchNormalization name=batch_normalization_22, built=True>, <Activation name=activation_22, built=True>, <Conv2D name=conv2d_20, built=True>, <Conv2D name=conv2d_23, built=True>, <BatchNormalization name=batch_normalization_20, built=True>, <BatchNormalization name=batch_normalization_23, built=True>, <Activation name=activation_20, built=True>, <Activation name=activation_23, built=True>, <AveragePooling2D name=average_pooling2d_2, built=True>, <Conv2D name=conv2d_19, built=True>, <Conv2D name=conv2d_21, built=True>, <Conv2D name=conv2d_24, built=True>, <Conv2D name=conv2d_25, built=True>, <BatchNormalization name=batch_normalization_19, built=True>, <BatchNormalization name=batch_normalization_21, built=True>, <BatchNormalization name=batch_normalization_24, built=True>, <BatchNormalization name=batch_normalization_25, built=True>, <Activation name=activation_19, built=True>, <Activation name=activation_21, built=True>, <Activation name=activation_24, built=True>, <Activation name=activation_25, built=True>, <Concatenate name=mixed2, built=True>, <Conv2D name=conv2d_27, built=True>, <BatchNormalization name=batch_normalization_27, built=True>, <Activation name=activation_27, built=True>, <Conv2D name=conv2d_28, built=True>, <BatchNormalization name=batch_normalization_28, built=True>, <Activation name=activation_28, built=True>, <Conv2D name=conv2d_26, built=True>, <Conv2D name=conv2d_29, built=True>, <BatchNormalization name=batch_normalization_26, built=True>, <BatchNormalization name=batch_normalization_29, built=True>, <Activation name=activation_26, built=True>, <Activation name=activation_29, built=True>, <MaxPooling2D name=max_pooling2d_2, built=True>, <Concatenate name=mixed3, built=True>, <Conv2D name=conv2d_34, built=True>, <BatchNormalization name=batch_normalization_34, built=True>, <Activation name=activation_34, built=True>, <Conv2D name=conv2d_35, built=True>, <BatchNormalization name=batch_normalization_35, built=True>, <Activation name=activation_35, built=True>, <Conv2D name=conv2d_31, built=True>, <Conv2D name=conv2d_36, built=True>, <BatchNormalization name=batch_normalization_31, built=True>, <BatchNormalization name=batch_normalization_36, built=True>, <Activation name=activation_31, built=True>, <Activation name=activation_36, built=True>, <Conv2D name=conv2d_32, built=True>, <Conv2D name=conv2d_37, built=True>, <BatchNormalization name=batch_normalization_32, built=True>, <BatchNormalization name=batch_normalization_37, built=True>, <Activation name=activation_32, built=True>, <Activation name=activation_37, built=True>, <AveragePooling2D name=average_pooling2d_3, built=True>, <Conv2D name=conv2d_30, built=True>, <Conv2D name=conv2d_33, built=True>, <Conv2D name=conv2d_38, built=True>, <Conv2D name=conv2d_39, built=True>, <BatchNormalization name=batch_normalization_30, built=True>, <BatchNormalization name=batch_normalization_33, built=True>, <BatchNormalization name=batch_normalization_38, built=True>, <BatchNormalization name=batch_normalization_39, built=True>, <Activation name=activation_30, built=True>, <Activation name=activation_33, built=True>, <Activation name=activation_38, built=True>, <Activation name=activation_39, built=True>, <Concatenate name=mixed4, built=True>, <Conv2D name=conv2d_44, built=True>, <BatchNormalization name=batch_normalization_44, built=True>, <Activation name=activation_44, built=True>, <Conv2D name=conv2d_45, built=True>, <BatchNormalization name=batch_normalization_45, built=True>, <Activation name=activation_45, built=True>, <Conv2D name=conv2d_41, built=True>, <Conv2D name=conv2d_46, built=True>, <BatchNormalization name=batch_normalization_41, built=True>, <BatchNormalization name=batch_normalization_46, built=True>, <Activation name=activation_41, built=True>, <Activation name=activation_46, built=True>, <Conv2D name=conv2d_42, built=True>, <Conv2D name=conv2d_47, built=True>, <BatchNormalization name=batch_normalization_42, built=True>, <BatchNormalization name=batch_normalization_47, built=True>, <Activation name=activation_42, built=True>, <Activation name=activation_47, built=True>, <AveragePooling2D name=average_pooling2d_4, built=True>, <Conv2D name=conv2d_40, built=True>, <Conv2D name=conv2d_43, built=True>, <Conv2D name=conv2d_48, built=True>, <Conv2D name=conv2d_49, built=True>, <BatchNormalization name=batch_normalization_40, built=True>, <BatchNormalization name=batch_normalization_43, built=True>, <BatchNormalization name=batch_normalization_48, built=True>, <BatchNormalization name=batch_normalization_49, built=True>, <Activation name=activation_40, built=True>, <Activation name=activation_43, built=True>, <Activation name=activation_48, built=True>, <Activation name=activation_49, built=True>, <Concatenate name=mixed5, built=True>, <Conv2D name=conv2d_54, built=True>, <BatchNormalization name=batch_normalization_54, built=True>, <Activation name=activation_54, built=True>, <Conv2D name=conv2d_55, built=True>, <BatchNormalization name=batch_normalization_55, built=True>, <Activation name=activation_55, built=True>, <Conv2D name=conv2d_51, built=True>, <Conv2D name=conv2d_56, built=True>, <BatchNormalization name=batch_normalization_51, built=True>, <BatchNormalization name=batch_normalization_56, built=True>, <Activation name=activation_51, built=True>, <Activation name=activation_56, built=True>, <Conv2D name=conv2d_52, built=True>, <Conv2D name=conv2d_57, built=True>, <BatchNormalization name=batch_normalization_52, built=True>, <BatchNormalization name=batch_normalization_57, built=True>, <Activation name=activation_52, built=True>, <Activation name=activation_57, built=True>, <AveragePooling2D name=average_pooling2d_5, built=True>, <Conv2D name=conv2d_50, built=True>, <Conv2D name=conv2d_53, built=True>, <Conv2D name=conv2d_58, built=True>, <Conv2D name=conv2d_59, built=True>, <BatchNormalization name=batch_normalization_50, built=True>, <BatchNormalization name=batch_normalization_53, built=True>, <BatchNormalization name=batch_normalization_58, built=True>, <BatchNormalization name=batch_normalization_59, built=True>, <Activation name=activation_50, built=True>, <Activation name=activation_53, built=True>, <Activation name=activation_58, built=True>, <Activation name=activation_59, built=True>, <Concatenate name=mixed6, built=True>, <Conv2D name=conv2d_64, built=True>, <BatchNormalization name=batch_normalization_64, built=True>, <Activation name=activation_64, built=True>, <Conv2D name=conv2d_65, built=True>, <BatchNormalization name=batch_normalization_65, built=True>, <Activation name=activation_65, built=True>, <Conv2D name=conv2d_61, built=True>, <Conv2D name=conv2d_66, built=True>, <BatchNormalization name=batch_normalization_61, built=True>, <BatchNormalization name=batch_normalization_66, built=True>, <Activation name=activation_61, built=True>, <Activation name=activation_66, built=True>, <Conv2D name=conv2d_62, built=True>, <Conv2D name=conv2d_67, built=True>, <BatchNormalization name=batch_normalization_62, built=True>, <BatchNormalization name=batch_normalization_67, built=True>, <Activation name=activation_62, built=True>, <Activation name=activation_67, built=True>, <AveragePooling2D name=average_pooling2d_6, built=True>, <Conv2D name=conv2d_60, built=True>, <Conv2D name=conv2d_63, built=True>, <Conv2D name=conv2d_68, built=True>, <Conv2D name=conv2d_69, built=True>, <BatchNormalization name=batch_normalization_60, built=True>, <BatchNormalization name=batch_normalization_63, built=True>, <BatchNormalization name=batch_normalization_68, built=True>, <BatchNormalization name=batch_normalization_69, built=True>, <Activation name=activation_60, built=True>, <Activation name=activation_63, built=True>, <Activation name=activation_68, built=True>, <Activation name=activation_69, built=True>, <Concatenate name=mixed7, built=True>, <Conv2D name=conv2d_72, built=True>, <BatchNormalization name=batch_normalization_72, built=True>, <Activation name=activation_72, built=True>, <Conv2D name=conv2d_73, built=True>, <BatchNormalization name=batch_normalization_73, built=True>, <Activation name=activation_73, built=True>, <Conv2D name=conv2d_70, built=True>, <Conv2D name=conv2d_74, built=True>, <BatchNormalization name=batch_normalization_70, built=True>, <BatchNormalization name=batch_normalization_74, built=True>, <Activation name=activation_70, built=True>, <Activation name=activation_74, built=True>, <Conv2D name=conv2d_71, built=True>, <Conv2D name=conv2d_75, built=True>, <BatchNormalization name=batch_normalization_71, built=True>, <BatchNormalization name=batch_normalization_75, built=True>, <Activation name=activation_71, built=True>, <Activation name=activation_75, built=True>, <MaxPooling2D name=max_pooling2d_3, built=True>, <Concatenate name=mixed8, built=True>, <Conv2D name=conv2d_80, built=True>, <BatchNormalization name=batch_normalization_80, built=True>, <Activation name=activation_80, built=True>, <Conv2D name=conv2d_77, built=True>, <Conv2D name=conv2d_81, built=True>, <BatchNormalization name=batch_normalization_77, built=True>, <BatchNormalization name=batch_normalization_81, built=True>, <Activation name=activation_77, built=True>, <Activation name=activation_81, built=True>, <Conv2D name=conv2d_78, built=True>, <Conv2D name=conv2d_79, built=True>, <Conv2D name=conv2d_82, built=True>, <Conv2D name=conv2d_83, built=True>, <AveragePooling2D name=average_pooling2d_7, built=True>, <Conv2D name=conv2d_76, built=True>, <BatchNormalization name=batch_normalization_78, built=True>, <BatchNormalization name=batch_normalization_79, built=True>, <BatchNormalization name=batch_normalization_82, built=True>, <BatchNormalization name=batch_normalization_83, built=True>, <Conv2D name=conv2d_84, built=True>, <BatchNormalization name=batch_normalization_76, built=True>, <Activation name=activation_78, built=True>, <Activation name=activation_79, built=True>, <Activation name=activation_82, built=True>, <Activation name=activation_83, built=True>, <BatchNormalization name=batch_normalization_84, built=True>, <Activation name=activation_76, built=True>, <Concatenate name=mixed9_0, built=True>, <Concatenate name=concatenate, built=True>, <Activation name=activation_84, built=True>, <Concatenate name=mixed9, built=True>, <Conv2D name=conv2d_89, built=True>, <BatchNormalization name=batch_normalization_89, built=True>, <Activation name=activation_89, built=True>, <Conv2D name=conv2d_86, built=True>, <Conv2D name=conv2d_90, built=True>, <BatchNormalization name=batch_normalization_86, built=True>, <BatchNormalization name=batch_normalization_90, built=True>, <Activation name=activation_86, built=True>, <Activation name=activation_90, built=True>, <Conv2D name=conv2d_87, built=True>, <Conv2D name=conv2d_88, built=True>, <Conv2D name=conv2d_91, built=True>, <Conv2D name=conv2d_92, built=True>, <AveragePooling2D name=average_pooling2d_8, built=True>, <Conv2D name=conv2d_85, built=True>, <BatchNormalization name=batch_normalization_87, built=True>, <BatchNormalization name=batch_normalization_88, built=True>, <BatchNormalization name=batch_normalization_91, built=True>, <BatchNormalization name=batch_normalization_92, built=True>, <Conv2D name=conv2d_93, built=True>, <BatchNormalization name=batch_normalization_85, built=True>, <Activation name=activation_87, built=True>, <Activation name=activation_88, built=True>, <Activation name=activation_91, built=True>, <Activation name=activation_92, built=True>, <BatchNormalization name=batch_normalization_93, built=True>, <Activation name=activation_85, built=True>, <Concatenate name=mixed9_1, built=True>, <Concatenate name=concatenate_1, built=True>, <Activation name=activation_93, built=True>, <Concatenate name=mixed10, built=True>, <GlobalAveragePooling2D name=global_average_pooling2d, built=True>, <Dense name=dense, built=True>, <Dropout name=dropout, built=True>, <Dense name=dense_1, built=True>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import collections.abc\n",
    "collections.Iterable = collections.abc.Iterable\n",
    "\n",
    "from vis.utils import utils\n",
    "from keras import layers, activations\n",
    "\n",
    "#Assorted modifications for model compatibility with gradCAM\n",
    "gmodel = copy.deepcopy(model)\n",
    "\n",
    "print(gmodel.layers)\n",
    "\n",
    "layer_idx = utils.find_layer_idx(gmodel,'dense_1')\n",
    "\n",
    "#swap with softmax with linear classifier for the reasons mentioned above\n",
    "gmodel.layers[layer_idx].activation = activations.linear\n",
    "gmodel = utils.apply_modifications(gmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(home_directory)\n",
    "os.chdir('paper-reading-analysis')\n",
    "%run gradcam_shap.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756597887.697950   18128 service.cc:152] XLA service 0x7f1b28004830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1756597887.697986   18128 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-08-30 23:51:27.826731: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1756597888.974931   18128 cuda_dnn.cc:529] Loaded cuDNN version 91200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 3/13\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756597894.893377   18128 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 768ms/step\n",
      "[[np.float32(0.9973978), np.float32(0.0026022196)], [np.float32(0.9710433), np.float32(0.028956711)], [np.float32(0.99713385), np.float32(0.002866149)], [np.float32(0.9988141), np.float32(0.001185894)], [np.float32(0.8533026), np.float32(0.1466974)], [np.float32(0.98563015), np.float32(0.014369845)], [np.float32(0.9967534), np.float32(0.0032466054)], [np.float32(0.9978903), np.float32(0.0021097064)], [np.float32(0.9992644), np.float32(0.0007355809)], [np.float32(0.2246893), np.float32(0.7753107)], [np.float32(0.99653196), np.float32(0.0034680367)], [np.float32(0.9423357), np.float32(0.057664275)], [np.float32(0.999062), np.float32(0.0009379983)], [np.float32(0.99596524), np.float32(0.0040347576)], [np.float32(0.96335), np.float32(0.036650002)], [np.float32(0.18505715), np.float32(0.81494284)], [np.float32(0.9793651), np.float32(0.02063489)], [np.float32(0.9939072), np.float32(0.006092787)], [np.float32(0.02192385), np.float32(0.97807616)], [np.float32(0.680553), np.float32(0.31944698)], [np.float32(0.99075925), np.float32(0.0092407465)], [np.float32(0.011571421), np.float32(0.9884286)], [np.float32(0.9996604), np.float32(0.00033962727)], [np.float32(0.0018977048), np.float32(0.9981023)], [np.float32(0.92476064), np.float32(0.07523936)], [np.float32(0.23458128), np.float32(0.7654187)], [np.float32(0.9969003), np.float32(0.00309968)], [np.float32(0.9013312), np.float32(0.09866881)], [np.float32(0.05736991), np.float32(0.9426301)], [np.float32(0.99994564), np.float32(5.4359436e-05)], [np.float32(0.28503075), np.float32(0.7149693)], [np.float32(0.8565942), np.float32(0.1434058)], [np.float32(0.9890673), np.float32(0.010932684)], [np.float32(0.93681175), np.float32(0.063188255)], [np.float32(0.9509044), np.float32(0.04909557)], [np.float32(0.9760641), np.float32(0.023935914)], [np.float32(0.9996295), np.float32(0.00037050247)], [np.float32(0.99802965), np.float32(0.0019703507)], [np.float32(0.13008402), np.float32(0.86991596)], [np.float32(0.95028514), np.float32(0.049714863)], [np.float32(0.99932843), np.float32(0.00067156553)], [np.float32(0.9971214), np.float32(0.0028786063)], [np.float32(0.9825422), np.float32(0.017457783)], [np.float32(0.0414982), np.float32(0.9585018)], [np.float32(0.58282745), np.float32(0.41717255)], [np.float32(0.97649676), np.float32(0.023503244)], [np.float32(0.9858681), np.float32(0.014131904)], [np.float32(0.17841889), np.float32(0.8215811)], [np.float32(0.96641713), np.float32(0.033582866)], [np.float32(0.99441046), np.float32(0.005589545)], [np.float32(0.9992605), np.float32(0.0007395148)], [np.float32(0.020629855), np.float32(0.9793701)], [np.float32(0.99056345), np.float32(0.009436548)], [np.float32(0.47689113), np.float32(0.52310884)], [np.float32(0.99372506), np.float32(0.0062749386)], [np.float32(0.9996047), np.float32(0.000395298)], [np.float32(0.98017806), np.float32(0.019821942)], [np.float32(0.009981313), np.float32(0.99001867)], [np.float32(0.9748052), np.float32(0.025194824)], [np.float32(0.99949205), np.float32(0.0005079508)], [np.float32(0.9546601), np.float32(0.045339882)], [np.float32(0.98247063), np.float32(0.017529368)], [np.float32(0.16725707), np.float32(0.8327429)], [np.float32(0.9883129), np.float32(0.0116871)], [np.float32(0.9979957), np.float32(0.0020043254)], [np.float32(0.832544), np.float32(0.16745597)], [np.float32(0.9992613), np.float32(0.00073868036)], [np.float32(0.045890164), np.float32(0.95410985)], [np.float32(0.90058064), np.float32(0.099419355)], [np.float32(0.99571663), np.float32(0.0042833686)], [np.float32(0.21700703), np.float32(0.78299296)], [np.float32(0.3690051), np.float32(0.6309949)], [np.float32(0.9661688), np.float32(0.03383118)], [np.float32(0.9875244), np.float32(0.01247561)], [np.float32(0.9826406), np.float32(0.017359376)], [np.float32(0.99956805), np.float32(0.00043195486)], [np.float32(0.9080519), np.float32(0.09194809)], [np.float32(0.4557068), np.float32(0.54429317)], [np.float32(0.9988117), np.float32(0.0011882782)], [np.float32(0.99909556), np.float32(0.0009044409)], [np.float32(0.9944042), np.float32(0.0055958033)], [np.float32(0.13959579), np.float32(0.8604042)], [np.float32(0.28465855), np.float32(0.71534145)], [np.float32(0.99465144), np.float32(0.005348563)], [np.float32(0.26293668), np.float32(0.7370633)], [np.float32(0.9984909), np.float32(0.0015090704)], [np.float32(0.9957599), np.float32(0.0042400956)], [np.float32(0.99887985), np.float32(0.0011201501)], [np.float32(0.9864633), np.float32(0.013536692)], [np.float32(0.986606), np.float32(0.013393998)], [np.float32(0.91315544), np.float32(0.08684456)], [np.float32(0.8965665), np.float32(0.10343349)], [np.float32(0.14138533), np.float32(0.8586147)], [np.float32(0.7000911), np.float32(0.29990888)], [np.float32(0.99913204), np.float32(0.00086796284)], [np.float32(0.1906865), np.float32(0.80931354)], [np.float32(0.993006), np.float32(0.006994009)], [np.float32(0.99829406), np.float32(0.0017059445)], [np.float32(0.9699636), np.float32(0.03003639)], [np.float32(0.95397586), np.float32(0.046024144)], [np.float32(0.9998199), np.float32(0.00018012524)], [np.float32(0.44802466), np.float32(0.55197537)], [np.float32(0.6859903), np.float32(0.31400973)], [np.float32(0.9792818), np.float32(0.020718217)], [np.float32(0.9681916), np.float32(0.031808376)], [np.float32(0.080898464), np.float32(0.91910154)], [np.float32(0.97937745), np.float32(0.020622551)], [np.float32(0.9952052), np.float32(0.0047947764)], [np.float32(0.985803), np.float32(0.014196992)], [np.float32(0.9971107), np.float32(0.0028892756)], [np.float32(0.16574071), np.float32(0.8342593)], [np.float32(0.26706186), np.float32(0.7329382)], [np.float32(0.87097156), np.float32(0.12902844)], [np.float32(0.991366), np.float32(0.008633971)], [np.float32(0.99747306), np.float32(0.002526939)], [np.float32(0.99888164), np.float32(0.001118362)], [np.float32(0.99656016), np.float32(0.0034398437)], [np.float32(0.0006823276), np.float32(0.99931765)], [np.float32(0.9522782), np.float32(0.047721803)], [np.float32(0.99793905), np.float32(0.0020609498)], [np.float32(0.9957671), np.float32(0.0042328835)], [np.float32(0.99867755), np.float32(0.0013224483)], [np.float32(0.32808873), np.float32(0.67191124)], [np.float32(0.996601), np.float32(0.0033990145)], [np.float32(0.9983405), np.float32(0.0016595125)], [np.float32(0.9324802), np.float32(0.067519784)], [np.float32(0.96183234), np.float32(0.038167655)], [np.float32(0.9881726), np.float32(0.011827409)], [np.float32(0.9989147), np.float32(0.0010852814)], [np.float32(0.97764677), np.float32(0.022353232)], [np.float32(0.040778358), np.float32(0.95922166)], [np.float32(0.98600405), np.float32(0.013995945)], [np.float32(0.9980374), np.float32(0.0019626021)], [np.float32(0.99418277), np.float32(0.0058172345)], [np.float32(0.8676239), np.float32(0.13237607)], [np.float32(0.99723476), np.float32(0.0027652383)], [np.float32(0.95666075), np.float32(0.043339252)], [np.float32(0.29953164), np.float32(0.70046836)], [np.float32(0.21871561), np.float32(0.7812844)], [np.float32(0.42534387), np.float32(0.5746561)], [np.float32(0.9890358), np.float32(0.010964215)], [np.float32(0.9974287), np.float32(0.0025712848)], [np.float32(0.99856037), np.float32(0.001439631)], [np.float32(0.99974805), np.float32(0.00025194883)], [np.float32(0.93614656), np.float32(0.06385344)], [np.float32(0.9983473), np.float32(0.0016527176)], [np.float32(0.9247175), np.float32(0.075282514)], [np.float32(0.9994081), np.float32(0.0005918741)], [np.float32(0.99804497), np.float32(0.0019550323)], [np.float32(0.99127185), np.float32(0.008728147)], [np.float32(0.2185592), np.float32(0.7814408)], [np.float32(0.15041672), np.float32(0.84958327)], [np.float32(0.9871171), np.float32(0.012882888)], [np.float32(0.004067268), np.float32(0.99593276)], [np.float32(0.8663229), np.float32(0.13367712)], [np.float32(0.52304804), np.float32(0.47695196)], [np.float32(0.9986505), np.float32(0.0013495088)], [np.float32(0.990945), np.float32(0.009055018)], [np.float32(0.7573187), np.float32(0.24268132)], [np.float32(0.99995446), np.float32(4.553795e-05)], [np.float32(0.9913857), np.float32(0.008614302)], [np.float32(0.93060875), np.float32(0.06939125)], [np.float32(0.55126536), np.float32(0.44873464)], [np.float32(0.65591335), np.float32(0.34408665)], [np.float32(0.99583906), np.float32(0.0041609406)], [np.float32(0.007289847), np.float32(0.9927102)], [np.float32(0.99545574), np.float32(0.004544258)], [np.float32(0.9329213), np.float32(0.06707871)], [np.float32(0.01365575), np.float32(0.9863443)], [np.float32(0.8456223), np.float32(0.1543777)], [np.float32(0.99698275), np.float32(0.0030172467)], [np.float32(0.2762531), np.float32(0.7237469)], [np.float32(0.9529265), np.float32(0.047073483)], [np.float32(0.19267547), np.float32(0.8073245)], [np.float32(0.9136106), np.float32(0.08638942)], [np.float32(0.9880601), np.float32(0.011939883)], [np.float32(0.08689322), np.float32(0.9131068)], [np.float32(0.6394185), np.float32(0.36058152)], [np.float32(0.9993362), np.float32(0.0006638169)], [np.float32(0.38490582), np.float32(0.6150942)], [np.float32(0.017712623), np.float32(0.9822874)], [np.float32(0.9367698), np.float32(0.06323022)], [np.float32(0.9357986), np.float32(0.064201415)], [np.float32(0.98790014), np.float32(0.012099862)], [np.float32(0.9614529), np.float32(0.0385471)], [np.float32(0.1898461), np.float32(0.8101539)], [np.float32(0.41229582), np.float32(0.5877042)], [np.float32(0.013555236), np.float32(0.9864448)], [np.float32(0.99954134), np.float32(0.00045865774)], [np.float32(0.6354682), np.float32(0.36453182)], [np.float32(0.043160345), np.float32(0.9568397)], [np.float32(0.9998889), np.float32(0.00011110306)], [np.float32(0.99624294), np.float32(0.0037570596)], [np.float32(0.82690316), np.float32(0.17309684)], [np.float32(0.96951866), np.float32(0.030481339)], [np.float32(0.964396), np.float32(0.035604)], [np.float32(0.9490183), np.float32(0.0509817)], [np.float32(0.2493434), np.float32(0.7506566)], [np.float32(0.74696857), np.float32(0.25303143)], [np.float32(0.91647863), np.float32(0.083521366)], [np.float32(0.0043181987), np.float32(0.9956818)], [np.float32(0.014605537), np.float32(0.9853945)], [np.float32(0.0012838293), np.float32(0.9987162)], [np.float32(0.005441053), np.float32(0.99455893)], [np.float32(0.00039464014), np.float32(0.99960536)], [np.float32(0.0002754553), np.float32(0.99972457)], [np.float32(0.5962997), np.float32(0.4037003)], [np.float32(0.0006098947), np.float32(0.9993901)], [np.float32(0.00021339886), np.float32(0.9997866)], [np.float32(0.021950768), np.float32(0.9780492)], [np.float32(0.0055167144), np.float32(0.9944833)], [np.float32(0.9885704), np.float32(0.011429608)], [np.float32(0.23774397), np.float32(0.762256)], [np.float32(0.9602015), np.float32(0.0397985)], [np.float32(0.0068778605), np.float32(0.99312216)], [np.float32(9.5441035e-05), np.float32(0.9999046)], [np.float32(0.0215509), np.float32(0.9784491)], [np.float32(0.032592367), np.float32(0.96740764)], [np.float32(0.0064995727), np.float32(0.9935004)], [np.float32(0.0003792851), np.float32(0.99962074)], [np.float32(0.0010085098), np.float32(0.9989915)], [np.float32(6.4715394e-05), np.float32(0.99993527)], [np.float32(0.17818147), np.float32(0.82181853)], [np.float32(0.025581855), np.float32(0.97441816)], [np.float32(0.0019646848), np.float32(0.9980353)], [np.float32(0.004206708), np.float32(0.9957933)], [np.float32(0.96942073), np.float32(0.030579269)], [np.float32(0.007644442), np.float32(0.9923556)], [np.float32(0.80853826), np.float32(0.19146174)], [np.float32(0.011599489), np.float32(0.9884005)], [np.float32(0.9739862), np.float32(0.026013792)], [np.float32(0.5352562), np.float32(0.4647438)], [np.float32(0.015568425), np.float32(0.98443156)], [np.float32(0.0032866672), np.float32(0.99671334)], [np.float32(0.9513822), np.float32(0.04861778)], [np.float32(0.0013842552), np.float32(0.99861574)], [np.float32(0.00024948907), np.float32(0.9997505)], [np.float32(8.2741084e-05), np.float32(0.99991727)], [np.float32(0.0008941367), np.float32(0.9991059)], [np.float32(0.0011694445), np.float32(0.99883056)], [np.float32(0.0029845864), np.float32(0.9970154)], [np.float32(0.0046112067), np.float32(0.9953888)], [np.float32(0.005821538), np.float32(0.9941785)], [np.float32(0.002478355), np.float32(0.99752164)], [np.float32(0.0038236931), np.float32(0.9961763)], [np.float32(0.002678641), np.float32(0.99732137)], [np.float32(0.00022859102), np.float32(0.9997714)], [np.float32(0.025643738), np.float32(0.97435623)], [np.float32(8.263006e-05), np.float32(0.9999174)], [np.float32(0.0044548823), np.float32(0.9955451)], [np.float32(7.822608e-06), np.float32(0.9999922)], [np.float32(0.0011863987), np.float32(0.9988136)], [np.float32(4.3251097e-05), np.float32(0.9999567)], [np.float32(0.002379965), np.float32(0.99762005)], [np.float32(0.9925575), np.float32(0.0074424744)], [np.float32(0.0009235171), np.float32(0.9990765)], [np.float32(0.0014799034), np.float32(0.9985201)], [np.float32(0.00028478063), np.float32(0.9997152)], [np.float32(0.96295166), np.float32(0.03704834)], [np.float32(0.22786295), np.float32(0.77213705)], [np.float32(0.045842275), np.float32(0.9541577)], [np.float32(0.011339524), np.float32(0.98866045)], [np.float32(0.00037226648), np.float32(0.9996277)], [np.float32(0.93480104), np.float32(0.06519896)], [np.float32(0.004350597), np.float32(0.9956494)], [np.float32(0.0008889325), np.float32(0.99911106)], [np.float32(0.0008393611), np.float32(0.99916065)], [np.float32(0.05118922), np.float32(0.94881076)], [np.float32(0.0015616638), np.float32(0.99843836)], [np.float32(0.0005144366), np.float32(0.99948555)], [np.float32(0.91321623), np.float32(0.08678377)], [np.float32(0.06780738), np.float32(0.9321926)], [np.float32(0.08022042), np.float32(0.9197796)], [np.float32(0.0033426574), np.float32(0.9966574)], [np.float32(0.002414825), np.float32(0.9975852)], [np.float32(0.0010039839), np.float32(0.998996)], [np.float32(0.04748765), np.float32(0.9525123)], [np.float32(0.0030773454), np.float32(0.9969227)], [np.float32(0.05711217), np.float32(0.94288784)], [np.float32(0.005077789), np.float32(0.9949222)], [np.float32(0.21477929), np.float32(0.78522074)], [np.float32(0.00036087012), np.float32(0.99963915)], [np.float32(0.0018627343), np.float32(0.9981373)], [np.float32(0.00818899), np.float32(0.99181104)], [np.float32(0.0019080924), np.float32(0.99809194)], [np.float32(0.08191136), np.float32(0.9180886)], [np.float32(0.13281663), np.float32(0.8671834)], [np.float32(0.0002541157), np.float32(0.9997459)], [np.float32(0.0052168802), np.float32(0.9947831)], [np.float32(0.0036629757), np.float32(0.996337)], [np.float32(0.00037095285), np.float32(0.999629)], [np.float32(0.00040756032), np.float32(0.9995924)], [np.float32(0.000119950935), np.float32(0.9998801)], [np.float32(0.33855948), np.float32(0.6614405)], [np.float32(0.0018362122), np.float32(0.99816376)], [np.float32(0.0012777506), np.float32(0.99872226)], [np.float32(0.0011273362), np.float32(0.99887264)], [np.float32(0.29745212), np.float32(0.7025479)], [np.float32(0.0010203492), np.float32(0.9989796)], [np.float32(0.00030340324), np.float32(0.9996966)], [np.float32(0.00033157217), np.float32(0.9996684)], [np.float32(7.282565e-05), np.float32(0.99992716)], [np.float32(5.8286543e-05), np.float32(0.9999417)], [np.float32(0.0037693137), np.float32(0.99623066)], [np.float32(0.50324637), np.float32(0.49675363)], [np.float32(0.003396157), np.float32(0.99660385)], [np.float32(0.0004856689), np.float32(0.99951434)], [np.float32(0.00017237374), np.float32(0.9998276)], [np.float32(0.00012610898), np.float32(0.9998739)], [np.float32(0.0011237604), np.float32(0.9988762)], [np.float32(0.0041159354), np.float32(0.99588406)], [np.float32(4.979662e-05), np.float32(0.99995023)], [np.float32(4.7555863e-05), np.float32(0.99995244)], [np.float32(0.0049335286), np.float32(0.99506646)], [np.float32(0.00013332946), np.float32(0.99986666)], [np.float32(0.0021678736), np.float32(0.9978321)], [np.float32(0.27748182), np.float32(0.7225182)], [np.float32(0.00434923), np.float32(0.99565077)], [np.float32(0.009653973), np.float32(0.990346)], [np.float32(1.3084127e-05), np.float32(0.9999869)], [np.float32(2.959373e-05), np.float32(0.9999704)], [np.float32(0.24350376), np.float32(0.75649625)], [np.float32(0.002054644), np.float32(0.99794537)], [np.float32(0.35468853), np.float32(0.6453115)], [np.float32(0.005850657), np.float32(0.9941493)], [np.float32(0.049471498), np.float32(0.9505285)], [np.float32(0.000528105), np.float32(0.9994719)], [np.float32(0.004216837), np.float32(0.99578315)], [np.float32(0.001742666), np.float32(0.99825734)], [np.float32(7.314469e-05), np.float32(0.99992687)], [np.float32(0.006683018), np.float32(0.993317)], [np.float32(0.00019971827), np.float32(0.99980026)], [np.float32(0.00024264914), np.float32(0.99975735)], [np.float32(0.012068945), np.float32(0.9879311)], [np.float32(0.00750657), np.float32(0.99249345)], [np.float32(0.00042694734), np.float32(0.99957305)], [np.float32(0.028812574), np.float32(0.9711874)], [np.float32(0.00034360072), np.float32(0.9996564)], [np.float32(0.005545558), np.float32(0.99445444)], [np.float32(0.00026680785), np.float32(0.9997332)], [np.float32(0.00104503), np.float32(0.99895495)], [np.float32(7.375391e-05), np.float32(0.99992627)], [np.float32(0.05784883), np.float32(0.9421512)], [np.float32(0.007211733), np.float32(0.99278826)], [np.float32(0.21494651), np.float32(0.7850535)], [np.float32(0.014164315), np.float32(0.9858357)], [np.float32(0.21273664), np.float32(0.7872634)], [np.float32(0.0002473385), np.float32(0.99975264)], [np.float32(0.00033639095), np.float32(0.9996636)], [np.float32(0.2417575), np.float32(0.7582425)], [np.float32(0.5902385), np.float32(0.4097615)], [np.float32(0.53969365), np.float32(0.46030635)], [np.float32(0.017937418), np.float32(0.9820626)], [np.float32(0.062994614), np.float32(0.9370054)], [np.float32(0.00013218723), np.float32(0.9998678)], [np.float32(0.02170778), np.float32(0.9782922)], [np.float32(0.025872082), np.float32(0.9741279)], [np.float32(0.022924976), np.float32(0.97707504)], [np.float32(0.0021567158), np.float32(0.99784327)], [np.float32(0.6277445), np.float32(0.3722555)], [np.float32(0.20053288), np.float32(0.7994671)], [np.float32(0.00023663863), np.float32(0.99976337)], [np.float32(0.0050823484), np.float32(0.99491763)], [np.float32(0.013957063), np.float32(0.9860429)], [np.float32(0.008217202), np.float32(0.9917828)], [np.float32(3.9026603e-05), np.float32(0.99996096)], [np.float32(0.00022776733), np.float32(0.99977225)], [np.float32(0.000251222), np.float32(0.99974877)], [np.float32(0.46343893), np.float32(0.5365611)], [np.float32(0.0024760705), np.float32(0.9975239)], [np.float32(0.0021723588), np.float32(0.99782765)], [np.float32(0.00021843858), np.float32(0.99978155)], [np.float32(0.98847944), np.float32(0.011520565)], [np.float32(7.554299e-05), np.float32(0.9999245)], [np.float32(0.013286652), np.float32(0.98671335)], [np.float32(0.8922503), np.float32(0.1077497)], [np.float32(0.0674569), np.float32(0.9325431)], [np.float32(0.80844563), np.float32(0.19155437)], [np.float32(0.0002245602), np.float32(0.99977547)], [np.float32(0.0013020657), np.float32(0.99869794)], [np.float32(0.009506439), np.float32(0.99049354)], [np.float32(0.0015505945), np.float32(0.9984494)], [np.float32(0.0010801834), np.float32(0.99891984)], [np.float32(0.7253508), np.float32(0.2746492)], [np.float32(0.00010804441), np.float32(0.99989194)], [np.float32(0.0010372372), np.float32(0.99896276)], [np.float32(0.0006308592), np.float32(0.99936914)], [np.float32(0.00092980656), np.float32(0.99907017)], [np.float32(1.7262808e-05), np.float32(0.9999827)], [np.float32(0.018788137), np.float32(0.98121184)], [np.float32(0.0034471604), np.float32(0.9965528)], [np.float32(0.00016469693), np.float32(0.9998353)], [np.float32(0.0072307815), np.float32(0.99276924)], [np.float32(0.47164604), np.float32(0.5283539)], [np.float32(0.0046235276), np.float32(0.99537647)], [np.float32(0.00017121463), np.float32(0.99982876)], [np.float32(0.008819062), np.float32(0.99118096)], [np.float32(0.00089049945), np.float32(0.9991095)], [np.float32(0.07411112), np.float32(0.9258889)], [np.float32(1.10922465e-05), np.float32(0.9999889)]]\n",
      "Unseen set\n",
      "      ID        Dx     % Mel     % Nev\n",
      "0      0  Melanoma  0.997398  0.002602\n",
      "1      1  Melanoma  0.971043  0.028957\n",
      "2      2  Melanoma  0.997134  0.002866\n",
      "3      3  Melanoma  0.998814  0.001186\n",
      "4      4  Melanoma  0.853303  0.146697\n",
      "..   ...       ...       ...       ...\n",
      "395  395     Nevus  0.000171  0.999829\n",
      "396  396     Nevus  0.008819  0.991181\n",
      "397  397     Nevus  0.000890  0.999110\n",
      "398  398     Nevus  0.074111  0.925889\n",
      "399  399     Nevus  0.000011  0.999989\n",
      "\n",
      "[400 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "os.chdir(home_directory)\n",
    "os.chdir('DNNorDermatologist')\n",
    "\n",
    "# Get the test dataset of 400 - 200 nevi and 200 melanoma\n",
    "test_df = pd.read_pickle('NvAndMelNoDuplicatesFullSizeTestSet.zip')\n",
    "\n",
    "# Change the idx column to be '0' where the diagnosis of the lesion was\n",
    "# nevi, and '1' when the diagnosis is diagnosis\n",
    "test_df['idx'] = np.where(test_df['id'] == 'mel', 1 , 0)\n",
    "\n",
    "# Save a new table 'features' to be test_df, without the idx column\n",
    "features=test_df.drop(columns=['idx'], axis = 1)\n",
    "# Create a new table with just the correct diagnosis (0 for melanoma (or nevi), 1 for nevi (or melanoma))\n",
    "target=test_df['idx']\n",
    "\n",
    "# Change features to be a numpy array of image pixel data ((R, G, B))\n",
    "features = np.asarray(features['image'].tolist())\n",
    "\n",
    "# I want to resize the images \n",
    "features = np.array([cv2.resize(image, (224, 224)) for image in features])\n",
    "\n",
    "# Normalise this data in an alternate table to be values from 0 ... 1\n",
    "# e.g. 255 -> 1, 0 --> 0\n",
    "# Normalises for original prediction and evaluation of model, the SHAP funciton below requires non normalised data\n",
    "# TODO: Standarise this so SHAP takes normalised\n",
    "\n",
    "features2 = features / 255\n",
    "\n",
    "# Convert the data to one-hot encoding\n",
    "target_cat = to_categorical(target, num_classes = 2)\n",
    "\n",
    "# Get predictions for image data\n",
    "# e.g.\n",
    "# Index 0 : [0.9222, 0.0778]\n",
    "# Index 1 : [0.4500, 0.5500]\n",
    "# etc..\n",
    "# This represents likelihood of melanoma and nevi respectively (according to the model)\n",
    "y_pred = model.predict(features2, verbose=1)\n",
    "y_pred = [[value[0], 1-value[0]] for value in y_pred]\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "# Create a new dataframe with entries for each element of the test set\n",
    "# Include an ID, diagnosis, and % likelihoods for each diagnosis from the model\n",
    "df = pd.DataFrame(columns=['ID', 'Dx', '% Mel', '% Nev'],index=[i for i in range(400)])\n",
    "df['ID'] = df.index\n",
    "\n",
    "# Create dictionaries to contain actual diagnosis and probabilities from the model\n",
    "dx_d = {}\n",
    "Pmel = {}\n",
    "Pnev = {}\n",
    "# Take the actual diagnoses from where we retrieved them earlier\n",
    "y_test_cat = target_cat\n",
    "\n",
    "# For each element in the test set:\n",
    "for ind in range(400):\n",
    "    # Append the diagnosis and predictions to their respective dictionaries\n",
    "    if y_test_cat[ind][1] == 1.0:\n",
    "        diagnosis = 'Melanoma'\n",
    "    elif y_test_cat[ind][0] == 1.0:\n",
    "        diagnosis = 'Nevus'\n",
    "    dx_d[ind] = diagnosis\n",
    "    Pmel[ind] = y_pred[ind][0]\n",
    "    Pnev[ind] = y_pred[ind][1]\n",
    "    \n",
    "# Take the above dictionaries and insert them into the data frame\n",
    "df['Dx'] = df['ID'].map(dx_d)\n",
    "df['% Mel'] = df['ID'].map(Pmel)\n",
    "df['% Nev'] = df['ID'].map(Pnev)\n",
    "\n",
    "# Change the prediction likelihoods to be floats \n",
    "df = df.astype({\"% Mel\": float, \"% Nev\": float})\n",
    "\n",
    "#df = df.iloc[id_list]\n",
    "\n",
    "# Print the first 5 entries in the data frame\n",
    "print('Unseen set') \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "os.chdir('..')\n",
    "os.chdir('paper-reading-analysis')\n",
    "\n",
    "# I want examine the results, so I will just save them\n",
    "df.to_csv(f'predictions_model_{seed}.csv')\n",
    "\n",
    "print(seed)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
