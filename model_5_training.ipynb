{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Reading Analysis - Code Implementation\n",
    "### Model 5 Training, Hyperparameter Search and Evaluation\n",
    "### Jonathan Alcineus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 22:36:27.358667: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-30 22:36:27.428889: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756593387.457962    5266 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756593387.466808    5266 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756593387.484916    5266 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756593387.484950    5266 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756593387.484952    5266 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756593387.484953    5266 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-30 22:36:27.490154: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# These handle the file locations and importing the dataframe from the saved datafile from the authors files\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# These handle the image processing, editing, or displaying that needs to be performed\n",
    "import cv2 \n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "\n",
    "# These handle training the convolutional neural network (CNN) model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "import time\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/DNNorDermatologist\n"
     ]
    }
   ],
   "source": [
    "# This changes the home directory\n",
    "home_directory = os.path.expanduser(\"~\")\n",
    "os.chdir(home_directory)\n",
    "\n",
    "# Then goes to the folder where the data lies\n",
    "os.chdir('DNNorDermatologist')\n",
    "\n",
    "# Ensures that we are in the correct folder\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin to build the classifier and the ranges for each model to find the optimal parameters, or searching through hyperparameters\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "\n",
    "space = [Real(1e-6, 0.01, \"log-uniform\", name='learning_rate'),\n",
    "          Real(0.1, 0.8, name='dropout'),\n",
    "          Real(0.8, 1.0, name='momentum'),\n",
    "          Real(0.9, 1.0, name='beta_1'),\n",
    "          Real(0.99, 1.0, name='beta_2'),\n",
    "          Integer(low=5,high=20, name = 'epochs'),\n",
    "          Integer(low=50, high=225, name='num_dense_nodes'),\n",
    "          Categorical(categories=['SGD', 'Adam'],\n",
    "                             name='optimizer_type')\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The first part to implenment is the creation of random models\n",
    "if not os.path.isdir('suite_of_models'):\n",
    "    os.mkdir('suite_of_models')\n",
    "\n",
    "def make_a_model(learning_rate, dropout, momentum, beta_1, beta_2, num_dense_nodes, optimizer_type):\n",
    "    # Like in the paper the base model for the image classifcation will be imagenet\n",
    "    base_model = InceptionV3(weights='imagenet',input_shape=(224, 224, 3), include_top=False)\n",
    "\n",
    "    # Fine tune the model with extra dense layers\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(num_dense_nodes, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = Dropout(rate=dropout)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Selects a type of model optimizer\n",
    "    if optimizer_type == \"Adam\":\n",
    "        optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "    elif optimizer_type == \"SGD\":\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start off with basic parameters and the batch size for the models\n",
    "batch_size = 16\n",
    "best_accuracy = {} \n",
    "for seed in range(15):\n",
    "  best_accuracy[seed] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are currently training on seed: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "learning rate: 1.2e-04\n",
      "num_dense_nodes: 98\n",
      "dropout: 0.30776521376363053\n",
      "optimizer_type: SGD\n",
      "epochs: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755995456.116816   62857 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1755995480.549317   64217 service.cc:152] XLA service 0x7ff25c014d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1755995480.549344   64217 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-08-24 00:31:21.496155: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1755995484.499733   64217 cuda_dnn.cc:529] Loaded cuDNN version 91200\n",
      "2025-08-24 00:31:32.804856: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-08-24 00:31:32.952362: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-08-24 00:31:33.316726: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-08-24 00:31:33.460780: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/52\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m44:51\u001b[0m 53s/step - accuracy: 0.6250 - loss: 0.6248"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755995512.479062   64217 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m51/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.5437 - loss: 0.7121"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 00:32:08.627750: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-08-24 00:32:08.775216: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-08-24 00:32:09.087535: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-08-24 00:32:09.230566: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 924ms/step - accuracy: 0.5906 - loss: 0.6745 - val_accuracy: 0.6812 - val_loss: 0.6191\n",
      "Epoch 2/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.8116 - loss: 0.4458 - val_accuracy: 0.6341 - val_loss: 0.6981\n",
      "Epoch 3/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.8418 - loss: 0.3656 - val_accuracy: 0.7899 - val_loss: 0.4415\n",
      "Epoch 4/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8623 - loss: 0.2948 - val_accuracy: 0.8188 - val_loss: 0.3783\n",
      "Epoch 5/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8841 - loss: 0.2490 - val_accuracy: 0.8176 - val_loss: 0.3892\n",
      "Epoch 6/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.9263 - loss: 0.1792 - val_accuracy: 0.8225 - val_loss: 0.4386\n",
      "Epoch 7/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.9457 - loss: 0.1373 - val_accuracy: 0.8152 - val_loss: 0.4258\n",
      "Epoch 8/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.9408 - loss: 0.1626 - val_accuracy: 0.7500 - val_loss: 0.7941\n",
      "Epoch 9/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.9589 - loss: 0.1126 - val_accuracy: 0.7126 - val_loss: 1.0029\n",
      "Epoch 10/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.9734 - loss: 0.0770 - val_accuracy: 0.8442 - val_loss: 0.5590\n",
      "Epoch 11/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.9698 - loss: 0.0878 - val_accuracy: 0.8454 - val_loss: 0.8130\n",
      "Epoch 12/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.9795 - loss: 0.0638 - val_accuracy: 0.8442 - val_loss: 0.7374\n",
      "Epoch 13/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.9698 - loss: 0.0969 - val_accuracy: 0.8357 - val_loss: 0.6042\n",
      "Epoch 14/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.9650 - loss: 0.0944 - val_accuracy: 0.8442 - val_loss: 0.6990\n",
      "Epoch 15/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.9855 - loss: 0.0428 - val_accuracy: 0.8514 - val_loss: 1.0312\n",
      "Epoch 16/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.9746 - loss: 0.0667 - val_accuracy: 0.8611 - val_loss: 0.8647\n",
      "Epoch 17/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.9783 - loss: 0.0562 - val_accuracy: 0.8696 - val_loss: 0.8579\n",
      "Epoch 18/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.9771 - loss: 0.0717 - val_accuracy: 0.8249 - val_loss: 1.2002\n",
      "\n",
      "Accuracy: 82.49%\n",
      "\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 237.7143\n",
      "Function value obtained: -0.8249\n",
      "Current minimum: -0.8249\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "learning rate: 1.1e-06\n",
      "num_dense_nodes: 77\n",
      "dropout: 0.7583920083546086\n",
      "optimizer_type: SGD\n",
      "epochs: 12\n",
      "Epoch 1/12\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 775ms/step - accuracy: 0.5217 - loss: 0.7999 - val_accuracy: 0.5169 - val_loss: 0.7556\n",
      "Epoch 2/12\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5181 - loss: 0.8098 - val_accuracy: 0.5229 - val_loss: 0.7321\n",
      "Epoch 3/12\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5145 - loss: 0.8327 - val_accuracy: 0.5157 - val_loss: 0.7197\n",
      "Epoch 4/12\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5254 - loss: 0.8288 - val_accuracy: 0.5350 - val_loss: 0.7045\n",
      "Epoch 5/12\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.5109 - loss: 0.8107 - val_accuracy: 0.5411 - val_loss: 0.6987\n",
      "Epoch 6/12\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.5290 - loss: 0.7915 - val_accuracy: 0.5326 - val_loss: 0.6984\n",
      "Epoch 7/12\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 151ms/step - accuracy: 0.5386 - loss: 0.7745 - val_accuracy: 0.5459 - val_loss: 0.6948\n",
      "Epoch 8/12\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.5036 - loss: 0.7983 - val_accuracy: 0.5386 - val_loss: 0.6919\n",
      "Epoch 9/12\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.5326 - loss: 0.7878 - val_accuracy: 0.5568 - val_loss: 0.6897\n",
      "Epoch 10/12\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.5229 - loss: 0.7917 - val_accuracy: 0.5495 - val_loss: 0.6882\n",
      "Epoch 11/12\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.5326 - loss: 0.8083 - val_accuracy: 0.5495 - val_loss: 0.6873\n",
      "Epoch 12/12\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.5531 - loss: 0.7570 - val_accuracy: 0.5616 - val_loss: 0.6855\n",
      "\n",
      "Accuracy: 56.16%\n",
      "\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 176.4005\n",
      "Function value obtained: -0.5616\n",
      "Current minimum: -0.8249\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "learning rate: 7.4e-04\n",
      "num_dense_nodes: 184\n",
      "dropout: 0.7755380180903263\n",
      "optimizer_type: Adam\n",
      "epochs: 20\n",
      "Epoch 1/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 914ms/step - accuracy: 0.7246 - loss: 0.6503 - val_accuracy: 0.5024 - val_loss: 17.8199\n",
      "Epoch 2/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.8080 - loss: 0.4574 - val_accuracy: 0.7428 - val_loss: 0.7086\n",
      "Epoch 3/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7971 - loss: 0.4607 - val_accuracy: 0.5845 - val_loss: 5.6427\n",
      "Epoch 4/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.8164 - loss: 0.4007 - val_accuracy: 0.8068 - val_loss: 0.7599\n",
      "Epoch 5/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.8527 - loss: 0.3664 - val_accuracy: 0.8478 - val_loss: 0.6571\n",
      "Epoch 6/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.8611 - loss: 0.3267 - val_accuracy: 0.8249 - val_loss: 0.5070\n",
      "Epoch 7/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.8188 - loss: 0.4365 - val_accuracy: 0.6087 - val_loss: 41.6278\n",
      "Epoch 8/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.8635 - loss: 0.3376 - val_accuracy: 0.8345 - val_loss: 0.8353\n",
      "Epoch 9/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 175ms/step - accuracy: 0.8671 - loss: 0.3102 - val_accuracy: 0.7899 - val_loss: 0.5570\n",
      "Epoch 10/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.8696 - loss: 0.3079 - val_accuracy: 0.8007 - val_loss: 0.4100\n",
      "Epoch 11/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.8853 - loss: 0.3508 - val_accuracy: 0.5000 - val_loss: 39.5122\n",
      "Epoch 12/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8418 - loss: 0.3761 - val_accuracy: 0.7222 - val_loss: 2.0115\n",
      "Epoch 13/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 158ms/step - accuracy: 0.8671 - loss: 0.3173 - val_accuracy: 0.7995 - val_loss: 0.4259\n",
      "Epoch 14/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9106 - loss: 0.2242 - val_accuracy: 0.8418 - val_loss: 0.8925\n",
      "Epoch 15/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9251 - loss: 0.2077 - val_accuracy: 0.6606 - val_loss: 1.4116\n",
      "Epoch 16/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 159ms/step - accuracy: 0.9348 - loss: 0.1863 - val_accuracy: 0.6860 - val_loss: 0.8435\n",
      "Epoch 17/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9324 - loss: 0.1647 - val_accuracy: 0.6775 - val_loss: 1.5907\n",
      "Epoch 18/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9263 - loss: 0.2118 - val_accuracy: 0.7705 - val_loss: 0.5174\n",
      "Epoch 19/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9493 - loss: 0.1579 - val_accuracy: 0.6486 - val_loss: 1.3238\n",
      "Epoch 20/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9481 - loss: 0.1326 - val_accuracy: 0.8382 - val_loss: 0.7632\n",
      "\n",
      "Accuracy: 83.82%\n",
      "\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 270.8972\n",
      "Function value obtained: -0.8382\n",
      "Current minimum: -0.8382\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "learning rate: 1.4e-06\n",
      "num_dense_nodes: 105\n",
      "dropout: 0.18611808199363974\n",
      "optimizer_type: Adam\n",
      "epochs: 18\n",
      "Epoch 1/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 898ms/step - accuracy: 0.5145 - loss: 0.7180 - val_accuracy: 0.4915 - val_loss: 0.7204\n",
      "Epoch 2/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.5495 - loss: 0.6867 - val_accuracy: 0.5374 - val_loss: 0.6934\n",
      "Epoch 3/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.5870 - loss: 0.6684 - val_accuracy: 0.5773 - val_loss: 0.6686\n",
      "Epoch 4/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.6643 - loss: 0.6309 - val_accuracy: 0.6473 - val_loss: 0.6428\n",
      "Epoch 5/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.7029 - loss: 0.6088 - val_accuracy: 0.6908 - val_loss: 0.6166\n",
      "Epoch 6/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.7162 - loss: 0.5859 - val_accuracy: 0.7283 - val_loss: 0.5961\n",
      "Epoch 7/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.7512 - loss: 0.5712 - val_accuracy: 0.7585 - val_loss: 0.5746\n",
      "Epoch 8/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.7560 - loss: 0.5584 - val_accuracy: 0.7705 - val_loss: 0.5564\n",
      "Epoch 9/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.7778 - loss: 0.5368 - val_accuracy: 0.7911 - val_loss: 0.5390\n",
      "Epoch 10/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8007 - loss: 0.5079 - val_accuracy: 0.7995 - val_loss: 0.5223\n",
      "Epoch 11/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 159ms/step - accuracy: 0.8068 - loss: 0.5002 - val_accuracy: 0.8080 - val_loss: 0.5076\n",
      "Epoch 12/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8213 - loss: 0.4670 - val_accuracy: 0.8140 - val_loss: 0.4945\n",
      "Epoch 13/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8225 - loss: 0.4512 - val_accuracy: 0.8140 - val_loss: 0.4801\n",
      "Epoch 14/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.8370 - loss: 0.4404 - val_accuracy: 0.8128 - val_loss: 0.4669\n",
      "Epoch 15/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8418 - loss: 0.4347 - val_accuracy: 0.8164 - val_loss: 0.4562\n",
      "Epoch 16/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.8430 - loss: 0.4177 - val_accuracy: 0.8164 - val_loss: 0.4471\n",
      "Epoch 17/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8454 - loss: 0.4050 - val_accuracy: 0.8200 - val_loss: 0.4360\n",
      "Epoch 18/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.8514 - loss: 0.3833 - val_accuracy: 0.8225 - val_loss: 0.4276\n",
      "\n",
      "Accuracy: 82.25%\n",
      "\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 254.3564\n",
      "Function value obtained: -0.8225\n",
      "Current minimum: -0.8382\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "learning rate: 1.8e-05\n",
      "num_dense_nodes: 75\n",
      "dropout: 0.6573157119426624\n",
      "optimizer_type: Adam\n",
      "epochs: 10\n",
      "Epoch 1/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 901ms/step - accuracy: 0.5749 - loss: 0.6910 - val_accuracy: 0.6280 - val_loss: 0.6335\n",
      "Epoch 2/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.7512 - loss: 0.4899 - val_accuracy: 0.7500 - val_loss: 0.5042\n",
      "Epoch 3/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8164 - loss: 0.4198 - val_accuracy: 0.7935 - val_loss: 0.4312\n",
      "Epoch 4/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 175ms/step - accuracy: 0.8599 - loss: 0.3538 - val_accuracy: 0.8333 - val_loss: 0.3793\n",
      "Epoch 5/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8841 - loss: 0.2918 - val_accuracy: 0.8357 - val_loss: 0.3579\n",
      "Epoch 6/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9082 - loss: 0.2506 - val_accuracy: 0.8478 - val_loss: 0.3403\n",
      "Epoch 7/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 159ms/step - accuracy: 0.9372 - loss: 0.1996 - val_accuracy: 0.8502 - val_loss: 0.3411\n",
      "Epoch 8/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9275 - loss: 0.1895 - val_accuracy: 0.8539 - val_loss: 0.3432\n",
      "Epoch 9/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9734 - loss: 0.1260 - val_accuracy: 0.8527 - val_loss: 0.3490\n",
      "Epoch 10/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9493 - loss: 0.1379 - val_accuracy: 0.8551 - val_loss: 0.3472\n",
      "\n",
      "Accuracy: 85.51%\n",
      "\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 192.2683\n",
      "Function value obtained: -0.8551\n",
      "Current minimum: -0.8551\n",
      "Iteration No: 6 started. Searching for the next optimal point.\n",
      "learning rate: 3.1e-05\n",
      "num_dense_nodes: 225\n",
      "dropout: 0.1\n",
      "optimizer_type: SGD\n",
      "epochs: 14\n",
      "Epoch 1/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 783ms/step - accuracy: 0.5495 - loss: 0.6906 - val_accuracy: 0.5266 - val_loss: 0.7493\n",
      "Epoch 2/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.6292 - loss: 0.6535 - val_accuracy: 0.5906 - val_loss: 0.6703\n",
      "Epoch 3/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.6546 - loss: 0.6274 - val_accuracy: 0.6727 - val_loss: 0.6181\n",
      "Epoch 4/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.7283 - loss: 0.5784 - val_accuracy: 0.6981 - val_loss: 0.5933\n",
      "Epoch 5/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.7464 - loss: 0.5657 - val_accuracy: 0.7379 - val_loss: 0.5655\n",
      "Epoch 6/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.7983 - loss: 0.5280 - val_accuracy: 0.7440 - val_loss: 0.5485\n",
      "Epoch 7/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 153ms/step - accuracy: 0.8043 - loss: 0.5109 - val_accuracy: 0.7717 - val_loss: 0.5323\n",
      "Epoch 8/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 152ms/step - accuracy: 0.8080 - loss: 0.4909 - val_accuracy: 0.7850 - val_loss: 0.5136\n",
      "Epoch 9/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 152ms/step - accuracy: 0.8188 - loss: 0.4664 - val_accuracy: 0.7947 - val_loss: 0.4968\n",
      "Epoch 10/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 152ms/step - accuracy: 0.8285 - loss: 0.4419 - val_accuracy: 0.8007 - val_loss: 0.4820\n",
      "Epoch 11/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 152ms/step - accuracy: 0.8333 - loss: 0.4385 - val_accuracy: 0.8068 - val_loss: 0.4708\n",
      "Epoch 12/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.8490 - loss: 0.4134 - val_accuracy: 0.8116 - val_loss: 0.4592\n",
      "Epoch 13/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.8418 - loss: 0.4126 - val_accuracy: 0.8225 - val_loss: 0.4471\n",
      "Epoch 14/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 152ms/step - accuracy: 0.8527 - loss: 0.3996 - val_accuracy: 0.8261 - val_loss: 0.4371\n",
      "\n",
      "Accuracy: 82.61%\n",
      "\n",
      "Iteration No: 6 ended. Search finished for the next optimal point.\n",
      "Time taken: 193.0779\n",
      "Function value obtained: -0.8261\n",
      "Current minimum: -0.8551\n",
      "Iteration No: 7 started. Searching for the next optimal point.\n",
      "learning rate: 1.1e-05\n",
      "num_dense_nodes: 213\n",
      "dropout: 0.44801910919439203\n",
      "optimizer_type: Adam\n",
      "epochs: 19\n",
      "Epoch 1/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 897ms/step - accuracy: 0.5942 - loss: 0.6637 - val_accuracy: 0.6220 - val_loss: 0.6312\n",
      "Epoch 2/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.7621 - loss: 0.5100 - val_accuracy: 0.6558 - val_loss: 0.5760\n",
      "Epoch 3/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.8527 - loss: 0.3800 - val_accuracy: 0.7271 - val_loss: 0.5202\n",
      "Epoch 4/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8696 - loss: 0.3200 - val_accuracy: 0.7935 - val_loss: 0.4344\n",
      "Epoch 5/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.8744 - loss: 0.2888 - val_accuracy: 0.8249 - val_loss: 0.3785\n",
      "Epoch 6/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9167 - loss: 0.2434 - val_accuracy: 0.8406 - val_loss: 0.3574\n",
      "Epoch 7/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9275 - loss: 0.1887 - val_accuracy: 0.8418 - val_loss: 0.3415\n",
      "Epoch 8/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9565 - loss: 0.1418 - val_accuracy: 0.8502 - val_loss: 0.3301\n",
      "Epoch 9/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9553 - loss: 0.1306 - val_accuracy: 0.8623 - val_loss: 0.3264\n",
      "Epoch 10/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9589 - loss: 0.1260 - val_accuracy: 0.8635 - val_loss: 0.3290\n",
      "Epoch 11/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 176ms/step - accuracy: 0.9734 - loss: 0.0858 - val_accuracy: 0.8684 - val_loss: 0.3352\n",
      "Epoch 12/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9831 - loss: 0.0674 - val_accuracy: 0.8659 - val_loss: 0.3427\n",
      "Epoch 13/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9819 - loss: 0.0666 - val_accuracy: 0.8708 - val_loss: 0.3575\n",
      "Epoch 14/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9843 - loss: 0.0552 - val_accuracy: 0.8623 - val_loss: 0.3746\n",
      "Epoch 15/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9879 - loss: 0.0432 - val_accuracy: 0.8611 - val_loss: 0.3861\n",
      "Epoch 16/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 161ms/step - accuracy: 0.9952 - loss: 0.0292 - val_accuracy: 0.8575 - val_loss: 0.4015\n",
      "Epoch 17/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9952 - loss: 0.0191 - val_accuracy: 0.8611 - val_loss: 0.4143\n",
      "Epoch 18/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9867 - loss: 0.0315 - val_accuracy: 0.8671 - val_loss: 0.4277\n",
      "Epoch 19/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 163ms/step - accuracy: 0.9891 - loss: 0.0339 - val_accuracy: 0.8623 - val_loss: 0.4354\n",
      "\n",
      "Accuracy: 86.23%\n",
      "\n",
      "Iteration No: 7 ended. Search finished for the next optimal point.\n",
      "Time taken: 265.0192\n",
      "Function value obtained: -0.8623\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 8 started. Searching for the next optimal point.\n",
      "learning rate: 1.0e-02\n",
      "num_dense_nodes: 225\n",
      "dropout: 0.5371764955662093\n",
      "optimizer_type: Adam\n",
      "epochs: 5\n",
      "Epoch 1/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 916ms/step - accuracy: 0.5085 - loss: 2.1686 - val_accuracy: 0.5000 - val_loss: 30862366720.0000\n",
      "Epoch 2/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.5229 - loss: 0.9534 - val_accuracy: 0.5000 - val_loss: 1039964.3125\n",
      "Epoch 3/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 157ms/step - accuracy: 0.5314 - loss: 0.6880 - val_accuracy: 0.5000 - val_loss: 1107.1851\n",
      "Epoch 4/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.5628 - loss: 0.6875 - val_accuracy: 0.5193 - val_loss: 436.4203\n",
      "Epoch 5/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.5471 - loss: 0.6827 - val_accuracy: 0.6473 - val_loss: 12.5264\n",
      "\n",
      "Accuracy: 64.73%\n",
      "\n",
      "Iteration No: 8 ended. Search finished for the next optimal point.\n",
      "Time taken: 147.3343\n",
      "Function value obtained: -0.6473\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 9 started. Searching for the next optimal point.\n",
      "learning rate: 1.0e-02\n",
      "num_dense_nodes: 105\n",
      "dropout: 0.8\n",
      "optimizer_type: SGD\n",
      "epochs: 20\n",
      "Epoch 1/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 781ms/step - accuracy: 0.6196 - loss: 0.8626 - val_accuracy: 0.5000 - val_loss: 7621516288.0000\n",
      "Epoch 2/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5072 - loss: 1.5040 - val_accuracy: 0.5000 - val_loss: 364600511954944.0000\n",
      "Epoch 3/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5024 - loss: 0.6950 - val_accuracy: 0.5000 - val_loss: 259333931008.0000\n",
      "Epoch 4/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.4988 - loss: 0.7011 - val_accuracy: 0.5000 - val_loss: 0.6936\n",
      "Epoch 5/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 146ms/step - accuracy: 0.5145 - loss: 0.6936 - val_accuracy: 0.5000 - val_loss: 0.6950\n",
      "Epoch 6/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 146ms/step - accuracy: 0.5036 - loss: 0.7049 - val_accuracy: 0.5000 - val_loss: 0.6984\n",
      "Epoch 7/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.4807 - loss: 0.7119 - val_accuracy: 0.5000 - val_loss: 0.7050\n",
      "Epoch 8/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.5097 - loss: 0.7017 - val_accuracy: 0.5000 - val_loss: 0.7214\n",
      "Epoch 9/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 145ms/step - accuracy: 0.5121 - loss: 0.7035 - val_accuracy: 0.5000 - val_loss: 0.7134\n",
      "Epoch 10/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.4879 - loss: 0.7089 - val_accuracy: 0.5000 - val_loss: 0.7024\n",
      "Epoch 11/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.5000 - loss: 0.7134 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 12/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 146ms/step - accuracy: 0.5000 - loss: 0.7208 - val_accuracy: 0.5000 - val_loss: 0.7041\n",
      "Epoch 13/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.5097 - loss: 0.7238 - val_accuracy: 0.5000 - val_loss: 0.7361\n",
      "Epoch 14/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 145ms/step - accuracy: 0.5072 - loss: 0.7173 - val_accuracy: 0.5000 - val_loss: 0.7626\n",
      "Epoch 15/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.4855 - loss: 0.7250 - val_accuracy: 0.5000 - val_loss: 0.7410\n",
      "Epoch 16/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 146ms/step - accuracy: 0.4831 - loss: 0.7459 - val_accuracy: 0.5000 - val_loss: 0.7013\n",
      "Epoch 17/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 146ms/step - accuracy: 0.5000 - loss: 0.7389 - val_accuracy: 0.5000 - val_loss: 0.6961\n",
      "Epoch 18/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 146ms/step - accuracy: 0.5097 - loss: 0.7311 - val_accuracy: 0.5000 - val_loss: 0.7289\n",
      "Epoch 19/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.4710 - loss: 0.7511 - val_accuracy: 0.5000 - val_loss: 0.7716\n",
      "Epoch 20/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 145ms/step - accuracy: 0.4783 - loss: 0.7459 - val_accuracy: 0.5000 - val_loss: 0.7936\n",
      "\n",
      "Accuracy: 50.00%\n",
      "\n",
      "Iteration No: 9 ended. Search finished for the next optimal point.\n",
      "Time taken: 244.7655\n",
      "Function value obtained: -0.5000\n",
      "Current minimum: -0.8623\n",
      "Iteration No: 10 started. Searching for the next optimal point.\n",
      "learning rate: 7.2e-05\n",
      "num_dense_nodes: 157\n",
      "dropout: 0.4788770289578971\n",
      "optimizer_type: Adam\n",
      "epochs: 16\n",
      "Epoch 1/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 899ms/step - accuracy: 0.7488 - loss: 0.5255 - val_accuracy: 0.5894 - val_loss: 0.8700\n",
      "Epoch 2/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.8889 - loss: 0.2740 - val_accuracy: 0.8225 - val_loss: 0.3973\n",
      "Epoch 3/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 175ms/step - accuracy: 0.9227 - loss: 0.1899 - val_accuracy: 0.8418 - val_loss: 0.3846\n",
      "Epoch 4/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9662 - loss: 0.1134 - val_accuracy: 0.8370 - val_loss: 0.4147\n",
      "Epoch 5/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9746 - loss: 0.0676 - val_accuracy: 0.8357 - val_loss: 0.4715\n",
      "Epoch 6/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9891 - loss: 0.0584 - val_accuracy: 0.8587 - val_loss: 0.4721\n",
      "Epoch 7/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 160ms/step - accuracy: 0.9903 - loss: 0.0334 - val_accuracy: 0.8502 - val_loss: 0.5130\n",
      "Epoch 8/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9915 - loss: 0.0269 - val_accuracy: 0.8708 - val_loss: 0.4788\n",
      "Epoch 9/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9879 - loss: 0.0464 - val_accuracy: 0.8551 - val_loss: 0.5026\n",
      "Epoch 10/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9903 - loss: 0.0284 - val_accuracy: 0.8454 - val_loss: 0.5653\n",
      "Epoch 11/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 176ms/step - accuracy: 0.9915 - loss: 0.0305 - val_accuracy: 0.8563 - val_loss: 0.5622\n",
      "Epoch 12/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.9783 - loss: 0.0546 - val_accuracy: 0.8671 - val_loss: 0.4899\n",
      "Epoch 13/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9903 - loss: 0.0305 - val_accuracy: 0.8684 - val_loss: 0.4804\n",
      "Epoch 14/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9891 - loss: 0.0363 - val_accuracy: 0.8671 - val_loss: 0.5532\n",
      "Epoch 15/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9915 - loss: 0.0213 - val_accuracy: 0.8466 - val_loss: 0.6495\n",
      "Epoch 16/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9976 - loss: 0.0140 - val_accuracy: 0.8708 - val_loss: 0.5838\n",
      "\n",
      "Accuracy: 87.08%\n",
      "\n",
      "Iteration No: 10 ended. Search finished for the next optimal point.\n",
      "Time taken: 245.0772\n",
      "Function value obtained: -0.8708\n",
      "Current minimum: -0.8708\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "learning rate: 8.2e-06\n",
      "num_dense_nodes: 155\n",
      "dropout: 0.5319265834632021\n",
      "optimizer_type: Adam\n",
      "epochs: 19\n",
      "Epoch 1/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 862ms/step - accuracy: 0.4952 - loss: 0.8317 - val_accuracy: 0.5217 - val_loss: 0.7810\n",
      "Epoch 2/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.4867 - loss: 0.8435 - val_accuracy: 0.5060 - val_loss: 0.7685\n",
      "Epoch 3/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.4746 - loss: 0.8715 - val_accuracy: 0.5024 - val_loss: 0.7775\n",
      "Epoch 4/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.5157 - loss: 0.8223 - val_accuracy: 0.5000 - val_loss: 0.7949\n",
      "Epoch 5/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.4867 - loss: 0.8530 - val_accuracy: 0.4976 - val_loss: 0.8069\n",
      "Epoch 6/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.4819 - loss: 0.8506 - val_accuracy: 0.4940 - val_loss: 0.8105\n",
      "Epoch 7/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.4614 - loss: 0.8637 - val_accuracy: 0.4976 - val_loss: 0.8144\n",
      "Epoch 8/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.4722 - loss: 0.8354 - val_accuracy: 0.5000 - val_loss: 0.8194\n",
      "Epoch 9/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.4577 - loss: 0.8574 - val_accuracy: 0.4952 - val_loss: 0.8171\n",
      "Epoch 10/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.4903 - loss: 0.8506 - val_accuracy: 0.4940 - val_loss: 0.8134\n",
      "Epoch 11/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.5097 - loss: 0.8265 - val_accuracy: 0.4952 - val_loss: 0.8127\n",
      "Epoch 12/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.4952 - loss: 0.8356 - val_accuracy: 0.4964 - val_loss: 0.8119\n",
      "Epoch 13/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.4915 - loss: 0.8584 - val_accuracy: 0.4964 - val_loss: 0.8121\n",
      "Epoch 14/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.5012 - loss: 0.8299 - val_accuracy: 0.4952 - val_loss: 0.8107\n",
      "Epoch 15/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.4952 - loss: 0.8438 - val_accuracy: 0.4952 - val_loss: 0.8101\n",
      "Epoch 16/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.5060 - loss: 0.8167 - val_accuracy: 0.4940 - val_loss: 0.8105\n",
      "Epoch 17/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.4758 - loss: 0.8398 - val_accuracy: 0.4940 - val_loss: 0.8111\n",
      "Epoch 18/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.4807 - loss: 0.8527 - val_accuracy: 0.4952 - val_loss: 0.8122\n",
      "Epoch 19/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.4819 - loss: 0.8466 - val_accuracy: 0.4952 - val_loss: 0.8103\n",
      "\n",
      "Accuracy: 49.52%\n",
      "\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 258.7847\n",
      "Function value obtained: -0.4952\n",
      "Current minimum: -0.8708\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "learning rate: 5.9e-05\n",
      "num_dense_nodes: 99\n",
      "dropout: 0.6998316704733416\n",
      "optimizer_type: Adam\n",
      "epochs: 16\n",
      "Epoch 1/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 970ms/step - accuracy: 0.6570 - loss: 0.6200 - val_accuracy: 0.6039 - val_loss: 0.6628\n",
      "Epoch 2/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 159ms/step - accuracy: 0.8587 - loss: 0.3568 - val_accuracy: 0.7415 - val_loss: 0.5667\n",
      "Epoch 3/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9046 - loss: 0.2325 - val_accuracy: 0.8164 - val_loss: 0.3890\n",
      "Epoch 4/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9457 - loss: 0.1546 - val_accuracy: 0.8249 - val_loss: 0.4585\n",
      "Epoch 5/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9517 - loss: 0.1273 - val_accuracy: 0.8345 - val_loss: 0.4852\n",
      "Epoch 6/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9734 - loss: 0.0845 - val_accuracy: 0.8647 - val_loss: 0.4389\n",
      "Epoch 7/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9734 - loss: 0.0812 - val_accuracy: 0.8768 - val_loss: 0.4599\n",
      "Epoch 8/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 160ms/step - accuracy: 0.9807 - loss: 0.0633 - val_accuracy: 0.8696 - val_loss: 0.4452\n",
      "Epoch 9/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9819 - loss: 0.0515 - val_accuracy: 0.8732 - val_loss: 0.4778\n",
      "Epoch 10/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9819 - loss: 0.0599 - val_accuracy: 0.8466 - val_loss: 0.5284\n",
      "Epoch 11/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9819 - loss: 0.0627 - val_accuracy: 0.8382 - val_loss: 0.5337\n",
      "Epoch 12/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.9795 - loss: 0.0573 - val_accuracy: 0.8611 - val_loss: 0.4880\n",
      "Epoch 13/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9940 - loss: 0.0248 - val_accuracy: 0.8611 - val_loss: 0.5591\n",
      "Epoch 14/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9867 - loss: 0.0380 - val_accuracy: 0.8611 - val_loss: 0.6290\n",
      "Epoch 15/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9964 - loss: 0.0164 - val_accuracy: 0.8732 - val_loss: 0.5112\n",
      "Epoch 16/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9976 - loss: 0.0081 - val_accuracy: 0.8804 - val_loss: 0.5030\n",
      "\n",
      "Accuracy: 88.04%\n",
      "\n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 247.0322\n",
      "Function value obtained: -0.8804\n",
      "Current minimum: -0.8804\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "learning rate: 8.3e-04\n",
      "num_dense_nodes: 162\n",
      "dropout: 0.8\n",
      "optimizer_type: SGD\n",
      "epochs: 11\n",
      "Epoch 1/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 802ms/step - accuracy: 0.6196 - loss: 0.7066 - val_accuracy: 0.5423 - val_loss: 0.7349\n",
      "Epoch 2/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 150ms/step - accuracy: 0.7983 - loss: 0.4191 - val_accuracy: 0.8128 - val_loss: 0.3921\n",
      "Epoch 3/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8406 - loss: 0.3624 - val_accuracy: 0.8333 - val_loss: 0.3353\n",
      "Epoch 4/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8768 - loss: 0.3019 - val_accuracy: 0.8345 - val_loss: 0.4178\n",
      "Epoch 5/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.9118 - loss: 0.2534 - val_accuracy: 0.8587 - val_loss: 0.3443\n",
      "Epoch 6/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.9118 - loss: 0.2217 - val_accuracy: 0.8285 - val_loss: 0.3790\n",
      "Epoch 7/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.9469 - loss: 0.1578 - val_accuracy: 0.7452 - val_loss: 0.7500\n",
      "Epoch 8/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.9686 - loss: 0.1006 - val_accuracy: 0.8527 - val_loss: 0.5324\n",
      "Epoch 9/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.9517 - loss: 0.1341 - val_accuracy: 0.7560 - val_loss: 1.0992\n",
      "Epoch 10/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.9372 - loss: 0.1677 - val_accuracy: 0.8659 - val_loss: 0.4372\n",
      "Epoch 11/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.9746 - loss: 0.0938 - val_accuracy: 0.8490 - val_loss: 0.4567\n",
      "\n",
      "Accuracy: 84.90%\n",
      "\n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 188.7363\n",
      "Function value obtained: -0.8490\n",
      "Current minimum: -0.8804\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "learning rate: 2.9e-05\n",
      "num_dense_nodes: 162\n",
      "dropout: 0.8\n",
      "optimizer_type: SGD\n",
      "epochs: 13\n",
      "Epoch 1/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 823ms/step - accuracy: 0.5169 - loss: 0.8404 - val_accuracy: 0.5447 - val_loss: 0.6952\n",
      "Epoch 2/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 149ms/step - accuracy: 0.5302 - loss: 0.7941 - val_accuracy: 0.6292 - val_loss: 0.6573\n",
      "Epoch 3/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5821 - loss: 0.7150 - val_accuracy: 0.6606 - val_loss: 0.6258\n",
      "Epoch 4/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.5978 - loss: 0.6744 - val_accuracy: 0.7017 - val_loss: 0.6018\n",
      "Epoch 5/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 152ms/step - accuracy: 0.6486 - loss: 0.6242 - val_accuracy: 0.7379 - val_loss: 0.5729\n",
      "Epoch 6/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.6727 - loss: 0.6108 - val_accuracy: 0.7657 - val_loss: 0.5509\n",
      "Epoch 7/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.6836 - loss: 0.5882 - val_accuracy: 0.7850 - val_loss: 0.5261\n",
      "Epoch 8/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.7174 - loss: 0.5602 - val_accuracy: 0.8092 - val_loss: 0.5059\n",
      "Epoch 9/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.7391 - loss: 0.5293 - val_accuracy: 0.8068 - val_loss: 0.4944\n",
      "Epoch 10/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.7464 - loss: 0.5271 - val_accuracy: 0.8273 - val_loss: 0.4801\n",
      "Epoch 11/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.7633 - loss: 0.4890 - val_accuracy: 0.8261 - val_loss: 0.4614\n",
      "Epoch 12/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.7609 - loss: 0.4990 - val_accuracy: 0.8237 - val_loss: 0.4474\n",
      "Epoch 13/13\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.7850 - loss: 0.4637 - val_accuracy: 0.8345 - val_loss: 0.4353\n",
      "\n",
      "Accuracy: 83.45%\n",
      "\n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 203.5446\n",
      "Function value obtained: -0.8345\n",
      "Current minimum: -0.8804\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "learning rate: 3.9e-06\n",
      "num_dense_nodes: 103\n",
      "dropout: 0.5757840503652437\n",
      "optimizer_type: Adam\n",
      "epochs: 5\n",
      "Epoch 1/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 920ms/step - accuracy: 0.5592 - loss: 0.7141 - val_accuracy: 0.5773 - val_loss: 0.6803\n",
      "Epoch 2/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.6244 - loss: 0.6584 - val_accuracy: 0.6606 - val_loss: 0.6326\n",
      "Epoch 3/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.6824 - loss: 0.5967 - val_accuracy: 0.6872 - val_loss: 0.5937\n",
      "Epoch 4/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7452 - loss: 0.5512 - val_accuracy: 0.7295 - val_loss: 0.5611\n",
      "Epoch 5/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.7500 - loss: 0.5193 - val_accuracy: 0.7729 - val_loss: 0.5212\n",
      "\n",
      "Accuracy: 77.29%\n",
      "\n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 145.5901\n",
      "Function value obtained: -0.7729\n",
      "Current minimum: -0.8804\n",
      "Seed:  4\n",
      "BEST ACCURACY:  {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.8804348111152649, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0}\n",
      "hyper_params  [5.944972152169406e-05, 0.6998316704733416, 0.9133009374400047, 0.9359633037030365, 0.9951677135137836, np.int64(16), np.int64(99), np.str_('Adam')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training on seed 0 for this cell\n",
    "\n",
    "seed = 4\n",
    "\n",
    "print('We are currently training on seed:', seed) \n",
    "# for each iteration of the hyperparameter search, return a set of parameters\n",
    "# and feed them into the relevant parts\n",
    "# run training of the model for this seed, save with seed num\n",
    "X_train = np.load(f'paper_reading_small_data/trial_{seed}_X_train.npy', allow_pickle=True)\n",
    "y_train = np.load(f'paper_reading_small_data/trial_{seed}_y_train.npy', allow_pickle=True)\n",
    "X_test = np.load(f'paper_reading_small_data/trial_{seed}_X_test.npy', allow_pickle=True)\n",
    "y_test = np.load(f'paper_reading_small_data/trial_{seed}_y_test.npy', allow_pickle=True)\n",
    "\n",
    "path_best_model = 'inception_saved_trial_{}.keras'.format(seed)\n",
    "  \n",
    "@use_named_args(dimensions=space)\n",
    "def fitness(learning_rate, dropout, momentum, beta_1, beta_2,\n",
    "              num_dense_nodes, optimizer_type, epochs):\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('dropout:', dropout)\n",
    "    print('optimizer_type:', optimizer_type)\n",
    "    print('epochs:', epochs)\n",
    "\n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = make_a_model(learning_rate=learning_rate, \n",
    "                         dropout=dropout, \n",
    "                         momentum=momentum, \n",
    "                         beta_1=beta_1, beta_2=beta_2,\n",
    "                         num_dense_nodes=num_dense_nodes, \n",
    "                         optimizer_type=optimizer_type)\n",
    "\n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(x=X_train,\n",
    "                          y=y_train,\n",
    "                          epochs=epochs,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data= (X_test,y_test))\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    # auc_val = history.history['val_auc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_accuracy\n",
    "\n",
    "    if accuracy > best_accuracy[seed]:\n",
    "      # Save the new model to harddisk in the recommended Keras format\n",
    "      model_path = os.path.join('DataSplitted', path_best_model)\n",
    "      model.save(model_path)\n",
    "    \n",
    "\n",
    "      # Update the classification accuracy.\n",
    "      best_accuracy[seed] = accuracy\n",
    "      # best_auc = auc_val\n",
    "          \n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "\n",
    "    import gc\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    try:\n",
    "      tf.config.experimental.reset_memory_stats(\"GPU:0\")\n",
    "    except:\n",
    "      pass  # in case older TF version\n",
    "    return -accuracy\n",
    "\n",
    "  \n",
    "#This conducts the hyperparameter search over each data split for details see: https://scikit-optimize.github.io/#skopt.gp_minimize\n",
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=space,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=15,\n",
    "\t\t\t    n_random_starts = 5,\n",
    "                            verbose = True)\n",
    "print('Seed: ',seed)\n",
    "print(\"BEST ACCURACY: \", best_accuracy)\n",
    "print('hyper_params ', search_result.x)\n",
    "\n",
    "del X_train, y_train, X_test, y_test \n",
    "\n",
    "import gc\n",
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# GradCAM and Kernel SHAP Experiments\n",
    "os.chdir(home_directory)\n",
    "os.chdir('paper-reading-analysis')\n",
    "\n",
    "# Library with the methods that I needed\n",
    "import gradcam_shap\n",
    "import scipy\n",
    "\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "from tf_keras_vis.utils.scores import CategoricalScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 22:36:52.522631: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(home_directory)\n",
    "os.chdir('DNNorDermatologist')\n",
    "os.chdir('DataSplitted')\n",
    "seed = 4\n",
    "model = load_model(f'inception_saved_trial_{seed}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<InputLayer name=input_layer, built=True>, <Conv2D name=conv2d, built=True>, <BatchNormalization name=batch_normalization, built=True>, <Activation name=activation, built=True>, <Conv2D name=conv2d_1, built=True>, <BatchNormalization name=batch_normalization_1, built=True>, <Activation name=activation_1, built=True>, <Conv2D name=conv2d_2, built=True>, <BatchNormalization name=batch_normalization_2, built=True>, <Activation name=activation_2, built=True>, <MaxPooling2D name=max_pooling2d, built=True>, <Conv2D name=conv2d_3, built=True>, <BatchNormalization name=batch_normalization_3, built=True>, <Activation name=activation_3, built=True>, <Conv2D name=conv2d_4, built=True>, <BatchNormalization name=batch_normalization_4, built=True>, <Activation name=activation_4, built=True>, <MaxPooling2D name=max_pooling2d_1, built=True>, <Conv2D name=conv2d_8, built=True>, <BatchNormalization name=batch_normalization_8, built=True>, <Activation name=activation_8, built=True>, <Conv2D name=conv2d_6, built=True>, <Conv2D name=conv2d_9, built=True>, <BatchNormalization name=batch_normalization_6, built=True>, <BatchNormalization name=batch_normalization_9, built=True>, <Activation name=activation_6, built=True>, <Activation name=activation_9, built=True>, <AveragePooling2D name=average_pooling2d, built=True>, <Conv2D name=conv2d_5, built=True>, <Conv2D name=conv2d_7, built=True>, <Conv2D name=conv2d_10, built=True>, <Conv2D name=conv2d_11, built=True>, <BatchNormalization name=batch_normalization_5, built=True>, <BatchNormalization name=batch_normalization_7, built=True>, <BatchNormalization name=batch_normalization_10, built=True>, <BatchNormalization name=batch_normalization_11, built=True>, <Activation name=activation_5, built=True>, <Activation name=activation_7, built=True>, <Activation name=activation_10, built=True>, <Activation name=activation_11, built=True>, <Concatenate name=mixed0, built=True>, <Conv2D name=conv2d_15, built=True>, <BatchNormalization name=batch_normalization_15, built=True>, <Activation name=activation_15, built=True>, <Conv2D name=conv2d_13, built=True>, <Conv2D name=conv2d_16, built=True>, <BatchNormalization name=batch_normalization_13, built=True>, <BatchNormalization name=batch_normalization_16, built=True>, <Activation name=activation_13, built=True>, <Activation name=activation_16, built=True>, <AveragePooling2D name=average_pooling2d_1, built=True>, <Conv2D name=conv2d_12, built=True>, <Conv2D name=conv2d_14, built=True>, <Conv2D name=conv2d_17, built=True>, <Conv2D name=conv2d_18, built=True>, <BatchNormalization name=batch_normalization_12, built=True>, <BatchNormalization name=batch_normalization_14, built=True>, <BatchNormalization name=batch_normalization_17, built=True>, <BatchNormalization name=batch_normalization_18, built=True>, <Activation name=activation_12, built=True>, <Activation name=activation_14, built=True>, <Activation name=activation_17, built=True>, <Activation name=activation_18, built=True>, <Concatenate name=mixed1, built=True>, <Conv2D name=conv2d_22, built=True>, <BatchNormalization name=batch_normalization_22, built=True>, <Activation name=activation_22, built=True>, <Conv2D name=conv2d_20, built=True>, <Conv2D name=conv2d_23, built=True>, <BatchNormalization name=batch_normalization_20, built=True>, <BatchNormalization name=batch_normalization_23, built=True>, <Activation name=activation_20, built=True>, <Activation name=activation_23, built=True>, <AveragePooling2D name=average_pooling2d_2, built=True>, <Conv2D name=conv2d_19, built=True>, <Conv2D name=conv2d_21, built=True>, <Conv2D name=conv2d_24, built=True>, <Conv2D name=conv2d_25, built=True>, <BatchNormalization name=batch_normalization_19, built=True>, <BatchNormalization name=batch_normalization_21, built=True>, <BatchNormalization name=batch_normalization_24, built=True>, <BatchNormalization name=batch_normalization_25, built=True>, <Activation name=activation_19, built=True>, <Activation name=activation_21, built=True>, <Activation name=activation_24, built=True>, <Activation name=activation_25, built=True>, <Concatenate name=mixed2, built=True>, <Conv2D name=conv2d_27, built=True>, <BatchNormalization name=batch_normalization_27, built=True>, <Activation name=activation_27, built=True>, <Conv2D name=conv2d_28, built=True>, <BatchNormalization name=batch_normalization_28, built=True>, <Activation name=activation_28, built=True>, <Conv2D name=conv2d_26, built=True>, <Conv2D name=conv2d_29, built=True>, <BatchNormalization name=batch_normalization_26, built=True>, <BatchNormalization name=batch_normalization_29, built=True>, <Activation name=activation_26, built=True>, <Activation name=activation_29, built=True>, <MaxPooling2D name=max_pooling2d_2, built=True>, <Concatenate name=mixed3, built=True>, <Conv2D name=conv2d_34, built=True>, <BatchNormalization name=batch_normalization_34, built=True>, <Activation name=activation_34, built=True>, <Conv2D name=conv2d_35, built=True>, <BatchNormalization name=batch_normalization_35, built=True>, <Activation name=activation_35, built=True>, <Conv2D name=conv2d_31, built=True>, <Conv2D name=conv2d_36, built=True>, <BatchNormalization name=batch_normalization_31, built=True>, <BatchNormalization name=batch_normalization_36, built=True>, <Activation name=activation_31, built=True>, <Activation name=activation_36, built=True>, <Conv2D name=conv2d_32, built=True>, <Conv2D name=conv2d_37, built=True>, <BatchNormalization name=batch_normalization_32, built=True>, <BatchNormalization name=batch_normalization_37, built=True>, <Activation name=activation_32, built=True>, <Activation name=activation_37, built=True>, <AveragePooling2D name=average_pooling2d_3, built=True>, <Conv2D name=conv2d_30, built=True>, <Conv2D name=conv2d_33, built=True>, <Conv2D name=conv2d_38, built=True>, <Conv2D name=conv2d_39, built=True>, <BatchNormalization name=batch_normalization_30, built=True>, <BatchNormalization name=batch_normalization_33, built=True>, <BatchNormalization name=batch_normalization_38, built=True>, <BatchNormalization name=batch_normalization_39, built=True>, <Activation name=activation_30, built=True>, <Activation name=activation_33, built=True>, <Activation name=activation_38, built=True>, <Activation name=activation_39, built=True>, <Concatenate name=mixed4, built=True>, <Conv2D name=conv2d_44, built=True>, <BatchNormalization name=batch_normalization_44, built=True>, <Activation name=activation_44, built=True>, <Conv2D name=conv2d_45, built=True>, <BatchNormalization name=batch_normalization_45, built=True>, <Activation name=activation_45, built=True>, <Conv2D name=conv2d_41, built=True>, <Conv2D name=conv2d_46, built=True>, <BatchNormalization name=batch_normalization_41, built=True>, <BatchNormalization name=batch_normalization_46, built=True>, <Activation name=activation_41, built=True>, <Activation name=activation_46, built=True>, <Conv2D name=conv2d_42, built=True>, <Conv2D name=conv2d_47, built=True>, <BatchNormalization name=batch_normalization_42, built=True>, <BatchNormalization name=batch_normalization_47, built=True>, <Activation name=activation_42, built=True>, <Activation name=activation_47, built=True>, <AveragePooling2D name=average_pooling2d_4, built=True>, <Conv2D name=conv2d_40, built=True>, <Conv2D name=conv2d_43, built=True>, <Conv2D name=conv2d_48, built=True>, <Conv2D name=conv2d_49, built=True>, <BatchNormalization name=batch_normalization_40, built=True>, <BatchNormalization name=batch_normalization_43, built=True>, <BatchNormalization name=batch_normalization_48, built=True>, <BatchNormalization name=batch_normalization_49, built=True>, <Activation name=activation_40, built=True>, <Activation name=activation_43, built=True>, <Activation name=activation_48, built=True>, <Activation name=activation_49, built=True>, <Concatenate name=mixed5, built=True>, <Conv2D name=conv2d_54, built=True>, <BatchNormalization name=batch_normalization_54, built=True>, <Activation name=activation_54, built=True>, <Conv2D name=conv2d_55, built=True>, <BatchNormalization name=batch_normalization_55, built=True>, <Activation name=activation_55, built=True>, <Conv2D name=conv2d_51, built=True>, <Conv2D name=conv2d_56, built=True>, <BatchNormalization name=batch_normalization_51, built=True>, <BatchNormalization name=batch_normalization_56, built=True>, <Activation name=activation_51, built=True>, <Activation name=activation_56, built=True>, <Conv2D name=conv2d_52, built=True>, <Conv2D name=conv2d_57, built=True>, <BatchNormalization name=batch_normalization_52, built=True>, <BatchNormalization name=batch_normalization_57, built=True>, <Activation name=activation_52, built=True>, <Activation name=activation_57, built=True>, <AveragePooling2D name=average_pooling2d_5, built=True>, <Conv2D name=conv2d_50, built=True>, <Conv2D name=conv2d_53, built=True>, <Conv2D name=conv2d_58, built=True>, <Conv2D name=conv2d_59, built=True>, <BatchNormalization name=batch_normalization_50, built=True>, <BatchNormalization name=batch_normalization_53, built=True>, <BatchNormalization name=batch_normalization_58, built=True>, <BatchNormalization name=batch_normalization_59, built=True>, <Activation name=activation_50, built=True>, <Activation name=activation_53, built=True>, <Activation name=activation_58, built=True>, <Activation name=activation_59, built=True>, <Concatenate name=mixed6, built=True>, <Conv2D name=conv2d_64, built=True>, <BatchNormalization name=batch_normalization_64, built=True>, <Activation name=activation_64, built=True>, <Conv2D name=conv2d_65, built=True>, <BatchNormalization name=batch_normalization_65, built=True>, <Activation name=activation_65, built=True>, <Conv2D name=conv2d_61, built=True>, <Conv2D name=conv2d_66, built=True>, <BatchNormalization name=batch_normalization_61, built=True>, <BatchNormalization name=batch_normalization_66, built=True>, <Activation name=activation_61, built=True>, <Activation name=activation_66, built=True>, <Conv2D name=conv2d_62, built=True>, <Conv2D name=conv2d_67, built=True>, <BatchNormalization name=batch_normalization_62, built=True>, <BatchNormalization name=batch_normalization_67, built=True>, <Activation name=activation_62, built=True>, <Activation name=activation_67, built=True>, <AveragePooling2D name=average_pooling2d_6, built=True>, <Conv2D name=conv2d_60, built=True>, <Conv2D name=conv2d_63, built=True>, <Conv2D name=conv2d_68, built=True>, <Conv2D name=conv2d_69, built=True>, <BatchNormalization name=batch_normalization_60, built=True>, <BatchNormalization name=batch_normalization_63, built=True>, <BatchNormalization name=batch_normalization_68, built=True>, <BatchNormalization name=batch_normalization_69, built=True>, <Activation name=activation_60, built=True>, <Activation name=activation_63, built=True>, <Activation name=activation_68, built=True>, <Activation name=activation_69, built=True>, <Concatenate name=mixed7, built=True>, <Conv2D name=conv2d_72, built=True>, <BatchNormalization name=batch_normalization_72, built=True>, <Activation name=activation_72, built=True>, <Conv2D name=conv2d_73, built=True>, <BatchNormalization name=batch_normalization_73, built=True>, <Activation name=activation_73, built=True>, <Conv2D name=conv2d_70, built=True>, <Conv2D name=conv2d_74, built=True>, <BatchNormalization name=batch_normalization_70, built=True>, <BatchNormalization name=batch_normalization_74, built=True>, <Activation name=activation_70, built=True>, <Activation name=activation_74, built=True>, <Conv2D name=conv2d_71, built=True>, <Conv2D name=conv2d_75, built=True>, <BatchNormalization name=batch_normalization_71, built=True>, <BatchNormalization name=batch_normalization_75, built=True>, <Activation name=activation_71, built=True>, <Activation name=activation_75, built=True>, <MaxPooling2D name=max_pooling2d_3, built=True>, <Concatenate name=mixed8, built=True>, <Conv2D name=conv2d_80, built=True>, <BatchNormalization name=batch_normalization_80, built=True>, <Activation name=activation_80, built=True>, <Conv2D name=conv2d_77, built=True>, <Conv2D name=conv2d_81, built=True>, <BatchNormalization name=batch_normalization_77, built=True>, <BatchNormalization name=batch_normalization_81, built=True>, <Activation name=activation_77, built=True>, <Activation name=activation_81, built=True>, <Conv2D name=conv2d_78, built=True>, <Conv2D name=conv2d_79, built=True>, <Conv2D name=conv2d_82, built=True>, <Conv2D name=conv2d_83, built=True>, <AveragePooling2D name=average_pooling2d_7, built=True>, <Conv2D name=conv2d_76, built=True>, <BatchNormalization name=batch_normalization_78, built=True>, <BatchNormalization name=batch_normalization_79, built=True>, <BatchNormalization name=batch_normalization_82, built=True>, <BatchNormalization name=batch_normalization_83, built=True>, <Conv2D name=conv2d_84, built=True>, <BatchNormalization name=batch_normalization_76, built=True>, <Activation name=activation_78, built=True>, <Activation name=activation_79, built=True>, <Activation name=activation_82, built=True>, <Activation name=activation_83, built=True>, <BatchNormalization name=batch_normalization_84, built=True>, <Activation name=activation_76, built=True>, <Concatenate name=mixed9_0, built=True>, <Concatenate name=concatenate, built=True>, <Activation name=activation_84, built=True>, <Concatenate name=mixed9, built=True>, <Conv2D name=conv2d_89, built=True>, <BatchNormalization name=batch_normalization_89, built=True>, <Activation name=activation_89, built=True>, <Conv2D name=conv2d_86, built=True>, <Conv2D name=conv2d_90, built=True>, <BatchNormalization name=batch_normalization_86, built=True>, <BatchNormalization name=batch_normalization_90, built=True>, <Activation name=activation_86, built=True>, <Activation name=activation_90, built=True>, <Conv2D name=conv2d_87, built=True>, <Conv2D name=conv2d_88, built=True>, <Conv2D name=conv2d_91, built=True>, <Conv2D name=conv2d_92, built=True>, <AveragePooling2D name=average_pooling2d_8, built=True>, <Conv2D name=conv2d_85, built=True>, <BatchNormalization name=batch_normalization_87, built=True>, <BatchNormalization name=batch_normalization_88, built=True>, <BatchNormalization name=batch_normalization_91, built=True>, <BatchNormalization name=batch_normalization_92, built=True>, <Conv2D name=conv2d_93, built=True>, <BatchNormalization name=batch_normalization_85, built=True>, <Activation name=activation_87, built=True>, <Activation name=activation_88, built=True>, <Activation name=activation_91, built=True>, <Activation name=activation_92, built=True>, <BatchNormalization name=batch_normalization_93, built=True>, <Activation name=activation_85, built=True>, <Concatenate name=mixed9_1, built=True>, <Concatenate name=concatenate_1, built=True>, <Activation name=activation_93, built=True>, <Concatenate name=mixed10, built=True>, <GlobalAveragePooling2D name=global_average_pooling2d, built=True>, <Dense name=dense, built=True>, <Dropout name=dropout, built=True>, <Dense name=dense_1, built=True>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import collections.abc\n",
    "collections.Iterable = collections.abc.Iterable\n",
    "\n",
    "from vis.utils import utils\n",
    "from keras import layers, activations\n",
    "\n",
    "#Assorted modifications for model compatibility with gradCAM\n",
    "gmodel = copy.deepcopy(model)\n",
    "\n",
    "print(gmodel.layers)\n",
    "\n",
    "layer_idx = utils.find_layer_idx(gmodel,'dense_1')\n",
    "\n",
    "#swap with softmax with linear classifier for the reasons mentioned above\n",
    "gmodel.layers[layer_idx].activation = activations.linear\n",
    "gmodel = utils.apply_modifications(gmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(home_directory)\n",
    "os.chdir('paper-reading-analysis')\n",
    "%run gradcam_shap.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756593434.328331    5970 service.cc:152] XLA service 0x7f5f7c055930 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1756593434.328381    5970 service.cc:160]   StreamExecutor device (0): Host, Default Version\n",
      "2025-08-30 22:37:14.425853: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1756593436.041709    5970 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step \n",
      "[[np.float32(0.9999862), np.float32(1.3828278e-05)], [np.float32(0.90381753), np.float32(0.096182466)], [np.float32(0.99999905), np.float32(9.536743e-07)], [np.float32(0.99974316), np.float32(0.0002568364)], [np.float32(0.077295326), np.float32(0.9227047)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.9999981), np.float32(1.9073486e-06)], [np.float32(0.9999862), np.float32(1.3828278e-05)], [np.float32(0.999972), np.float32(2.8014183e-05)], [np.float32(0.9802422), np.float32(0.019757807)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.99978036), np.float32(0.00021964312)], [np.float32(0.99998367), np.float32(1.6331673e-05)], [np.float32(0.99999976), np.float32(2.3841858e-07)], [np.float32(0.98588485), np.float32(0.014115155)], [np.float32(0.5735389), np.float32(0.4264611)], [np.float32(0.99923706), np.float32(0.00076293945)], [np.float32(0.9997805), np.float32(0.0002195239)], [np.float32(0.97511566), np.float32(0.024884343)], [np.float32(0.9993079), np.float32(0.00069212914)], [np.float32(0.98269975), np.float32(0.017300248)], [np.float32(0.02082288), np.float32(0.9791771)], [np.float32(0.99653244), np.float32(0.0034675598)], [np.float32(3.8779857e-05), np.float32(0.9999612)], [np.float32(0.9259077), np.float32(0.07409233)], [np.float32(0.5113336), np.float32(0.48866642)], [np.float32(0.9724626), np.float32(0.027537405)], [np.float32(0.8665674), np.float32(0.13343263)], [np.float32(0.0044476176), np.float32(0.99555236)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.67255795), np.float32(0.32744205)], [np.float32(0.6855749), np.float32(0.3144251)], [np.float32(0.99970645), np.float32(0.00029355288)], [np.float32(0.93581027), np.float32(0.06418973)], [np.float32(0.0008501391), np.float32(0.99914986)], [np.float32(0.9977927), np.float32(0.0022072792)], [np.float32(0.9999887), np.float32(1.13248825e-05)], [np.float32(0.9999566), np.float32(4.339218e-05)], [np.float32(0.9987632), np.float32(0.0012367964)], [np.float32(0.94880736), np.float32(0.05119264)], [np.float32(0.99957615), np.float32(0.00042384863)], [np.float32(0.99998116), np.float32(1.8835068e-05)], [np.float32(0.9937972), np.float32(0.006202817)], [np.float32(0.18682386), np.float32(0.81317616)], [np.float32(0.99051815), np.float32(0.009481847)], [np.float32(0.6909598), np.float32(0.3090402)], [np.float32(0.9825569), np.float32(0.01744312)], [np.float32(0.9958067), np.float32(0.004193306)], [np.float32(0.9345696), np.float32(0.0654304)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.99999833), np.float32(1.66893e-06)], [np.float32(0.9843799), np.float32(0.015620112)], [np.float32(0.99905473), np.float32(0.00094527006)], [np.float32(0.9856647), np.float32(0.014335275)], [np.float32(0.5783793), np.float32(0.42162073)], [np.float32(0.9999995), np.float32(4.7683716e-07)], [np.float32(0.9990797), np.float32(0.0009202957)], [np.float32(0.0031173134), np.float32(0.9968827)], [np.float32(0.9999851), np.float32(1.4901161e-05)], [np.float32(0.9999956), np.float32(4.4107437e-06)], [np.float32(0.31174386), np.float32(0.68825614)], [np.float32(0.99960965), np.float32(0.00039035082)], [np.float32(0.888136), np.float32(0.11186397)], [np.float32(0.9999993), np.float32(7.1525574e-07)], [np.float32(0.1632309), np.float32(0.8367691)], [np.float32(0.9999999), np.float32(1.1920929e-07)], [np.float32(0.9997929), np.float32(0.00020712614)], [np.float32(6.557485e-05), np.float32(0.99993443)], [np.float32(0.99970967), np.float32(0.00029033422)], [np.float32(0.9957646), np.float32(0.004235387)], [np.float32(0.9964181), np.float32(0.0035818815)], [np.float32(0.999967), np.float32(3.3020973e-05)], [np.float32(0.99996793), np.float32(3.20673e-05)], [np.float32(0.9977569), np.float32(0.0022431016)], [np.float32(0.9716271), np.float32(0.028372884)], [np.float32(0.9994455), np.float32(0.000554502)], [np.float32(0.9989806), np.float32(0.0010194182)], [np.float32(0.017475892), np.float32(0.9825241)], [np.float32(0.99970406), np.float32(0.00029593706)], [np.float32(0.9999956), np.float32(4.4107437e-06)], [np.float32(0.9999474), np.float32(5.2571297e-05)], [np.float32(0.80632097), np.float32(0.19367903)], [np.float32(0.037264824), np.float32(0.9627352)], [np.float32(0.99972886), np.float32(0.00027114153)], [np.float32(0.95919484), np.float32(0.04080516)], [np.float32(0.9999974), np.float32(2.6226044e-06)], [np.float32(0.9998235), np.float32(0.00017648935)], [np.float32(0.9649011), np.float32(0.03509891)], [np.float32(0.9921503), np.float32(0.007849693)], [np.float32(0.9978903), np.float32(0.0021097064)], [np.float32(0.9238662), np.float32(0.07613379)], [np.float32(0.17884897), np.float32(0.821151)], [np.float32(0.9796794), np.float32(0.020320594)], [np.float32(0.04989579), np.float32(0.95010424)], [np.float32(0.9993581), np.float32(0.0006418824)], [np.float32(0.04794981), np.float32(0.9520502)], [np.float32(0.9999993), np.float32(7.1525574e-07)], [np.float32(0.9983394), np.float32(0.0016605854)], [np.float32(0.9997193), np.float32(0.00028067827)], [np.float32(0.9998066), np.float32(0.00019341707)], [np.float32(0.99248546), np.float32(0.0075145364)], [np.float32(0.99947613), np.float32(0.0005238652)], [np.float32(0.99999666), np.float32(3.33786e-06)], [np.float32(0.93851084), np.float32(0.061489165)], [np.float32(0.99559706), np.float32(0.0044029355)], [np.float32(0.024732022), np.float32(0.975268)], [np.float32(0.13845162), np.float32(0.86154836)], [np.float32(0.9988372), np.float32(0.001162827)], [np.float32(0.9981141), np.float32(0.001885891)], [np.float32(0.9966239), np.float32(0.0033761263)], [np.float32(0.927951), np.float32(0.07204902)], [np.float32(0.111574285), np.float32(0.8884257)], [np.float32(0.59379494), np.float32(0.40620506)], [np.float32(0.99955815), np.float32(0.00044184923)], [np.float32(0.9963147), np.float32(0.0036852956)], [np.float32(0.99850607), np.float32(0.0014939308)], [np.float32(0.9916112), np.float32(0.008388817)], [np.float32(5.1710518e-05), np.float32(0.99994826)], [np.float32(0.99988985), np.float32(0.00011014938)], [np.float32(0.18403067), np.float32(0.81596935)], [np.float32(0.9132762), np.float32(0.086723804)], [np.float32(0.9998192), np.float32(0.00018078089)], [np.float32(0.99984765), np.float32(0.00015234947)], [np.float32(0.9458201), np.float32(0.054179907)], [np.float32(0.94617224), np.float32(0.053827763)], [np.float32(0.28642574), np.float32(0.7135743)], [np.float32(0.9158295), np.float32(0.08417052)], [np.float32(0.99995756), np.float32(4.2438507e-05)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.9998903), np.float32(0.00010967255)], [np.float32(0.0006878646), np.float32(0.99931216)], [np.float32(0.9998266), np.float32(0.00017338991)], [np.float32(0.9999993), np.float32(7.1525574e-07)], [np.float32(0.99992967), np.float32(7.033348e-05)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.9952429), np.float32(0.0047571063)], [np.float32(0.99625504), np.float32(0.0037449598)], [np.float32(0.40885782), np.float32(0.5911422)], [np.float32(0.9981679), np.float32(0.0018321276)], [np.float32(0.9653689), np.float32(0.034631073)], [np.float32(0.99999285), np.float32(7.1525574e-06)], [np.float32(0.99966586), np.float32(0.00033414364)], [np.float32(0.99885607), np.float32(0.0011439323)], [np.float32(0.9999528), np.float32(4.720688e-05)], [np.float32(0.99970526), np.float32(0.00029474497)], [np.float32(0.936538), np.float32(0.06346202)], [np.float32(0.997174), np.float32(0.0028259754)], [np.float32(0.999998), np.float32(2.026558e-06)], [np.float32(0.9998086), np.float32(0.00019139051)], [np.float32(0.9999999), np.float32(1.1920929e-07)], [np.float32(0.9952173), np.float32(0.0047826767)], [np.float32(0.00011284816), np.float32(0.99988717)], [np.float32(0.99916863), np.float32(0.0008313656)], [np.float32(0.07463927), np.float32(0.92536074)], [np.float32(0.96437573), np.float32(0.035624266)], [np.float32(0.99929297), np.float32(0.0007070303)], [np.float32(0.99997056), np.float32(2.9444695e-05)], [np.float32(0.9979989), np.float32(0.0020011067)], [np.float32(0.8572493), np.float32(0.14275068)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.9999279), np.float32(7.212162e-05)], [np.float32(0.99721646), np.float32(0.002783537)], [np.float32(0.7063511), np.float32(0.2936489)], [np.float32(0.17502151), np.float32(0.8249785)], [np.float32(0.99897456), np.float32(0.0010254383)], [np.float32(0.99927884), np.float32(0.0007211566)], [np.float32(0.9970848), np.float32(0.0029152036)], [np.float32(0.98055446), np.float32(0.019445539)], [np.float32(0.0019611276), np.float32(0.9980389)], [np.float32(0.9934715), np.float32(0.0065284967)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.0001080879), np.float32(0.99989194)], [np.float32(0.9927925), np.float32(0.007207513)], [np.float32(0.9994368), np.float32(0.0005632043)], [np.float32(0.99780554), np.float32(0.0021944642)], [np.float32(0.99999785), np.float32(2.1457672e-06)], [np.float32(0.9852496), np.float32(0.014750421)], [np.float32(0.99883527), np.float32(0.0011647344)], [np.float32(0.9999883), np.float32(1.168251e-05)], [np.float32(0.038315367), np.float32(0.96168464)], [np.float32(0.99368334), np.float32(0.006316662)], [np.float32(0.99818987), np.float32(0.0018101335)], [np.float32(0.9808625), np.float32(0.019137502)], [np.float32(0.9999925), np.float32(7.5101852e-06)], [np.float32(0.9953335), np.float32(0.0046665072)], [np.float32(0.14890914), np.float32(0.85109085)], [np.float32(0.9999863), np.float32(1.3709068e-05)], [np.float32(0.03217139), np.float32(0.96782863)], [np.float32(0.9999981), np.float32(1.9073486e-06)], [np.float32(0.99998546), np.float32(1.4543533e-05)], [np.float32(0.99196494), np.float32(0.008035064)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.99706024), np.float32(0.0029397607)], [np.float32(0.23340248), np.float32(0.7665975)], [np.float32(0.9999987), np.float32(1.3113022e-06)], [np.float32(0.9989293), np.float32(0.0010706782)], [np.float32(0.9955296), np.float32(0.004470408)], [np.float32(0.3583526), np.float32(0.6416474)], [np.float32(0.7825505), np.float32(0.21744949)], [np.float32(0.95477706), np.float32(0.045222938)], [np.float32(5.86273e-06), np.float32(0.99999416)], [np.float32(2.0018928e-05), np.float32(0.99998)], [np.float32(2.9357192e-05), np.float32(0.9999706)], [np.float32(0.0015591066), np.float32(0.9984409)], [np.float32(1.7202823e-05), np.float32(0.9999828)], [np.float32(2.442286e-08), np.float32(1.0)], [np.float32(0.76299655), np.float32(0.23700345)], [np.float32(1.4132786e-06), np.float32(0.99999857)], [np.float32(1.4564299e-07), np.float32(0.9999999)], [np.float32(1.9008334e-05), np.float32(0.999981)], [np.float32(2.2998172e-05), np.float32(0.999977)], [np.float32(0.99764603), np.float32(0.0023539662)], [np.float32(0.0012142458), np.float32(0.99878573)], [np.float32(0.9620921), np.float32(0.0379079)], [np.float32(5.1762076e-05), np.float32(0.99994826)], [np.float32(1.5234606e-06), np.float32(0.99999845)], [np.float32(1.3840265e-07), np.float32(0.9999999)], [np.float32(0.0004304799), np.float32(0.99956954)], [np.float32(0.011134029), np.float32(0.988866)], [np.float32(0.0005100041), np.float32(0.99949)], [np.float32(0.042646665), np.float32(0.95735335)], [np.float32(1.9741788e-05), np.float32(0.9999803)], [np.float32(0.055804897), np.float32(0.9441951)], [np.float32(3.0553554e-05), np.float32(0.9999694)], [np.float32(8.426568e-05), np.float32(0.9999157)], [np.float32(0.0021083711), np.float32(0.9978916)], [np.float32(0.07637381), np.float32(0.9236262)], [np.float32(2.3500597e-07), np.float32(0.99999976)], [np.float32(0.725964), np.float32(0.274036)], [np.float32(0.030721042), np.float32(0.96927893)], [np.float32(0.9999641), np.float32(3.5881996e-05)], [np.float32(0.6553696), np.float32(0.34463042)], [np.float32(0.0002376414), np.float32(0.99976236)], [np.float32(0.013022911), np.float32(0.9869771)], [np.float32(0.59290075), np.float32(0.40709925)], [np.float32(0.00016419719), np.float32(0.9998358)], [np.float32(8.414413e-07), np.float32(0.99999917)], [np.float32(1.2201966e-07), np.float32(0.9999999)], [np.float32(1.8887255e-06), np.float32(0.9999981)], [np.float32(0.00015474728), np.float32(0.99984527)], [np.float32(0.00012400582), np.float32(0.999876)], [np.float32(0.016469311), np.float32(0.9835307)], [np.float32(0.15820439), np.float32(0.8417956)], [np.float32(7.695309e-06), np.float32(0.9999923)], [np.float32(8.311441e-09), np.float32(1.0)], [np.float32(0.0043761763), np.float32(0.9956238)], [np.float32(0.00013761553), np.float32(0.9998624)], [np.float32(0.05210388), np.float32(0.9478961)], [np.float32(1.615761e-06), np.float32(0.9999984)], [np.float32(6.3164654e-05), np.float32(0.9999368)], [np.float32(3.2100306e-05), np.float32(0.9999679)], [np.float32(0.00011555261), np.float32(0.9998844)], [np.float32(1.5330404e-05), np.float32(0.9999847)], [np.float32(0.0004606966), np.float32(0.9995393)], [np.float32(0.042148404), np.float32(0.9578516)], [np.float32(0.0009584384), np.float32(0.99904156)], [np.float32(8.100972e-06), np.float32(0.9999919)], [np.float32(6.725109e-10), np.float32(1.0)], [np.float32(0.5365012), np.float32(0.46349877)], [np.float32(0.0010158423), np.float32(0.99898416)], [np.float32(0.0007324768), np.float32(0.9992675)], [np.float32(3.4337318e-05), np.float32(0.99996567)], [np.float32(5.3124448e-05), np.float32(0.9999469)], [np.float32(0.68610424), np.float32(0.31389576)], [np.float32(0.14289927), np.float32(0.8571007)], [np.float32(0.0008170693), np.float32(0.99918294)], [np.float32(4.509891e-08), np.float32(0.99999994)], [np.float32(0.002139681), np.float32(0.9978603)], [np.float32(0.00029925065), np.float32(0.9997007)], [np.float32(1.4140153e-08), np.float32(1.0)], [np.float32(0.00560019), np.float32(0.9943998)], [np.float32(0.12510993), np.float32(0.8748901)], [np.float32(0.002934455), np.float32(0.99706554)], [np.float32(7.0960737e-06), np.float32(0.9999929)], [np.float32(6.1638605e-07), np.float32(0.9999994)], [np.float32(6.6651623e-06), np.float32(0.9999933)], [np.float32(2.3430089e-06), np.float32(0.9999977)], [np.float32(7.329537e-05), np.float32(0.9999267)], [np.float32(8.414619e-06), np.float32(0.9999916)], [np.float32(1.2022743e-06), np.float32(0.9999988)], [np.float32(0.0005453946), np.float32(0.9994546)], [np.float32(0.0001686727), np.float32(0.9998313)], [np.float32(0.00018756602), np.float32(0.9998124)], [np.float32(4.44073e-05), np.float32(0.9999556)], [np.float32(2.6773367e-05), np.float32(0.99997324)], [np.float32(0.9859299), np.float32(0.014070094)], [np.float32(0.00023894985), np.float32(0.99976104)], [np.float32(1.3047561e-07), np.float32(0.9999999)], [np.float32(7.1969266e-06), np.float32(0.9999928)], [np.float32(4.663967e-06), np.float32(0.99999535)], [np.float32(6.086944e-05), np.float32(0.99993914)], [np.float32(8.582298e-08), np.float32(0.99999994)], [np.float32(1.1833938e-05), np.float32(0.99998814)], [np.float32(0.0066114515), np.float32(0.99338853)], [np.float32(8.361435e-08), np.float32(0.99999994)], [np.float32(0.00023065887), np.float32(0.99976933)], [np.float32(1.6053658e-06), np.float32(0.9999984)], [np.float32(0.026558137), np.float32(0.97344184)], [np.float32(0.00094287156), np.float32(0.9990571)], [np.float32(0.0013545094), np.float32(0.9986455)], [np.float32(2.170189e-05), np.float32(0.9999783)], [np.float32(5.0528595e-09), np.float32(1.0)], [np.float32(1.2838252e-06), np.float32(0.9999987)], [np.float32(1.121846e-06), np.float32(0.99999887)], [np.float32(0.97994405), np.float32(0.02005595)], [np.float32(3.983572e-06), np.float32(0.999996)], [np.float32(5.8012443e-05), np.float32(0.999942)], [np.float32(5.750618e-07), np.float32(0.9999994)], [np.float32(1.5093475e-07), np.float32(0.9999998)], [np.float32(0.0001371017), np.float32(0.9998629)], [np.float32(0.6796317), np.float32(0.3203683)], [np.float32(4.4978835e-05), np.float32(0.999955)], [np.float32(5.015095e-07), np.float32(0.9999995)], [np.float32(0.0003598335), np.float32(0.99964017)], [np.float32(1.1031334e-06), np.float32(0.99999887)], [np.float32(0.00065229693), np.float32(0.9993477)], [np.float32(0.14386208), np.float32(0.85613793)], [np.float32(6.488111e-09), np.float32(1.0)], [np.float32(3.115758e-05), np.float32(0.9999688)], [np.float32(3.0628862e-06), np.float32(0.99999696)], [np.float32(2.586982e-06), np.float32(0.99999744)], [np.float32(0.57652885), np.float32(0.42347115)], [np.float32(3.436749e-05), np.float32(0.9999656)], [np.float32(0.26427305), np.float32(0.73572695)], [np.float32(0.0020910476), np.float32(0.99790895)], [np.float32(1.1339154e-05), np.float32(0.9999887)], [np.float32(1.597237e-06), np.float32(0.9999984)], [np.float32(6.3669326e-08), np.float32(0.99999994)], [np.float32(6.260964e-05), np.float32(0.9999374)], [np.float32(2.7808096e-06), np.float32(0.9999972)], [np.float32(0.00022604587), np.float32(0.999774)], [np.float32(5.7705546e-07), np.float32(0.9999994)], [np.float32(1.624821e-05), np.float32(0.9999837)], [np.float32(0.00051496836), np.float32(0.999485)], [np.float32(0.000281503), np.float32(0.9997185)], [np.float32(0.00023543218), np.float32(0.99976456)], [np.float32(9.0338153e-07), np.float32(0.9999991)], [np.float32(1.7423203e-06), np.float32(0.9999983)], [np.float32(0.00071939314), np.float32(0.99928063)], [np.float32(2.646378e-08), np.float32(1.0)], [np.float32(5.539825e-05), np.float32(0.9999446)], [np.float32(1.4120586e-08), np.float32(1.0)], [np.float32(0.00055908284), np.float32(0.9994409)], [np.float32(6.5343265e-05), np.float32(0.9999347)], [np.float32(0.77444196), np.float32(0.22555804)], [np.float32(0.01978472), np.float32(0.98021525)], [np.float32(0.033326615), np.float32(0.9666734)], [np.float32(3.13854e-07), np.float32(0.9999997)], [np.float32(4.476384e-06), np.float32(0.9999955)], [np.float32(0.0076285675), np.float32(0.99237144)], [np.float32(3.0010478e-06), np.float32(0.999997)], [np.float32(0.20244859), np.float32(0.7975514)], [np.float32(6.6289535e-06), np.float32(0.9999934)], [np.float32(0.0043903333), np.float32(0.99560964)], [np.float32(9.499499e-09), np.float32(1.0)], [np.float32(0.002085343), np.float32(0.9979147)], [np.float32(3.593699e-05), np.float32(0.99996406)], [np.float32(8.280412e-06), np.float32(0.9999917)], [np.float32(1.9830048e-07), np.float32(0.9999998)], [np.float32(0.024770895), np.float32(0.9752291)], [np.float32(0.42116365), np.float32(0.5788363)], [np.float32(1.1876221e-05), np.float32(0.99998814)], [np.float32(0.0009465474), np.float32(0.9990535)], [np.float32(1.0607623e-05), np.float32(0.9999894)], [np.float32(7.0692254e-06), np.float32(0.9999929)], [np.float32(1.3795114e-08), np.float32(1.0)], [np.float32(1.5415994e-06), np.float32(0.99999845)], [np.float32(1.3527202e-05), np.float32(0.99998647)], [np.float32(0.118505634), np.float32(0.88149434)], [np.float32(5.2823184e-06), np.float32(0.9999947)], [np.float32(2.0099635e-05), np.float32(0.9999799)], [np.float32(1.6484849e-08), np.float32(1.0)], [np.float32(0.88134617), np.float32(0.118653834)], [np.float32(1.8281859e-05), np.float32(0.9999817)], [np.float32(4.0361138e-06), np.float32(0.99999595)], [np.float32(0.048761886), np.float32(0.9512381)], [np.float32(0.12998381), np.float32(0.8700162)], [np.float32(3.8355756e-05), np.float32(0.9999616)], [np.float32(1.2834837e-12), np.float32(1.0)], [np.float32(8.359762e-05), np.float32(0.9999164)], [np.float32(0.11842327), np.float32(0.8815767)], [np.float32(1.3454279e-05), np.float32(0.9999865)], [np.float32(8.34293e-07), np.float32(0.99999917)], [np.float32(0.9999776), np.float32(2.2411346e-05)], [np.float32(7.135564e-08), np.float32(0.99999994)], [np.float32(4.4980405e-07), np.float32(0.9999995)], [np.float32(0.00083452667), np.float32(0.9991655)], [np.float32(3.859892e-06), np.float32(0.9999961)], [np.float32(1.3085512e-08), np.float32(1.0)], [np.float32(0.000105042716), np.float32(0.999895)], [np.float32(3.6078002e-06), np.float32(0.99999636)], [np.float32(0.0002495521), np.float32(0.99975044)], [np.float32(0.0022467608), np.float32(0.99775326)], [np.float32(1.09744315e-05), np.float32(0.99998903)], [np.float32(0.00031432795), np.float32(0.99968565)], [np.float32(7.806986e-08), np.float32(0.99999994)], [np.float32(9.9164865e-08), np.float32(0.9999999)], [np.float32(7.865195e-05), np.float32(0.9999213)], [np.float32(0.047394402), np.float32(0.9526056)], [np.float32(2.8070872e-07), np.float32(0.9999997)]]\n",
      "Unseen set\n",
      "      ID        Dx         % Mel         % Nev\n",
      "0      0  Melanoma  9.999862e-01  1.382828e-05\n",
      "1      1  Melanoma  9.038175e-01  9.618247e-02\n",
      "2      2  Melanoma  9.999990e-01  9.536743e-07\n",
      "3      3  Melanoma  9.997432e-01  2.568364e-04\n",
      "4      4  Melanoma  7.729533e-02  9.227047e-01\n",
      "..   ...       ...           ...           ...\n",
      "395  395     Nevus  7.806986e-08  9.999999e-01\n",
      "396  396     Nevus  9.916486e-08  9.999999e-01\n",
      "397  397     Nevus  7.865195e-05  9.999213e-01\n",
      "398  398     Nevus  4.739440e-02  9.526056e-01\n",
      "399  399     Nevus  2.807087e-07  9.999997e-01\n",
      "\n",
      "[400 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "os.chdir(home_directory)\n",
    "os.chdir('DNNorDermatologist')\n",
    "\n",
    "# Get the test dataset of 400 - 200 nevi and 200 melanoma\n",
    "test_df = pd.read_pickle('NvAndMelNoDuplicatesFullSizeTestSet.zip')\n",
    "\n",
    "# Change the idx column to be '0' where the diagnosis of the lesion was\n",
    "# nevi, and '1' when the diagnosis is diagnosis\n",
    "test_df['idx'] = np.where(test_df['id'] == 'mel', 1 , 0)\n",
    "\n",
    "# Save a new table 'features' to be test_df, without the idx column\n",
    "features=test_df.drop(columns=['idx'], axis = 1)\n",
    "# Create a new table with just the correct diagnosis (0 for melanoma (or nevi), 1 for nevi (or melanoma))\n",
    "target=test_df['idx']\n",
    "\n",
    "# Change features to be a numpy array of image pixel data ((R, G, B))\n",
    "features = np.asarray(features['image'].tolist())\n",
    "\n",
    "# I want to resize the images \n",
    "features = np.array([cv2.resize(image, (224, 224)) for image in features])\n",
    "\n",
    "# Normalise this data in an alternate table to be values from 0 ... 1\n",
    "# e.g. 255 -> 1, 0 --> 0\n",
    "# Normalises for original prediction and evaluation of model, the SHAP funciton below requires non normalised data\n",
    "# TODO: Standarise this so SHAP takes normalised\n",
    "\n",
    "features2 = features / 255\n",
    "\n",
    "# Convert the data to one-hot encoding\n",
    "target_cat = to_categorical(target, num_classes = 2)\n",
    "\n",
    "# Get predictions for image data\n",
    "# e.g.\n",
    "# Index 0 : [0.9222, 0.0778]\n",
    "# Index 1 : [0.4500, 0.5500]\n",
    "# etc..\n",
    "# This represents likelihood of melanoma and nevi respectively (according to the model)\n",
    "y_pred = model.predict(features2, verbose=1)\n",
    "y_pred = [[value[0], 1-value[0]] for value in y_pred]\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "# Create a new dataframe with entries for each element of the test set\n",
    "# Include an ID, diagnosis, and % likelihoods for each diagnosis from the model\n",
    "df = pd.DataFrame(columns=['ID', 'Dx', '% Mel', '% Nev'],index=[i for i in range(400)])\n",
    "df['ID'] = df.index\n",
    "\n",
    "# Create dictionaries to contain actual diagnosis and probabilities from the model\n",
    "dx_d = {}\n",
    "Pmel = {}\n",
    "Pnev = {}\n",
    "# Take the actual diagnoses from where we retrieved them earlier\n",
    "y_test_cat = target_cat\n",
    "\n",
    "# For each element in the test set:\n",
    "for ind in range(400):\n",
    "    # Append the diagnosis and predictions to their respective dictionaries\n",
    "    if y_test_cat[ind][1] == 1.0:\n",
    "        diagnosis = 'Melanoma'\n",
    "    elif y_test_cat[ind][0] == 1.0:\n",
    "        diagnosis = 'Nevus'\n",
    "    dx_d[ind] = diagnosis\n",
    "    Pmel[ind] = y_pred[ind][0]\n",
    "    Pnev[ind] = y_pred[ind][1]\n",
    "    \n",
    "# Take the above dictionaries and insert them into the data frame\n",
    "df['Dx'] = df['ID'].map(dx_d)\n",
    "df['% Mel'] = df['ID'].map(Pmel)\n",
    "df['% Nev'] = df['ID'].map(Pnev)\n",
    "\n",
    "# Change the prediction likelihoods to be floats \n",
    "df = df.astype({\"% Mel\": float, \"% Nev\": float})\n",
    "\n",
    "#df = df.iloc[id_list]\n",
    "\n",
    "# Print the first 5 entries in the data frame\n",
    "print('Unseen set') \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "os.chdir('paper-reading-analysis')\n",
    "\n",
    "# I want examine the results, so I will just save them\n",
    "df.to_csv(f'predictions_model_{seed}.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
