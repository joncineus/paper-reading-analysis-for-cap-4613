{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Reading Analysis - Code Implementation\n",
    "### Model 2 Training, Hyperparameter Search and Evaluation\n",
    "### Jonathan Alcineus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 21:49:01.133672: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-30 21:49:01.151977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756590541.173956   22108 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756590541.181040   22108 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756590541.200286   22108 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756590541.200326   22108 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756590541.200330   22108 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756590541.200333   22108 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-30 21:49:01.207739: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# These handle the file locations and importing the dataframe from the saved datafile from the authors files\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# These handle the image processing, editing, or displaying that needs to be performed\n",
    "import cv2 \n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "\n",
    "# These handle training the convolutional neural network (CNN) model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "import time\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/DNNorDermatologist\n"
     ]
    }
   ],
   "source": [
    "# This changes the home directory\n",
    "home_directory = os.path.expanduser(\"~\")\n",
    "os.chdir(home_directory)\n",
    "\n",
    "# Then goes to the folder where the data lies\n",
    "os.chdir('DNNorDermatologist')\n",
    "\n",
    "# Ensures that we are in the correct folder\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin to build the classifier and the ranges for each model to find the optimal parameters, or searching through hyperparameters\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "\n",
    "space = [Real(1e-6, 0.01, \"log-uniform\", name='learning_rate'),\n",
    "          Real(0.1, 0.8, name='dropout'),\n",
    "          Real(0.8, 1.0, name='momentum'),\n",
    "          Real(0.9, 1.0, name='beta_1'),\n",
    "          Real(0.99, 1.0, name='beta_2'),\n",
    "          Integer(low=5,high=20, name = 'epochs'),\n",
    "          Integer(low=50, high=225, name='num_dense_nodes'),\n",
    "          Categorical(categories=['SGD', 'Adam'],\n",
    "                             name='optimizer_type')\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The first part to implenment is the creation of random models\n",
    "if not os.path.isdir('suite_of_models'):\n",
    "    os.mkdir('suite_of_models')\n",
    "\n",
    "def make_a_model(learning_rate, dropout, momentum, beta_1, beta_2, num_dense_nodes, optimizer_type):\n",
    "    # Like in the paper the base model for the image classifcation will be imagenet\n",
    "    base_model = InceptionV3(weights='imagenet',input_shape=(224, 224, 3), include_top=False)\n",
    "\n",
    "    # Fine tune the model with extra dense layers\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(num_dense_nodes, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = Dropout(rate=dropout)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Selects a type of model optimizer\n",
    "    if optimizer_type == \"Adam\":\n",
    "        optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "    elif optimizer_type == \"SGD\":\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start off with basic parameters and the batch size for the models\n",
    "batch_size = 16\n",
    "best_accuracy = {} \n",
    "for seed in range(15):\n",
    "  best_accuracy[seed] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are currently training on seed: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "learning rate: 2.1e-05\n",
      "num_dense_nodes: 64\n",
      "dropout: 0.3274425957559373\n",
      "optimizer_type: SGD\n",
      "epochs: 16\n",
      "Epoch 1/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 815ms/step - accuracy: 0.4903 - loss: 0.7654 - val_accuracy: 0.5024 - val_loss: 0.7447\n",
      "Epoch 2/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 147ms/step - accuracy: 0.5242 - loss: 0.7238 - val_accuracy: 0.5157 - val_loss: 0.7103\n",
      "Epoch 3/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 147ms/step - accuracy: 0.5290 - loss: 0.7153 - val_accuracy: 0.5640 - val_loss: 0.6855\n",
      "Epoch 4/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 147ms/step - accuracy: 0.5725 - loss: 0.6903 - val_accuracy: 0.5713 - val_loss: 0.6733\n",
      "Epoch 5/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 147ms/step - accuracy: 0.6051 - loss: 0.6620 - val_accuracy: 0.5978 - val_loss: 0.6623\n",
      "Epoch 6/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 147ms/step - accuracy: 0.6341 - loss: 0.6447 - val_accuracy: 0.6208 - val_loss: 0.6506\n",
      "Epoch 7/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.6461 - loss: 0.6316 - val_accuracy: 0.6546 - val_loss: 0.6382\n",
      "Epoch 8/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.6691 - loss: 0.6204 - val_accuracy: 0.6522 - val_loss: 0.6277\n",
      "Epoch 9/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.6775 - loss: 0.6063 - val_accuracy: 0.6703 - val_loss: 0.6172\n",
      "Epoch 10/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.6993 - loss: 0.5932 - val_accuracy: 0.6920 - val_loss: 0.6063\n",
      "Epoch 11/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.7138 - loss: 0.5752 - val_accuracy: 0.7210 - val_loss: 0.5961\n",
      "Epoch 12/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.7258 - loss: 0.5645 - val_accuracy: 0.7428 - val_loss: 0.5852\n",
      "Epoch 13/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.7379 - loss: 0.5610 - val_accuracy: 0.7548 - val_loss: 0.5753\n",
      "Epoch 14/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.7162 - loss: 0.5648 - val_accuracy: 0.7693 - val_loss: 0.5668\n",
      "Epoch 15/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.7512 - loss: 0.5377 - val_accuracy: 0.7778 - val_loss: 0.5563\n",
      "Epoch 16/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.7633 - loss: 0.5268 - val_accuracy: 0.7826 - val_loss: 0.5476\n",
      "\n",
      "Accuracy: 78.26%\n",
      "\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 211.2636\n",
      "Function value obtained: -0.7826\n",
      "Current minimum: -0.7826\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "learning rate: 4.1e-04\n",
      "num_dense_nodes: 118\n",
      "dropout: 0.31873008880078346\n",
      "optimizer_type: Adam\n",
      "epochs: 14\n",
      "Epoch 1/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 939ms/step - accuracy: 0.7899 - loss: 0.4406 - val_accuracy: 0.6196 - val_loss: 4.0295\n",
      "Epoch 2/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.8442 - loss: 0.3478 - val_accuracy: 0.7548 - val_loss: 1.3310\n",
      "Epoch 3/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.8949 - loss: 0.2758 - val_accuracy: 0.7814 - val_loss: 1.2476\n",
      "Epoch 4/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.9300 - loss: 0.1710 - val_accuracy: 0.7935 - val_loss: 0.7407\n",
      "Epoch 5/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.9469 - loss: 0.1501 - val_accuracy: 0.7355 - val_loss: 1.0821\n",
      "Epoch 6/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9251 - loss: 0.2038 - val_accuracy: 0.8249 - val_loss: 1.1180\n",
      "Epoch 7/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9650 - loss: 0.0981 - val_accuracy: 0.8490 - val_loss: 0.4071\n",
      "Epoch 8/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9432 - loss: 0.1416 - val_accuracy: 0.8309 - val_loss: 0.4911\n",
      "Epoch 9/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9577 - loss: 0.1197 - val_accuracy: 0.6002 - val_loss: 2.9147\n",
      "Epoch 10/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9783 - loss: 0.0707 - val_accuracy: 0.7283 - val_loss: 2.0193\n",
      "Epoch 11/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9686 - loss: 0.0797 - val_accuracy: 0.8285 - val_loss: 1.2413\n",
      "Epoch 12/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9722 - loss: 0.0723 - val_accuracy: 0.8104 - val_loss: 0.6287\n",
      "Epoch 13/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9843 - loss: 0.0471 - val_accuracy: 0.8176 - val_loss: 1.4890\n",
      "Epoch 14/14\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9855 - loss: 0.0359 - val_accuracy: 0.8563 - val_loss: 0.7244\n",
      "\n",
      "Accuracy: 85.63%\n",
      "\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 225.5490\n",
      "Function value obtained: -0.8563\n",
      "Current minimum: -0.8563\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "learning rate: 3.0e-04\n",
      "num_dense_nodes: 186\n",
      "dropout: 0.12724151733443134\n",
      "optimizer_type: Adam\n",
      "epochs: 10\n",
      "Epoch 1/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 961ms/step - accuracy: 0.7886 - loss: 0.4584 - val_accuracy: 0.8140 - val_loss: 1.4976\n",
      "Epoch 2/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.8961 - loss: 0.2583 - val_accuracy: 0.8309 - val_loss: 1.0364\n",
      "Epoch 3/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9324 - loss: 0.1751 - val_accuracy: 0.8406 - val_loss: 0.7210\n",
      "Epoch 4/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9722 - loss: 0.0797 - val_accuracy: 0.7790 - val_loss: 1.3537\n",
      "Epoch 5/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 174ms/step - accuracy: 0.9457 - loss: 0.1546 - val_accuracy: 0.6522 - val_loss: 2.8142\n",
      "Epoch 6/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9686 - loss: 0.0868 - val_accuracy: 0.6896 - val_loss: 2.5569\n",
      "Epoch 7/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9601 - loss: 0.1029 - val_accuracy: 0.8176 - val_loss: 1.2007\n",
      "Epoch 8/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9469 - loss: 0.1467 - val_accuracy: 0.8454 - val_loss: 0.5930\n",
      "Epoch 9/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 158ms/step - accuracy: 0.9758 - loss: 0.0615 - val_accuracy: 0.8068 - val_loss: 0.6513\n",
      "Epoch 10/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9855 - loss: 0.0519 - val_accuracy: 0.8647 - val_loss: 0.4345\n",
      "\n",
      "Accuracy: 86.47%\n",
      "\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 196.7708\n",
      "Function value obtained: -0.8647\n",
      "Current minimum: -0.8647\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "learning rate: 6.2e-04\n",
      "num_dense_nodes: 163\n",
      "dropout: 0.16544177339693286\n",
      "optimizer_type: SGD\n",
      "epochs: 19\n",
      "Epoch 1/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 824ms/step - accuracy: 0.6715 - loss: 0.5660 - val_accuracy: 0.6123 - val_loss: 0.7179\n",
      "Epoch 2/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.8502 - loss: 0.3569 - val_accuracy: 0.5954 - val_loss: 2.3093\n",
      "Epoch 3/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.8454 - loss: 0.3825 - val_accuracy: 0.6896 - val_loss: 2.9100\n",
      "Epoch 4/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8490 - loss: 0.3434 - val_accuracy: 0.5302 - val_loss: 0.6709\n",
      "Epoch 5/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.8708 - loss: 0.3223 - val_accuracy: 0.5097 - val_loss: 0.7485\n",
      "Epoch 6/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8756 - loss: 0.2910 - val_accuracy: 0.5097 - val_loss: 0.9121\n",
      "Epoch 7/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8720 - loss: 0.3251 - val_accuracy: 0.5978 - val_loss: 2.8135\n",
      "Epoch 8/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8611 - loss: 0.3375 - val_accuracy: 0.5314 - val_loss: 1.7659\n",
      "Epoch 9/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.8744 - loss: 0.3163 - val_accuracy: 0.5072 - val_loss: 8.3268\n",
      "Epoch 10/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8684 - loss: 0.3286 - val_accuracy: 0.3659 - val_loss: 5.2536\n",
      "Epoch 11/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.8780 - loss: 0.2793 - val_accuracy: 0.5000 - val_loss: 124.5276\n",
      "Epoch 12/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.8841 - loss: 0.3222 - val_accuracy: 0.4843 - val_loss: 418.3820\n",
      "Epoch 13/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 150ms/step - accuracy: 0.8841 - loss: 0.2991 - val_accuracy: 0.5000 - val_loss: 276.4835\n",
      "Epoch 14/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.8466 - loss: 0.3877 - val_accuracy: 0.5072 - val_loss: 65.4781\n",
      "Epoch 15/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 150ms/step - accuracy: 0.8321 - loss: 0.5020 - val_accuracy: 0.5000 - val_loss: 80.3568\n",
      "Epoch 16/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8321 - loss: 0.3397 - val_accuracy: 0.5000 - val_loss: 223.1142\n",
      "Epoch 17/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.8514 - loss: 0.3993 - val_accuracy: 0.4964 - val_loss: 70.5759\n",
      "Epoch 18/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 150ms/step - accuracy: 0.8345 - loss: 0.3835 - val_accuracy: 0.4915 - val_loss: 161.2218\n",
      "Epoch 19/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.8394 - loss: 0.4972 - val_accuracy: 0.5121 - val_loss: 113.0802\n",
      "\n",
      "Accuracy: 51.21%\n",
      "\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 242.1616\n",
      "Function value obtained: -0.5121\n",
      "Current minimum: -0.8647\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "learning rate: 7.6e-06\n",
      "num_dense_nodes: 155\n",
      "dropout: 0.14641124931032432\n",
      "optimizer_type: SGD\n",
      "epochs: 10\n",
      "Epoch 1/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 818ms/step - accuracy: 0.4771 - loss: 0.7277 - val_accuracy: 0.5290 - val_loss: 0.7105\n",
      "Epoch 2/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.5169 - loss: 0.7149 - val_accuracy: 0.5302 - val_loss: 0.7054\n",
      "Epoch 3/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5181 - loss: 0.7112 - val_accuracy: 0.5254 - val_loss: 0.7045\n",
      "Epoch 4/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5169 - loss: 0.7066 - val_accuracy: 0.5145 - val_loss: 0.7086\n",
      "Epoch 5/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5302 - loss: 0.6938 - val_accuracy: 0.5072 - val_loss: 0.7050\n",
      "Epoch 6/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5254 - loss: 0.6970 - val_accuracy: 0.5242 - val_loss: 0.6997\n",
      "Epoch 7/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5495 - loss: 0.6992 - val_accuracy: 0.5205 - val_loss: 0.6923\n",
      "Epoch 8/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.5229 - loss: 0.6972 - val_accuracy: 0.5411 - val_loss: 0.6875\n",
      "Epoch 9/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.5676 - loss: 0.6783 - val_accuracy: 0.5519 - val_loss: 0.6849\n",
      "Epoch 10/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 150ms/step - accuracy: 0.5568 - loss: 0.6902 - val_accuracy: 0.5713 - val_loss: 0.6810\n",
      "\n",
      "Accuracy: 57.13%\n",
      "\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 164.4097\n",
      "Function value obtained: -0.5713\n",
      "Current minimum: -0.8647\n",
      "Iteration No: 6 started. Searching for the next optimal point.\n",
      "learning rate: 7.9e-03\n",
      "num_dense_nodes: 138\n",
      "dropout: 0.7198138332911741\n",
      "optimizer_type: Adam\n",
      "epochs: 8\n",
      "Epoch 1/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 971ms/step - accuracy: 0.5048 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 2/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 3/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 4/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 5/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 6/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 7/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "Epoch 8/8\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.5000 - loss: nan - val_accuracy: 0.5000 - val_loss: nan\n",
      "\n",
      "Accuracy: 50.00%\n",
      "\n",
      "Iteration No: 6 ended. Search finished for the next optimal point.\n",
      "Time taken: 174.4841\n",
      "Function value obtained: -0.5000\n",
      "Current minimum: -0.8647\n",
      "Iteration No: 7 started. Searching for the next optimal point.\n",
      "learning rate: 1.2e-04\n",
      "num_dense_nodes: 161\n",
      "dropout: 0.44136997817519663\n",
      "optimizer_type: Adam\n",
      "epochs: 19\n",
      "Epoch 1/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 956ms/step - accuracy: 0.7983 - loss: 0.4560 - val_accuracy: 0.7295 - val_loss: 0.6141\n",
      "Epoch 2/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.8961 - loss: 0.2878 - val_accuracy: 0.7621 - val_loss: 0.6158\n",
      "Epoch 3/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9481 - loss: 0.1427 - val_accuracy: 0.7657 - val_loss: 0.7491\n",
      "Epoch 4/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9698 - loss: 0.0857 - val_accuracy: 0.8490 - val_loss: 0.7501\n",
      "Epoch 5/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9758 - loss: 0.0714 - val_accuracy: 0.8357 - val_loss: 0.6287\n",
      "Epoch 6/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9795 - loss: 0.0514 - val_accuracy: 0.8418 - val_loss: 0.7168\n",
      "Epoch 7/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9577 - loss: 0.1212 - val_accuracy: 0.8176 - val_loss: 0.6837\n",
      "Epoch 8/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9722 - loss: 0.0768 - val_accuracy: 0.8575 - val_loss: 0.6108\n",
      "Epoch 9/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9867 - loss: 0.0369 - val_accuracy: 0.8490 - val_loss: 0.6555\n",
      "Epoch 10/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9903 - loss: 0.0228 - val_accuracy: 0.8502 - val_loss: 1.0109\n",
      "Epoch 11/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9771 - loss: 0.0783 - val_accuracy: 0.8466 - val_loss: 0.8259\n",
      "Epoch 12/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9674 - loss: 0.0858 - val_accuracy: 0.8599 - val_loss: 0.8526\n",
      "Epoch 13/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9843 - loss: 0.0352 - val_accuracy: 0.8623 - val_loss: 0.7933\n",
      "Epoch 14/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9964 - loss: 0.0137 - val_accuracy: 0.8563 - val_loss: 0.6674\n",
      "Epoch 15/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9867 - loss: 0.0381 - val_accuracy: 0.8382 - val_loss: 0.7686\n",
      "Epoch 16/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 158ms/step - accuracy: 0.9831 - loss: 0.1061 - val_accuracy: 0.8080 - val_loss: 0.9071\n",
      "Epoch 17/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9758 - loss: 0.0729 - val_accuracy: 0.8466 - val_loss: 0.6166\n",
      "Epoch 18/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9976 - loss: 0.0174 - val_accuracy: 0.8490 - val_loss: 0.6075\n",
      "Epoch 19/19\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 0.8551 - val_loss: 0.6667\n",
      "\n",
      "Accuracy: 85.51%\n",
      "\n",
      "Iteration No: 7 ended. Search finished for the next optimal point.\n",
      "Time taken: 271.0961\n",
      "Function value obtained: -0.8551\n",
      "Current minimum: -0.8647\n",
      "Iteration No: 8 started. Searching for the next optimal point.\n",
      "learning rate: 1.3e-05\n",
      "num_dense_nodes: 188\n",
      "dropout: 0.2951820485100298\n",
      "optimizer_type: Adam\n",
      "epochs: 7\n",
      "Epoch 1/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 941ms/step - accuracy: 0.6280 - loss: 0.6262 - val_accuracy: 0.5688 - val_loss: 0.6892\n",
      "Epoch 2/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.8164 - loss: 0.4478 - val_accuracy: 0.6147 - val_loss: 0.6485\n",
      "Epoch 3/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.8442 - loss: 0.3677 - val_accuracy: 0.7585 - val_loss: 0.4910\n",
      "Epoch 4/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.8901 - loss: 0.2879 - val_accuracy: 0.8043 - val_loss: 0.4267\n",
      "Epoch 5/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9010 - loss: 0.2470 - val_accuracy: 0.8418 - val_loss: 0.3759\n",
      "Epoch 6/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9432 - loss: 0.1797 - val_accuracy: 0.8478 - val_loss: 0.3541\n",
      "Epoch 7/7\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9565 - loss: 0.1374 - val_accuracy: 0.8490 - val_loss: 0.3667\n",
      "\n",
      "Accuracy: 84.90%\n",
      "\n",
      "Iteration No: 8 ended. Search finished for the next optimal point.\n",
      "Time taken: 167.7543\n",
      "Function value obtained: -0.8490\n",
      "Current minimum: -0.8647\n",
      "Iteration No: 9 started. Searching for the next optimal point.\n",
      "learning rate: 3.0e-05\n",
      "num_dense_nodes: 101\n",
      "dropout: 0.54594847770864\n",
      "optimizer_type: SGD\n",
      "epochs: 18\n",
      "Epoch 1/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 833ms/step - accuracy: 0.5556 - loss: 0.7124 - val_accuracy: 0.4807 - val_loss: 0.7458\n",
      "Epoch 2/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.5604 - loss: 0.6975 - val_accuracy: 0.5060 - val_loss: 0.7062\n",
      "Epoch 3/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5773 - loss: 0.6852 - val_accuracy: 0.5857 - val_loss: 0.6727\n",
      "Epoch 4/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 165ms/step - accuracy: 0.5616 - loss: 0.6988 - val_accuracy: 0.6039 - val_loss: 0.6574\n",
      "Epoch 5/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5942 - loss: 0.6758 - val_accuracy: 0.6329 - val_loss: 0.6485\n",
      "Epoch 6/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 149ms/step - accuracy: 0.6159 - loss: 0.6693 - val_accuracy: 0.6268 - val_loss: 0.6442\n",
      "Epoch 7/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.6171 - loss: 0.6448 - val_accuracy: 0.6594 - val_loss: 0.6345\n",
      "Epoch 8/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.6437 - loss: 0.6236 - val_accuracy: 0.6630 - val_loss: 0.6237\n",
      "Epoch 9/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 151ms/step - accuracy: 0.6643 - loss: 0.6196 - val_accuracy: 0.6715 - val_loss: 0.6146\n",
      "Epoch 10/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.6824 - loss: 0.6006 - val_accuracy: 0.6896 - val_loss: 0.6064\n",
      "Epoch 11/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 151ms/step - accuracy: 0.6630 - loss: 0.6113 - val_accuracy: 0.7077 - val_loss: 0.5985\n",
      "Epoch 12/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.6824 - loss: 0.5988 - val_accuracy: 0.7234 - val_loss: 0.5908\n",
      "Epoch 13/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.7077 - loss: 0.5739 - val_accuracy: 0.7222 - val_loss: 0.5839\n",
      "Epoch 14/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.6908 - loss: 0.5784 - val_accuracy: 0.7331 - val_loss: 0.5770\n",
      "Epoch 15/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.6908 - loss: 0.5780 - val_accuracy: 0.7548 - val_loss: 0.5691\n",
      "Epoch 16/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.7258 - loss: 0.5622 - val_accuracy: 0.7609 - val_loss: 0.5617\n",
      "Epoch 17/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.7633 - loss: 0.5294 - val_accuracy: 0.7633 - val_loss: 0.5551\n",
      "Epoch 18/18\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.7162 - loss: 0.5571 - val_accuracy: 0.7766 - val_loss: 0.5477\n",
      "\n",
      "Accuracy: 77.66%\n",
      "\n",
      "Iteration No: 9 ended. Search finished for the next optimal point.\n",
      "Time taken: 235.8831\n",
      "Function value obtained: -0.7766\n",
      "Current minimum: -0.8647\n",
      "Iteration No: 10 started. Searching for the next optimal point.\n",
      "learning rate: 1.2e-04\n",
      "num_dense_nodes: 199\n",
      "dropout: 0.1\n",
      "optimizer_type: Adam\n",
      "epochs: 11\n",
      "Epoch 1/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 940ms/step - accuracy: 0.7790 - loss: 0.4493 - val_accuracy: 0.7222 - val_loss: 0.5750\n",
      "Epoch 2/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9312 - loss: 0.1786 - val_accuracy: 0.7283 - val_loss: 0.8294\n",
      "Epoch 3/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9638 - loss: 0.1139 - val_accuracy: 0.8309 - val_loss: 0.5543\n",
      "Epoch 4/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 158ms/step - accuracy: 0.9771 - loss: 0.0852 - val_accuracy: 0.7609 - val_loss: 0.8611\n",
      "Epoch 5/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 157ms/step - accuracy: 0.9807 - loss: 0.0632 - val_accuracy: 0.8176 - val_loss: 0.7183\n",
      "Epoch 6/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9831 - loss: 0.0428 - val_accuracy: 0.8043 - val_loss: 0.8450\n",
      "Epoch 7/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.9601 - loss: 0.0985 - val_accuracy: 0.8466 - val_loss: 0.9161\n",
      "Epoch 8/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9891 - loss: 0.0510 - val_accuracy: 0.8611 - val_loss: 0.7505\n",
      "Epoch 9/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 159ms/step - accuracy: 0.9903 - loss: 0.0262 - val_accuracy: 0.8756 - val_loss: 0.5766\n",
      "Epoch 10/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9952 - loss: 0.0248 - val_accuracy: 0.8780 - val_loss: 0.5361\n",
      "Epoch 11/11\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9952 - loss: 0.0136 - val_accuracy: 0.8684 - val_loss: 0.5166\n",
      "\n",
      "Accuracy: 86.84%\n",
      "\n",
      "Iteration No: 10 ended. Search finished for the next optimal point.\n",
      "Time taken: 207.4520\n",
      "Function value obtained: -0.8684\n",
      "Current minimum: -0.8684\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "learning rate: 1.0e-02\n",
      "num_dense_nodes: 213\n",
      "dropout: 0.8\n",
      "optimizer_type: SGD\n",
      "epochs: 17\n",
      "Epoch 1/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 842ms/step - accuracy: 0.6171 - loss: 0.7614 - val_accuracy: 0.5000 - val_loss: 732445120.0000\n",
      "Epoch 2/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 147ms/step - accuracy: 0.5700 - loss: 0.9719 - val_accuracy: 0.5000 - val_loss: 9903167488.0000\n",
      "Epoch 3/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5048 - loss: 1.4033 - val_accuracy: 0.5000 - val_loss: 436091648.0000\n",
      "Epoch 4/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 147ms/step - accuracy: 0.5060 - loss: 1.3229 - val_accuracy: 0.5000 - val_loss: 34560.3242\n",
      "Epoch 5/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 147ms/step - accuracy: 0.5060 - loss: 0.6942 - val_accuracy: 0.4952 - val_loss: 0.7422\n",
      "Epoch 6/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.5000 - loss: 0.7020 - val_accuracy: 0.5000 - val_loss: 0.6934\n",
      "Epoch 7/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 144ms/step - accuracy: 0.5097 - loss: 0.7038 - val_accuracy: 0.5000 - val_loss: 0.7004\n",
      "Epoch 8/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 143ms/step - accuracy: 0.5048 - loss: 0.7061 - val_accuracy: 0.5000 - val_loss: 0.7131\n",
      "Epoch 9/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 144ms/step - accuracy: 0.5121 - loss: 0.6952 - val_accuracy: 0.5048 - val_loss: 0.7002\n",
      "Epoch 10/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.4891 - loss: 0.7021 - val_accuracy: 0.5000 - val_loss: 1.7821\n",
      "Epoch 11/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.5012 - loss: 0.6969 - val_accuracy: 0.5229 - val_loss: 0.7953\n",
      "Epoch 12/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.5085 - loss: 0.6929 - val_accuracy: 0.5797 - val_loss: 3.0006\n",
      "Epoch 13/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.5097 - loss: 0.7315 - val_accuracy: 0.4432 - val_loss: 3.9746\n",
      "Epoch 14/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.4903 - loss: 0.6939 - val_accuracy: 0.5000 - val_loss: 3.6409\n",
      "Epoch 15/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.4831 - loss: 0.6938 - val_accuracy: 0.4867 - val_loss: 2.0513\n",
      "Epoch 16/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 144ms/step - accuracy: 0.5024 - loss: 0.6936 - val_accuracy: 0.5000 - val_loss: 1.0851\n",
      "Epoch 17/17\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 145ms/step - accuracy: 0.4976 - loss: 0.6946 - val_accuracy: 0.5000 - val_loss: 0.8029\n",
      "\n",
      "Accuracy: 50.00%\n",
      "\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 219.8882\n",
      "Function value obtained: -0.5000\n",
      "Current minimum: -0.8684\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "learning rate: 1.0e-02\n",
      "num_dense_nodes: 150\n",
      "dropout: 0.1\n",
      "optimizer_type: Adam\n",
      "epochs: 20\n",
      "Epoch 1/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 969ms/step - accuracy: 0.5097 - loss: 0.8989 - val_accuracy: 0.5000 - val_loss: 74984497152.0000\n",
      "Epoch 2/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.5072 - loss: 0.7542 - val_accuracy: 0.5000 - val_loss: 1137.0142\n",
      "Epoch 3/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.5338 - loss: 0.7162 - val_accuracy: 0.5072 - val_loss: 0.6917\n",
      "Epoch 4/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.5169 - loss: 0.6823 - val_accuracy: 0.5121 - val_loss: 0.7269\n",
      "Epoch 5/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.6389 - loss: 0.6615 - val_accuracy: 0.6002 - val_loss: 1683.4014\n",
      "Epoch 6/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.6341 - loss: 0.6338 - val_accuracy: 0.7609 - val_loss: 23.6725\n",
      "Epoch 7/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 157ms/step - accuracy: 0.6473 - loss: 0.6312 - val_accuracy: 0.7355 - val_loss: 3.3341\n",
      "Epoch 8/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.6171 - loss: 0.6181 - val_accuracy: 0.5000 - val_loss: 0.6948\n",
      "Epoch 9/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.6244 - loss: 0.6602 - val_accuracy: 0.5085 - val_loss: 0.7126\n",
      "Epoch 10/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.7331 - loss: 0.5552 - val_accuracy: 0.6703 - val_loss: 0.6062\n",
      "Epoch 11/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.7778 - loss: 0.5087 - val_accuracy: 0.6957 - val_loss: 0.5767\n",
      "Epoch 12/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.7874 - loss: 0.4919 - val_accuracy: 0.6775 - val_loss: 2.8504\n",
      "Epoch 13/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.7862 - loss: 0.4755 - val_accuracy: 0.7476 - val_loss: 1.4012\n",
      "Epoch 14/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 173ms/step - accuracy: 0.8056 - loss: 0.4420 - val_accuracy: 0.7198 - val_loss: 0.9041\n",
      "Epoch 15/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7681 - loss: 0.5190 - val_accuracy: 0.8068 - val_loss: 6.0246\n",
      "Epoch 16/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7862 - loss: 0.4634 - val_accuracy: 0.5435 - val_loss: 27.3761\n",
      "Epoch 17/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.7826 - loss: 0.4759 - val_accuracy: 0.7995 - val_loss: 0.7074\n",
      "Epoch 18/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.7899 - loss: 0.4757 - val_accuracy: 0.5205 - val_loss: 479.3678\n",
      "Epoch 19/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.7947 - loss: 0.4666 - val_accuracy: 0.5362 - val_loss: 1.0800\n",
      "Epoch 20/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 156ms/step - accuracy: 0.7995 - loss: 0.4567 - val_accuracy: 0.7778 - val_loss: 0.8882\n",
      "\n",
      "Accuracy: 77.78%\n",
      "\n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 280.8415\n",
      "Function value obtained: -0.7778\n",
      "Current minimum: -0.8684\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "learning rate: 2.7e-05\n",
      "num_dense_nodes: 78\n",
      "dropout: 0.1\n",
      "optimizer_type: Adam\n",
      "epochs: 20\n",
      "Epoch 1/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 967ms/step - accuracy: 0.6957 - loss: 0.5856 - val_accuracy: 0.5700 - val_loss: 0.7576\n",
      "Epoch 2/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.8514 - loss: 0.3611 - val_accuracy: 0.6848 - val_loss: 0.6170\n",
      "Epoch 3/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.9155 - loss: 0.2360 - val_accuracy: 0.8019 - val_loss: 0.4638\n",
      "Epoch 4/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.9493 - loss: 0.1475 - val_accuracy: 0.8068 - val_loss: 0.4833\n",
      "Epoch 5/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.9734 - loss: 0.0960 - val_accuracy: 0.8309 - val_loss: 0.4021\n",
      "Epoch 6/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9831 - loss: 0.0591 - val_accuracy: 0.8502 - val_loss: 0.4002\n",
      "Epoch 7/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9903 - loss: 0.0490 - val_accuracy: 0.8454 - val_loss: 0.4391\n",
      "Epoch 8/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9867 - loss: 0.0442 - val_accuracy: 0.8599 - val_loss: 0.4345\n",
      "Epoch 9/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9807 - loss: 0.0492 - val_accuracy: 0.8466 - val_loss: 0.4472\n",
      "Epoch 10/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9952 - loss: 0.0340 - val_accuracy: 0.8514 - val_loss: 0.4375\n",
      "Epoch 11/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9952 - loss: 0.0303 - val_accuracy: 0.8454 - val_loss: 0.4804\n",
      "Epoch 12/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9928 - loss: 0.0209 - val_accuracy: 0.8418 - val_loss: 0.4987\n",
      "Epoch 13/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9952 - loss: 0.0206 - val_accuracy: 0.8466 - val_loss: 0.5079\n",
      "Epoch 14/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9952 - loss: 0.0146 - val_accuracy: 0.8345 - val_loss: 0.5659\n",
      "Epoch 15/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9879 - loss: 0.0330 - val_accuracy: 0.8430 - val_loss: 0.5335\n",
      "Epoch 16/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 176ms/step - accuracy: 0.9940 - loss: 0.0150 - val_accuracy: 0.8466 - val_loss: 0.5873\n",
      "Epoch 17/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9976 - loss: 0.0161 - val_accuracy: 0.8466 - val_loss: 0.6003\n",
      "Epoch 18/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.9952 - loss: 0.0119 - val_accuracy: 0.8478 - val_loss: 0.5972\n",
      "Epoch 19/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9928 - loss: 0.0192 - val_accuracy: 0.8599 - val_loss: 0.6480\n",
      "Epoch 20/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9915 - loss: 0.0301 - val_accuracy: 0.8635 - val_loss: 0.5648\n",
      "\n",
      "Accuracy: 86.35%\n",
      "\n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 278.9301\n",
      "Function value obtained: -0.8635\n",
      "Current minimum: -0.8684\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "learning rate: 6.2e-05\n",
      "num_dense_nodes: 146\n",
      "dropout: 0.1\n",
      "optimizer_type: Adam\n",
      "epochs: 20\n",
      "Epoch 1/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 959ms/step - accuracy: 0.5193 - loss: 0.7234 - val_accuracy: 0.5024 - val_loss: 0.8049\n",
      "Epoch 2/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.5229 - loss: 0.7243 - val_accuracy: 0.5072 - val_loss: 0.7737\n",
      "Epoch 3/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 155ms/step - accuracy: 0.5242 - loss: 0.7313 - val_accuracy: 0.5133 - val_loss: 0.7595\n",
      "Epoch 4/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.5036 - loss: 0.7456 - val_accuracy: 0.5109 - val_loss: 0.7542\n",
      "Epoch 5/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.5024 - loss: 0.7281 - val_accuracy: 0.5109 - val_loss: 0.7489\n",
      "Epoch 6/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.5048 - loss: 0.7294 - val_accuracy: 0.5133 - val_loss: 0.7415\n",
      "Epoch 7/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.5145 - loss: 0.7276 - val_accuracy: 0.5157 - val_loss: 0.7429\n",
      "Epoch 8/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.5072 - loss: 0.7363 - val_accuracy: 0.5157 - val_loss: 0.7418\n",
      "Epoch 9/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.5060 - loss: 0.7294 - val_accuracy: 0.5133 - val_loss: 0.7398\n",
      "Epoch 10/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.5133 - loss: 0.7377 - val_accuracy: 0.5193 - val_loss: 0.7398\n",
      "Epoch 11/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.5060 - loss: 0.7308 - val_accuracy: 0.5169 - val_loss: 0.7409\n",
      "Epoch 12/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.5072 - loss: 0.7329 - val_accuracy: 0.5157 - val_loss: 0.7419\n",
      "Epoch 13/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 159ms/step - accuracy: 0.5085 - loss: 0.7300 - val_accuracy: 0.5193 - val_loss: 0.7425\n",
      "Epoch 14/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.5254 - loss: 0.7283 - val_accuracy: 0.5205 - val_loss: 0.7417\n",
      "Epoch 15/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.5036 - loss: 0.7323 - val_accuracy: 0.5169 - val_loss: 0.7417\n",
      "Epoch 16/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.5097 - loss: 0.7305 - val_accuracy: 0.5157 - val_loss: 0.7426\n",
      "Epoch 17/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 160ms/step - accuracy: 0.5024 - loss: 0.7309 - val_accuracy: 0.5157 - val_loss: 0.7427\n",
      "Epoch 18/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.5109 - loss: 0.7304 - val_accuracy: 0.5169 - val_loss: 0.7428\n",
      "Epoch 19/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 160ms/step - accuracy: 0.5181 - loss: 0.7298 - val_accuracy: 0.5193 - val_loss: 0.7424\n",
      "Epoch 20/20\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.5109 - loss: 0.7447 - val_accuracy: 0.5181 - val_loss: 0.7421\n",
      "\n",
      "Accuracy: 51.81%\n",
      "\n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 284.0946\n",
      "Function value obtained: -0.5181\n",
      "Current minimum: -0.8684\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "learning rate: 1.0e-06\n",
      "num_dense_nodes: 89\n",
      "dropout: 0.14085596022049443\n",
      "optimizer_type: SGD\n",
      "epochs: 16\n",
      "Epoch 1/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 855ms/step - accuracy: 0.4855 - loss: 0.7454 - val_accuracy: 0.5290 - val_loss: 0.7349\n",
      "Epoch 2/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.5024 - loss: 0.7483 - val_accuracy: 0.5036 - val_loss: 0.7327\n",
      "Epoch 3/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step - accuracy: 0.4903 - loss: 0.7425 - val_accuracy: 0.5024 - val_loss: 0.7432\n",
      "Epoch 4/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.4976 - loss: 0.7406 - val_accuracy: 0.4867 - val_loss: 0.7441\n",
      "Epoch 5/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 166ms/step - accuracy: 0.5048 - loss: 0.7372 - val_accuracy: 0.4831 - val_loss: 0.7459\n",
      "Epoch 6/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.4952 - loss: 0.7396 - val_accuracy: 0.4638 - val_loss: 0.7508\n",
      "Epoch 7/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.4928 - loss: 0.7412 - val_accuracy: 0.4698 - val_loss: 0.7462\n",
      "Epoch 8/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.5012 - loss: 0.7446 - val_accuracy: 0.4650 - val_loss: 0.7445\n",
      "Epoch 9/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 167ms/step - accuracy: 0.4964 - loss: 0.7344 - val_accuracy: 0.4698 - val_loss: 0.7444\n",
      "Epoch 10/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.4891 - loss: 0.7392 - val_accuracy: 0.4674 - val_loss: 0.7449\n",
      "Epoch 11/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 167ms/step - accuracy: 0.4710 - loss: 0.7536 - val_accuracy: 0.4662 - val_loss: 0.7436\n",
      "Epoch 12/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 152ms/step - accuracy: 0.4855 - loss: 0.7385 - val_accuracy: 0.4662 - val_loss: 0.7442\n",
      "Epoch 13/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 152ms/step - accuracy: 0.4928 - loss: 0.7410 - val_accuracy: 0.4710 - val_loss: 0.7427\n",
      "Epoch 14/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 153ms/step - accuracy: 0.5012 - loss: 0.7313 - val_accuracy: 0.4722 - val_loss: 0.7419\n",
      "Epoch 15/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 152ms/step - accuracy: 0.5012 - loss: 0.7360 - val_accuracy: 0.4710 - val_loss: 0.7413\n",
      "Epoch 16/16\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 152ms/step - accuracy: 0.5012 - loss: 0.7309 - val_accuracy: 0.4710 - val_loss: 0.7405\n",
      "\n",
      "Accuracy: 47.10%\n",
      "\n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 226.1398\n",
      "Function value obtained: -0.4710\n",
      "Current minimum: -0.8684\n",
      "Seed:  1\n",
      "BEST ACCURACY:  {0: 0.0, 1: 0.8683574795722961, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0}\n",
      "hyper_params  [0.00011843392827870588, 0.1, 0.9833822074121836, 0.9, 0.9997196526068891, np.int64(11), np.int64(199), np.str_('Adam')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training on seed 0 for this cell\n",
    "\n",
    "seed = 1\n",
    "\n",
    "print('We are currently training on seed:', seed) \n",
    "# for each iteration of the hyperparameter search, return a set of parameters\n",
    "# and feed them into the relevant parts\n",
    "# run training of the model for this seed, save with seed num\n",
    "X_train = np.load(f'paper_reading_small_data/trial_{seed}_X_train.npy', allow_pickle=True)\n",
    "y_train = np.load(f'paper_reading_small_data/trial_{seed}_y_train.npy', allow_pickle=True)\n",
    "X_test = np.load(f'paper_reading_small_data/trial_{seed}_X_test.npy', allow_pickle=True)\n",
    "y_test = np.load(f'paper_reading_small_data/trial_{seed}_y_test.npy', allow_pickle=True)\n",
    "\n",
    "path_best_model = 'inception_saved_trial_{}.keras'.format(seed)\n",
    "  \n",
    "@use_named_args(dimensions=space)\n",
    "def fitness(learning_rate, dropout, momentum, beta_1, beta_2,\n",
    "              num_dense_nodes, optimizer_type, epochs):\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('dropout:', dropout)\n",
    "    print('optimizer_type:', optimizer_type)\n",
    "    print('epochs:', epochs)\n",
    "\n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = make_a_model(learning_rate=learning_rate, \n",
    "                         dropout=dropout, \n",
    "                         momentum=momentum, \n",
    "                         beta_1=beta_1, beta_2=beta_2,\n",
    "                         num_dense_nodes=num_dense_nodes, \n",
    "                         optimizer_type=optimizer_type)\n",
    "\n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(x=X_train,\n",
    "                          y=y_train,\n",
    "                          epochs=epochs,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data= (X_test,y_test))\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    # auc_val = history.history['val_auc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_accuracy\n",
    "\n",
    "    if accuracy > best_accuracy[seed]:\n",
    "      # Save the new model to harddisk in the recommended Keras format\n",
    "      model_path = os.path.join('DataSplitted', path_best_model)\n",
    "      model.save(model_path)\n",
    "    \n",
    "\n",
    "      # Update the classification accuracy.\n",
    "      best_accuracy[seed] = accuracy\n",
    "      # best_auc = auc_val\n",
    "          \n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "\n",
    "    import gc\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    try:\n",
    "      tf.config.experimental.reset_memory_stats(\"GPU:0\")\n",
    "    except:\n",
    "      pass  # in case older TF version\n",
    "    return -accuracy\n",
    "\n",
    "  \n",
    "#This conducts the hyperparameter search over each data split for details see: https://scikit-optimize.github.io/#skopt.gp_minimize\n",
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=space,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=15,\n",
    "\t\t\t    n_random_starts = 5,\n",
    "                            verbose = True)\n",
    "print('Seed: ',seed)\n",
    "print(\"BEST ACCURACY: \", best_accuracy)\n",
    "print('hyper_params ', search_result.x)\n",
    "\n",
    "del X_train, y_train, X_test, y_test \n",
    "\n",
    "import gc\n",
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# GradCAM and Kernel SHAP Experiments\n",
    "os.chdir(home_directory)\n",
    "os.chdir('paper-reading-analysis')\n",
    "\n",
    "# Library with the methods that I needed\n",
    "import gradcam_shap\n",
    "import scipy\n",
    "\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "from tf_keras_vis.utils.scores import CategoricalScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756590557.434472   22108 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "os.chdir(home_directory)\n",
    "os.chdir('DNNorDermatologist')\n",
    "os.chdir('DataSplitted')\n",
    "model = load_model('inception_saved_trial_1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<InputLayer name=input_layer, built=True>, <Conv2D name=conv2d, built=True>, <BatchNormalization name=batch_normalization, built=True>, <Activation name=activation, built=True>, <Conv2D name=conv2d_1, built=True>, <BatchNormalization name=batch_normalization_1, built=True>, <Activation name=activation_1, built=True>, <Conv2D name=conv2d_2, built=True>, <BatchNormalization name=batch_normalization_2, built=True>, <Activation name=activation_2, built=True>, <MaxPooling2D name=max_pooling2d, built=True>, <Conv2D name=conv2d_3, built=True>, <BatchNormalization name=batch_normalization_3, built=True>, <Activation name=activation_3, built=True>, <Conv2D name=conv2d_4, built=True>, <BatchNormalization name=batch_normalization_4, built=True>, <Activation name=activation_4, built=True>, <MaxPooling2D name=max_pooling2d_1, built=True>, <Conv2D name=conv2d_8, built=True>, <BatchNormalization name=batch_normalization_8, built=True>, <Activation name=activation_8, built=True>, <Conv2D name=conv2d_6, built=True>, <Conv2D name=conv2d_9, built=True>, <BatchNormalization name=batch_normalization_6, built=True>, <BatchNormalization name=batch_normalization_9, built=True>, <Activation name=activation_6, built=True>, <Activation name=activation_9, built=True>, <AveragePooling2D name=average_pooling2d, built=True>, <Conv2D name=conv2d_5, built=True>, <Conv2D name=conv2d_7, built=True>, <Conv2D name=conv2d_10, built=True>, <Conv2D name=conv2d_11, built=True>, <BatchNormalization name=batch_normalization_5, built=True>, <BatchNormalization name=batch_normalization_7, built=True>, <BatchNormalization name=batch_normalization_10, built=True>, <BatchNormalization name=batch_normalization_11, built=True>, <Activation name=activation_5, built=True>, <Activation name=activation_7, built=True>, <Activation name=activation_10, built=True>, <Activation name=activation_11, built=True>, <Concatenate name=mixed0, built=True>, <Conv2D name=conv2d_15, built=True>, <BatchNormalization name=batch_normalization_15, built=True>, <Activation name=activation_15, built=True>, <Conv2D name=conv2d_13, built=True>, <Conv2D name=conv2d_16, built=True>, <BatchNormalization name=batch_normalization_13, built=True>, <BatchNormalization name=batch_normalization_16, built=True>, <Activation name=activation_13, built=True>, <Activation name=activation_16, built=True>, <AveragePooling2D name=average_pooling2d_1, built=True>, <Conv2D name=conv2d_12, built=True>, <Conv2D name=conv2d_14, built=True>, <Conv2D name=conv2d_17, built=True>, <Conv2D name=conv2d_18, built=True>, <BatchNormalization name=batch_normalization_12, built=True>, <BatchNormalization name=batch_normalization_14, built=True>, <BatchNormalization name=batch_normalization_17, built=True>, <BatchNormalization name=batch_normalization_18, built=True>, <Activation name=activation_12, built=True>, <Activation name=activation_14, built=True>, <Activation name=activation_17, built=True>, <Activation name=activation_18, built=True>, <Concatenate name=mixed1, built=True>, <Conv2D name=conv2d_22, built=True>, <BatchNormalization name=batch_normalization_22, built=True>, <Activation name=activation_22, built=True>, <Conv2D name=conv2d_20, built=True>, <Conv2D name=conv2d_23, built=True>, <BatchNormalization name=batch_normalization_20, built=True>, <BatchNormalization name=batch_normalization_23, built=True>, <Activation name=activation_20, built=True>, <Activation name=activation_23, built=True>, <AveragePooling2D name=average_pooling2d_2, built=True>, <Conv2D name=conv2d_19, built=True>, <Conv2D name=conv2d_21, built=True>, <Conv2D name=conv2d_24, built=True>, <Conv2D name=conv2d_25, built=True>, <BatchNormalization name=batch_normalization_19, built=True>, <BatchNormalization name=batch_normalization_21, built=True>, <BatchNormalization name=batch_normalization_24, built=True>, <BatchNormalization name=batch_normalization_25, built=True>, <Activation name=activation_19, built=True>, <Activation name=activation_21, built=True>, <Activation name=activation_24, built=True>, <Activation name=activation_25, built=True>, <Concatenate name=mixed2, built=True>, <Conv2D name=conv2d_27, built=True>, <BatchNormalization name=batch_normalization_27, built=True>, <Activation name=activation_27, built=True>, <Conv2D name=conv2d_28, built=True>, <BatchNormalization name=batch_normalization_28, built=True>, <Activation name=activation_28, built=True>, <Conv2D name=conv2d_26, built=True>, <Conv2D name=conv2d_29, built=True>, <BatchNormalization name=batch_normalization_26, built=True>, <BatchNormalization name=batch_normalization_29, built=True>, <Activation name=activation_26, built=True>, <Activation name=activation_29, built=True>, <MaxPooling2D name=max_pooling2d_2, built=True>, <Concatenate name=mixed3, built=True>, <Conv2D name=conv2d_34, built=True>, <BatchNormalization name=batch_normalization_34, built=True>, <Activation name=activation_34, built=True>, <Conv2D name=conv2d_35, built=True>, <BatchNormalization name=batch_normalization_35, built=True>, <Activation name=activation_35, built=True>, <Conv2D name=conv2d_31, built=True>, <Conv2D name=conv2d_36, built=True>, <BatchNormalization name=batch_normalization_31, built=True>, <BatchNormalization name=batch_normalization_36, built=True>, <Activation name=activation_31, built=True>, <Activation name=activation_36, built=True>, <Conv2D name=conv2d_32, built=True>, <Conv2D name=conv2d_37, built=True>, <BatchNormalization name=batch_normalization_32, built=True>, <BatchNormalization name=batch_normalization_37, built=True>, <Activation name=activation_32, built=True>, <Activation name=activation_37, built=True>, <AveragePooling2D name=average_pooling2d_3, built=True>, <Conv2D name=conv2d_30, built=True>, <Conv2D name=conv2d_33, built=True>, <Conv2D name=conv2d_38, built=True>, <Conv2D name=conv2d_39, built=True>, <BatchNormalization name=batch_normalization_30, built=True>, <BatchNormalization name=batch_normalization_33, built=True>, <BatchNormalization name=batch_normalization_38, built=True>, <BatchNormalization name=batch_normalization_39, built=True>, <Activation name=activation_30, built=True>, <Activation name=activation_33, built=True>, <Activation name=activation_38, built=True>, <Activation name=activation_39, built=True>, <Concatenate name=mixed4, built=True>, <Conv2D name=conv2d_44, built=True>, <BatchNormalization name=batch_normalization_44, built=True>, <Activation name=activation_44, built=True>, <Conv2D name=conv2d_45, built=True>, <BatchNormalization name=batch_normalization_45, built=True>, <Activation name=activation_45, built=True>, <Conv2D name=conv2d_41, built=True>, <Conv2D name=conv2d_46, built=True>, <BatchNormalization name=batch_normalization_41, built=True>, <BatchNormalization name=batch_normalization_46, built=True>, <Activation name=activation_41, built=True>, <Activation name=activation_46, built=True>, <Conv2D name=conv2d_42, built=True>, <Conv2D name=conv2d_47, built=True>, <BatchNormalization name=batch_normalization_42, built=True>, <BatchNormalization name=batch_normalization_47, built=True>, <Activation name=activation_42, built=True>, <Activation name=activation_47, built=True>, <AveragePooling2D name=average_pooling2d_4, built=True>, <Conv2D name=conv2d_40, built=True>, <Conv2D name=conv2d_43, built=True>, <Conv2D name=conv2d_48, built=True>, <Conv2D name=conv2d_49, built=True>, <BatchNormalization name=batch_normalization_40, built=True>, <BatchNormalization name=batch_normalization_43, built=True>, <BatchNormalization name=batch_normalization_48, built=True>, <BatchNormalization name=batch_normalization_49, built=True>, <Activation name=activation_40, built=True>, <Activation name=activation_43, built=True>, <Activation name=activation_48, built=True>, <Activation name=activation_49, built=True>, <Concatenate name=mixed5, built=True>, <Conv2D name=conv2d_54, built=True>, <BatchNormalization name=batch_normalization_54, built=True>, <Activation name=activation_54, built=True>, <Conv2D name=conv2d_55, built=True>, <BatchNormalization name=batch_normalization_55, built=True>, <Activation name=activation_55, built=True>, <Conv2D name=conv2d_51, built=True>, <Conv2D name=conv2d_56, built=True>, <BatchNormalization name=batch_normalization_51, built=True>, <BatchNormalization name=batch_normalization_56, built=True>, <Activation name=activation_51, built=True>, <Activation name=activation_56, built=True>, <Conv2D name=conv2d_52, built=True>, <Conv2D name=conv2d_57, built=True>, <BatchNormalization name=batch_normalization_52, built=True>, <BatchNormalization name=batch_normalization_57, built=True>, <Activation name=activation_52, built=True>, <Activation name=activation_57, built=True>, <AveragePooling2D name=average_pooling2d_5, built=True>, <Conv2D name=conv2d_50, built=True>, <Conv2D name=conv2d_53, built=True>, <Conv2D name=conv2d_58, built=True>, <Conv2D name=conv2d_59, built=True>, <BatchNormalization name=batch_normalization_50, built=True>, <BatchNormalization name=batch_normalization_53, built=True>, <BatchNormalization name=batch_normalization_58, built=True>, <BatchNormalization name=batch_normalization_59, built=True>, <Activation name=activation_50, built=True>, <Activation name=activation_53, built=True>, <Activation name=activation_58, built=True>, <Activation name=activation_59, built=True>, <Concatenate name=mixed6, built=True>, <Conv2D name=conv2d_64, built=True>, <BatchNormalization name=batch_normalization_64, built=True>, <Activation name=activation_64, built=True>, <Conv2D name=conv2d_65, built=True>, <BatchNormalization name=batch_normalization_65, built=True>, <Activation name=activation_65, built=True>, <Conv2D name=conv2d_61, built=True>, <Conv2D name=conv2d_66, built=True>, <BatchNormalization name=batch_normalization_61, built=True>, <BatchNormalization name=batch_normalization_66, built=True>, <Activation name=activation_61, built=True>, <Activation name=activation_66, built=True>, <Conv2D name=conv2d_62, built=True>, <Conv2D name=conv2d_67, built=True>, <BatchNormalization name=batch_normalization_62, built=True>, <BatchNormalization name=batch_normalization_67, built=True>, <Activation name=activation_62, built=True>, <Activation name=activation_67, built=True>, <AveragePooling2D name=average_pooling2d_6, built=True>, <Conv2D name=conv2d_60, built=True>, <Conv2D name=conv2d_63, built=True>, <Conv2D name=conv2d_68, built=True>, <Conv2D name=conv2d_69, built=True>, <BatchNormalization name=batch_normalization_60, built=True>, <BatchNormalization name=batch_normalization_63, built=True>, <BatchNormalization name=batch_normalization_68, built=True>, <BatchNormalization name=batch_normalization_69, built=True>, <Activation name=activation_60, built=True>, <Activation name=activation_63, built=True>, <Activation name=activation_68, built=True>, <Activation name=activation_69, built=True>, <Concatenate name=mixed7, built=True>, <Conv2D name=conv2d_72, built=True>, <BatchNormalization name=batch_normalization_72, built=True>, <Activation name=activation_72, built=True>, <Conv2D name=conv2d_73, built=True>, <BatchNormalization name=batch_normalization_73, built=True>, <Activation name=activation_73, built=True>, <Conv2D name=conv2d_70, built=True>, <Conv2D name=conv2d_74, built=True>, <BatchNormalization name=batch_normalization_70, built=True>, <BatchNormalization name=batch_normalization_74, built=True>, <Activation name=activation_70, built=True>, <Activation name=activation_74, built=True>, <Conv2D name=conv2d_71, built=True>, <Conv2D name=conv2d_75, built=True>, <BatchNormalization name=batch_normalization_71, built=True>, <BatchNormalization name=batch_normalization_75, built=True>, <Activation name=activation_71, built=True>, <Activation name=activation_75, built=True>, <MaxPooling2D name=max_pooling2d_3, built=True>, <Concatenate name=mixed8, built=True>, <Conv2D name=conv2d_80, built=True>, <BatchNormalization name=batch_normalization_80, built=True>, <Activation name=activation_80, built=True>, <Conv2D name=conv2d_77, built=True>, <Conv2D name=conv2d_81, built=True>, <BatchNormalization name=batch_normalization_77, built=True>, <BatchNormalization name=batch_normalization_81, built=True>, <Activation name=activation_77, built=True>, <Activation name=activation_81, built=True>, <Conv2D name=conv2d_78, built=True>, <Conv2D name=conv2d_79, built=True>, <Conv2D name=conv2d_82, built=True>, <Conv2D name=conv2d_83, built=True>, <AveragePooling2D name=average_pooling2d_7, built=True>, <Conv2D name=conv2d_76, built=True>, <BatchNormalization name=batch_normalization_78, built=True>, <BatchNormalization name=batch_normalization_79, built=True>, <BatchNormalization name=batch_normalization_82, built=True>, <BatchNormalization name=batch_normalization_83, built=True>, <Conv2D name=conv2d_84, built=True>, <BatchNormalization name=batch_normalization_76, built=True>, <Activation name=activation_78, built=True>, <Activation name=activation_79, built=True>, <Activation name=activation_82, built=True>, <Activation name=activation_83, built=True>, <BatchNormalization name=batch_normalization_84, built=True>, <Activation name=activation_76, built=True>, <Concatenate name=mixed9_0, built=True>, <Concatenate name=concatenate, built=True>, <Activation name=activation_84, built=True>, <Concatenate name=mixed9, built=True>, <Conv2D name=conv2d_89, built=True>, <BatchNormalization name=batch_normalization_89, built=True>, <Activation name=activation_89, built=True>, <Conv2D name=conv2d_86, built=True>, <Conv2D name=conv2d_90, built=True>, <BatchNormalization name=batch_normalization_86, built=True>, <BatchNormalization name=batch_normalization_90, built=True>, <Activation name=activation_86, built=True>, <Activation name=activation_90, built=True>, <Conv2D name=conv2d_87, built=True>, <Conv2D name=conv2d_88, built=True>, <Conv2D name=conv2d_91, built=True>, <Conv2D name=conv2d_92, built=True>, <AveragePooling2D name=average_pooling2d_8, built=True>, <Conv2D name=conv2d_85, built=True>, <BatchNormalization name=batch_normalization_87, built=True>, <BatchNormalization name=batch_normalization_88, built=True>, <BatchNormalization name=batch_normalization_91, built=True>, <BatchNormalization name=batch_normalization_92, built=True>, <Conv2D name=conv2d_93, built=True>, <BatchNormalization name=batch_normalization_85, built=True>, <Activation name=activation_87, built=True>, <Activation name=activation_88, built=True>, <Activation name=activation_91, built=True>, <Activation name=activation_92, built=True>, <BatchNormalization name=batch_normalization_93, built=True>, <Activation name=activation_85, built=True>, <Concatenate name=mixed9_1, built=True>, <Concatenate name=concatenate_1, built=True>, <Activation name=activation_93, built=True>, <Concatenate name=mixed10, built=True>, <GlobalAveragePooling2D name=global_average_pooling2d, built=True>, <Dense name=dense, built=True>, <Dropout name=dropout, built=True>, <Dense name=dense_1, built=True>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import collections.abc\n",
    "collections.Iterable = collections.abc.Iterable\n",
    "\n",
    "from vis.utils import utils\n",
    "from keras import layers, activations\n",
    "\n",
    "#Assorted modifications for model compatibility with gradCAM\n",
    "gmodel = copy.deepcopy(model)\n",
    "\n",
    "print(gmodel.layers)\n",
    "\n",
    "layer_idx = utils.find_layer_idx(gmodel,'dense_1')\n",
    "\n",
    "#swap with softmax with linear classifier for the reasons mentioned above\n",
    "gmodel.layers[layer_idx].activation = activations.linear\n",
    "gmodel = utils.apply_modifications(gmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(home_directory)\n",
    "os.chdir('paper-reading-analysis')\n",
    "%run gradcam_shap.py\n",
    "\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756590581.288780   22749 service.cc:152] XLA service 0x7f3070003af0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1756590581.288806   22749 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-08-30 21:49:41.405509: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1756590582.563195   22749 cuda_dnn.cc:529] Loaded cuDNN version 91200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 3/13\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756590588.352405   22749 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 743ms/step\n",
      "[[np.float32(0.99994075), np.float32(5.9247017e-05)], [np.float32(0.99989915), np.float32(0.00010085106)], [np.float32(0.99993837), np.float32(6.16312e-05)], [np.float32(0.99972767), np.float32(0.00027233362)], [np.float32(0.15652683), np.float32(0.8434732)], [np.float32(0.9998617), np.float32(0.00013828278)], [np.float32(0.9996674), np.float32(0.00033259392)], [np.float32(0.9999924), np.float32(7.6293945e-06)], [np.float32(0.9999386), np.float32(6.1392784e-05)], [np.float32(0.9754007), np.float32(0.024599314)], [np.float32(0.9999989), np.float32(1.0728836e-06)], [np.float32(0.98270565), np.float32(0.017294347)], [np.float32(0.99853456), np.float32(0.0014654398)], [np.float32(0.99804294), np.float32(0.001957059)], [np.float32(0.9962692), np.float32(0.003730774)], [np.float32(0.6692472), np.float32(0.3307528)], [np.float32(0.99832875), np.float32(0.0016712546)], [np.float32(0.9996394), np.float32(0.0003606081)], [np.float32(0.90041316), np.float32(0.099586844)], [np.float32(0.88954), np.float32(0.11045998)], [np.float32(0.99804556), np.float32(0.0019544363)], [np.float32(0.51989955), np.float32(0.48010045)], [np.float32(0.9999896), np.float32(1.0371208e-05)], [np.float32(0.51792824), np.float32(0.48207176)], [np.float32(0.8245871), np.float32(0.1754129)], [np.float32(0.99998903), np.float32(1.0967255e-05)], [np.float32(0.18683267), np.float32(0.81316733)], [np.float32(0.996691), np.float32(0.0033090115)], [np.float32(0.9855461), np.float32(0.014453888)], [np.float32(0.9999995), np.float32(4.7683716e-07)], [np.float32(0.9998157), np.float32(0.00018429756)], [np.float32(0.9858952), np.float32(0.014104784)], [np.float32(0.999708), np.float32(0.00029200315)], [np.float32(0.42992538), np.float32(0.5700746)], [np.float32(0.99231064), np.float32(0.007689357)], [np.float32(0.00986597), np.float32(0.990134)], [np.float32(0.9988601), np.float32(0.0011398792)], [np.float32(0.9999429), np.float32(5.710125e-05)], [np.float32(0.9996201), np.float32(0.00037992)], [np.float32(0.9723307), np.float32(0.02766931)], [np.float32(0.98800826), np.float32(0.011991739)], [np.float32(0.99965787), np.float32(0.00034213066)], [np.float32(0.99867815), np.float32(0.0013218522)], [np.float32(0.9587899), np.float32(0.041210115)], [np.float32(0.9998754), np.float32(0.00012457371)], [np.float32(0.98530245), np.float32(0.014697552)], [np.float32(0.99997115), np.float32(2.8848648e-05)], [np.float32(0.012207415), np.float32(0.9877926)], [np.float32(0.9991941), np.float32(0.0008059144)], [np.float32(0.9483751), np.float32(0.051624894)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.9630222), np.float32(0.036977828)], [np.float32(0.9999672), np.float32(3.2782555e-05)], [np.float32(0.9453674), np.float32(0.054632604)], [np.float32(0.99772805), np.float32(0.0022719502)], [np.float32(0.9999993), np.float32(7.1525574e-07)], [np.float32(0.99222565), np.float32(0.007774353)], [np.float32(0.0007365573), np.float32(0.99926347)], [np.float32(0.9988073), np.float32(0.001192689)], [np.float32(0.999931), np.float32(6.902218e-05)], [np.float32(0.9384649), np.float32(0.06153512)], [np.float32(0.23355839), np.float32(0.7664416)], [np.float32(0.06048209), np.float32(0.9395179)], [np.float32(0.9997156), np.float32(0.00028437376)], [np.float32(0.98001844), np.float32(0.019981563)], [np.float32(0.9952709), np.float32(0.004729092)], [np.float32(0.9998158), np.float32(0.00018417835)], [np.float32(0.005492024), np.float32(0.99450797)], [np.float32(0.9984657), np.float32(0.0015342832)], [np.float32(0.99767166), np.float32(0.0023283362)], [np.float32(0.9999385), np.float32(6.151199e-05)], [np.float32(0.99998486), np.float32(1.513958e-05)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.9446827), np.float32(0.055317283)], [np.float32(0.99999845), np.float32(1.5497208e-06)], [np.float32(0.99973494), np.float32(0.00026506186)], [np.float32(0.92631793), np.float32(0.07368207)], [np.float32(0.98777974), np.float32(0.0122202635)], [np.float32(0.99963105), np.float32(0.00036895275)], [np.float32(0.9988053), np.float32(0.0011947155)], [np.float32(0.9999951), np.float32(4.887581e-06)], [np.float32(0.20832215), np.float32(0.79167783)], [np.float32(0.9828682), np.float32(0.017131805)], [np.float32(0.9999701), np.float32(2.9921532e-05)], [np.float32(0.9895076), np.float32(0.010492384)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.9998987), np.float32(0.000101327896)], [np.float32(0.99862915), np.float32(0.0013708472)], [np.float32(0.996145), np.float32(0.00385499)], [np.float32(0.9992348), np.float32(0.00076520443)], [np.float32(0.9165059), np.float32(0.08349413)], [np.float32(0.57869273), np.float32(0.42130727)], [np.float32(0.9998888), np.float32(0.00011122227)], [np.float32(0.25561395), np.float32(0.7443861)], [np.float32(0.9970304), np.float32(0.0029696226)], [np.float32(0.53323513), np.float32(0.46676487)], [np.float32(0.99980366), np.float32(0.0001963377)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.99661165), np.float32(0.0033883452)], [np.float32(0.99992025), np.float32(7.9751015e-05)], [np.float32(0.99476105), np.float32(0.0052389503)], [np.float32(0.8468847), np.float32(0.15311527)], [np.float32(0.999864), np.float32(0.0001360178)], [np.float32(0.99839145), np.float32(0.0016085505)], [np.float32(0.96234196), np.float32(0.037658036)], [np.float32(0.013217068), np.float32(0.9867829)], [np.float32(0.999987), np.float32(1.2993813e-05)], [np.float32(0.99997425), np.float32(2.5749207e-05)], [np.float32(0.99359244), np.float32(0.006407559)], [np.float32(0.99994266), np.float32(5.733967e-05)], [np.float32(0.986201), np.float32(0.013799012)], [np.float32(0.1819899), np.float32(0.8180101)], [np.float32(0.541827), np.float32(0.45817298)], [np.float32(0.8169673), np.float32(0.18303269)], [np.float32(0.9999511), np.float32(4.887581e-05)], [np.float32(0.9987092), np.float32(0.0012907982)], [np.float32(0.9825607), np.float32(0.017439306)], [np.float32(0.0010679038), np.float32(0.9989321)], [np.float32(0.99912566), np.float32(0.00087434053)], [np.float32(0.96416616), np.float32(0.035833836)], [np.float32(0.9966769), np.float32(0.0033230782)], [np.float32(0.99813205), np.float32(0.00186795)], [np.float32(0.99649906), np.float32(0.0035009384)], [np.float32(0.9953676), np.float32(0.0046324134)], [np.float32(0.9978538), np.float32(0.0021461844)], [np.float32(0.97812414), np.float32(0.021875858)], [np.float32(0.9940811), np.float32(0.00591892)], [np.float32(0.99996674), np.float32(3.325939e-05)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.9684023), np.float32(0.031597674)], [np.float32(0.000119827106), np.float32(0.9998802)], [np.float32(0.9985234), np.float32(0.0014765859)], [np.float32(1.0), np.float32(0.0)], [np.float32(0.9999769), np.float32(2.3126602e-05)], [np.float32(0.99999964), np.float32(3.5762787e-07)], [np.float32(0.99994934), np.float32(5.0663948e-05)], [np.float32(0.99731857), np.float32(0.0026814342)], [np.float32(0.92360294), np.float32(0.07639706)], [np.float32(0.8845791), np.float32(0.11542088)], [np.float32(0.95992315), np.float32(0.04007685)], [np.float32(0.99715245), np.float32(0.0028475523)], [np.float32(0.9999628), np.float32(3.71933e-05)], [np.float32(0.99956733), np.float32(0.00043267012)], [np.float32(0.9999647), np.float32(3.528595e-05)], [np.float32(0.98493665), np.float32(0.015063345)], [np.float32(0.8595022), np.float32(0.1404978)], [np.float32(0.9234552), np.float32(0.07654482)], [np.float32(0.9994467), np.float32(0.0005533099)], [np.float32(0.99981654), np.float32(0.0001834631)], [np.float32(0.9999938), np.float32(6.198883e-06)], [np.float32(0.9943919), np.float32(0.005608082)], [np.float32(0.028021837), np.float32(0.9719782)], [np.float32(0.9931692), np.float32(0.0068308115)], [np.float32(0.6794612), np.float32(0.32053882)], [np.float32(0.95712626), np.float32(0.04287374)], [np.float32(0.9966241), np.float32(0.0033758879)], [np.float32(0.9999571), np.float32(4.2915344e-05)], [np.float32(0.99485797), np.float32(0.005142033)], [np.float32(0.93176657), np.float32(0.06823343)], [np.float32(0.99999976), np.float32(2.3841858e-07)], [np.float32(0.99794894), np.float32(0.0020510554)], [np.float32(0.99991906), np.float32(8.094311e-05)], [np.float32(0.9992199), np.float32(0.0007801056)], [np.float32(0.9732862), np.float32(0.026713789)], [np.float32(0.99951494), np.float32(0.0004850626)], [np.float32(0.20908056), np.float32(0.7909194)], [np.float32(0.9991819), np.float32(0.00081807375)], [np.float32(0.72822046), np.float32(0.27177954)], [np.float32(0.57765955), np.float32(0.42234045)], [np.float32(0.9995586), np.float32(0.0004413724)], [np.float32(0.99999857), np.float32(1.4305115e-06)], [np.float32(0.3523921), np.float32(0.6476079)], [np.float32(0.99335605), np.float32(0.006643951)], [np.float32(0.993642), np.float32(0.0063580275)], [np.float32(0.99992824), np.float32(7.176399e-05)], [np.float32(0.99999344), np.float32(6.556511e-06)], [np.float32(0.9969326), np.float32(0.0030673742)], [np.float32(0.9987911), np.float32(0.0012089014)], [np.float32(0.9991635), np.float32(0.0008364916)], [np.float32(0.07110031), np.float32(0.9288997)], [np.float32(0.99913514), np.float32(0.0008648634)], [np.float32(0.96197385), np.float32(0.038026154)], [np.float32(0.9998927), np.float32(0.00010728836)], [np.float32(0.9978834), np.float32(0.0021166205)], [np.float32(0.99979746), np.float32(0.00020253658)], [np.float32(0.3419595), np.float32(0.6580405)], [np.float32(0.99711144), np.float32(0.0028885603)], [np.float32(0.029933682), np.float32(0.9700663)], [np.float32(0.9999999), np.float32(1.1920929e-07)], [np.float32(0.94345313), np.float32(0.056546867)], [np.float32(0.9914465), np.float32(0.008553505)], [np.float32(0.9999999), np.float32(1.1920929e-07)], [np.float32(0.99955744), np.float32(0.0004425645)], [np.float32(0.8945032), np.float32(0.105496824)], [np.float32(0.99999917), np.float32(8.34465e-07)], [np.float32(0.98216635), np.float32(0.01783365)], [np.float32(0.9071604), np.float32(0.0928396)], [np.float32(0.34277716), np.float32(0.65722287)], [np.float32(0.9862049), np.float32(0.013795078)], [np.float32(0.9999025), np.float32(9.75132e-05)], [np.float32(0.00030163236), np.float32(0.99969834)], [np.float32(2.2109878e-06), np.float32(0.9999978)], [np.float32(7.5363896e-05), np.float32(0.99992466)], [np.float32(0.023825023), np.float32(0.97617495)], [np.float32(7.2634253e-07), np.float32(0.9999993)], [np.float32(1.36286245e-08), np.float32(1.0)], [np.float32(0.078686304), np.float32(0.9213137)], [np.float32(2.9925977e-05), np.float32(0.9999701)], [np.float32(3.055489e-11), np.float32(1.0)], [np.float32(1.3692743e-06), np.float32(0.9999986)], [np.float32(0.0132017415), np.float32(0.9867983)], [np.float32(0.62436485), np.float32(0.37563515)], [np.float32(0.72418666), np.float32(0.27581334)], [np.float32(0.988252), np.float32(0.011748016)], [np.float32(1.350761e-06), np.float32(0.9999986)], [np.float32(1.0581116e-08), np.float32(1.0)], [np.float32(5.387368e-07), np.float32(0.99999946)], [np.float32(7.450575e-06), np.float32(0.99999255)], [np.float32(0.9651864), np.float32(0.034813583)], [np.float32(2.7216949e-08), np.float32(1.0)], [np.float32(0.006359806), np.float32(0.9936402)], [np.float32(1.157381e-12), np.float32(1.0)], [np.float32(0.20047326), np.float32(0.79952675)], [np.float32(0.05795522), np.float32(0.9420448)], [np.float32(5.6435525e-05), np.float32(0.99994355)], [np.float32(0.0057771318), np.float32(0.9942229)], [np.float32(0.99785763), np.float32(0.0021423697)], [np.float32(0.00017370952), np.float32(0.9998263)], [np.float32(0.99314886), np.float32(0.0068511367)], [np.float32(0.031261954), np.float32(0.968738)], [np.float32(0.07945546), np.float32(0.92054456)], [np.float32(0.8152994), np.float32(0.18470061)], [np.float32(1.08603435e-05), np.float32(0.99998915)], [np.float32(0.0007333373), np.float32(0.9992667)], [np.float32(0.41754162), np.float32(0.5824584)], [np.float32(8.0830596e-09), np.float32(1.0)], [np.float32(2.2088382e-06), np.float32(0.9999978)], [np.float32(1.3621755e-07), np.float32(0.9999999)], [np.float32(2.7728734e-06), np.float32(0.9999972)], [np.float32(3.5314156e-06), np.float32(0.9999965)], [np.float32(0.011223947), np.float32(0.988776)], [np.float32(0.009344291), np.float32(0.9906557)], [np.float32(0.000738772), np.float32(0.9992612)], [np.float32(6.6260696e-07), np.float32(0.99999934)], [np.float32(1.666375e-06), np.float32(0.99999833)], [np.float32(0.912892), np.float32(0.087108016)], [np.float32(5.718251e-09), np.float32(1.0)], [np.float32(0.867978), np.float32(0.13202202)], [np.float32(3.3351715e-13), np.float32(1.0)], [np.float32(0.00011460156), np.float32(0.9998854)], [np.float32(1.3124554e-05), np.float32(0.9999869)], [np.float32(0.025370687), np.float32(0.9746293)], [np.float32(6.1537125e-05), np.float32(0.9999385)], [np.float32(1.0226026e-05), np.float32(0.99998975)], [np.float32(3.249851e-05), np.float32(0.9999675)], [np.float32(7.0783057e-07), np.float32(0.9999993)], [np.float32(1.8064073e-06), np.float32(0.9999982)], [np.float32(1.6122895e-08), np.float32(1.0)], [np.float32(0.22878972), np.float32(0.7712103)], [np.float32(0.30927366), np.float32(0.69072634)], [np.float32(1.2605085e-09), np.float32(1.0)], [np.float32(0.00016087007), np.float32(0.9998391)], [np.float32(2.0378961e-07), np.float32(0.9999998)], [np.float32(0.5558094), np.float32(0.44419062)], [np.float32(4.4508546e-05), np.float32(0.9999555)], [np.float32(2.2599411e-10), np.float32(1.0)], [np.float32(1.1937436e-05), np.float32(0.9999881)], [np.float32(2.6655935e-06), np.float32(0.9999973)], [np.float32(0.00065095857), np.float32(0.99934906)], [np.float32(8.1850005e-10), np.float32(1.0)], [np.float32(0.99724954), np.float32(0.0027504563)], [np.float32(0.51872563), np.float32(0.48127437)], [np.float32(0.8281639), np.float32(0.17183608)], [np.float32(3.9962044e-05), np.float32(0.99996006)], [np.float32(0.00041078115), np.float32(0.9995892)], [np.float32(9.881432e-06), np.float32(0.9999901)], [np.float32(1.1234146e-08), np.float32(1.0)], [np.float32(1.1091475e-05), np.float32(0.9999889)], [np.float32(0.00018539323), np.float32(0.9998146)], [np.float32(1.3030157e-06), np.float32(0.9999987)], [np.float32(0.009656937), np.float32(0.99034303)], [np.float32(5.8058994e-08), np.float32(0.99999994)], [np.float32(0.631831), np.float32(0.368169)], [np.float32(5.837663e-06), np.float32(0.99999416)], [np.float32(2.1592823e-06), np.float32(0.99999785)], [np.float32(0.9780409), np.float32(0.021959126)], [np.float32(0.016381105), np.float32(0.9836189)], [np.float32(6.3174894e-13), np.float32(1.0)], [np.float32(2.513432e-06), np.float32(0.9999975)], [np.float32(1.0836344e-07), np.float32(0.9999999)], [np.float32(3.4131784e-07), np.float32(0.99999964)], [np.float32(1.227454e-20), np.float32(1.0)], [np.float32(1.9760417e-05), np.float32(0.9999802)], [np.float32(0.0008420306), np.float32(0.99915797)], [np.float32(7.223353e-05), np.float32(0.99992776)], [np.float32(2.2575307e-05), np.float32(0.9999774)], [np.float32(4.006657e-06), np.float32(0.999996)], [np.float32(0.0024644565), np.float32(0.9975355)], [np.float32(5.3215956e-09), np.float32(1.0)], [np.float32(0.0028789598), np.float32(0.99712104)], [np.float32(0.009090736), np.float32(0.9909093)], [np.float32(3.4732889e-15), np.float32(1.0)], [np.float32(2.8302685e-10), np.float32(1.0)], [np.float32(6.019215e-10), np.float32(1.0)], [np.float32(0.005185404), np.float32(0.9948146)], [np.float32(0.00011293072), np.float32(0.99988705)], [np.float32(6.4878844e-07), np.float32(0.99999934)], [np.float32(4.555471e-11), np.float32(1.0)], [np.float32(7.230836e-09), np.float32(1.0)], [np.float32(1.4497109e-12), np.float32(1.0)], [np.float32(0.753072), np.float32(0.24692798)], [np.float32(3.691145e-07), np.float32(0.99999964)], [np.float32(1.2748354e-11), np.float32(1.0)], [np.float32(5.2464975e-06), np.float32(0.99999475)], [np.float32(5.2268575e-07), np.float32(0.99999946)], [np.float32(7.2929595e-07), np.float32(0.9999993)], [np.float32(0.92626727), np.float32(0.073732734)], [np.float32(5.5713538e-11), np.float32(1.0)], [np.float32(0.019652946), np.float32(0.98034704)], [np.float32(1.226551e-08), np.float32(1.0)], [np.float32(1.414811e-08), np.float32(1.0)], [np.float32(0.6801928), np.float32(0.31980717)], [np.float32(0.0008514556), np.float32(0.99914855)], [np.float32(0.9031538), np.float32(0.09684622)], [np.float32(9.92834e-06), np.float32(0.99999005)], [np.float32(0.0017863577), np.float32(0.99821365)], [np.float32(5.658306e-06), np.float32(0.99999434)], [np.float32(2.9864594e-07), np.float32(0.9999997)], [np.float32(5.8640385e-06), np.float32(0.99999416)], [np.float32(3.2857224e-05), np.float32(0.99996716)], [np.float32(0.023553153), np.float32(0.97644687)], [np.float32(1.0472339e-07), np.float32(0.9999999)], [np.float32(9.557693e-09), np.float32(1.0)], [np.float32(2.2694255e-07), np.float32(0.99999976)], [np.float32(0.010042893), np.float32(0.9899571)], [np.float32(1.1153144e-08), np.float32(1.0)], [np.float32(0.01061949), np.float32(0.98938054)], [np.float32(7.616063e-07), np.float32(0.9999992)], [np.float32(0.03770875), np.float32(0.96229124)], [np.float32(6.801644e-07), np.float32(0.99999934)], [np.float32(0.00011089611), np.float32(0.9998891)], [np.float32(7.249742e-05), np.float32(0.9999275)], [np.float32(0.95135003), np.float32(0.048649967)], [np.float32(2.4764908e-05), np.float32(0.99997526)], [np.float32(0.88931453), np.float32(0.11068547)], [np.float32(0.03752159), np.float32(0.9624784)], [np.float32(0.006887961), np.float32(0.993112)], [np.float32(6.2944814e-09), np.float32(1.0)], [np.float32(1.2563851e-06), np.float32(0.99999875)], [np.float32(0.79557407), np.float32(0.20442593)], [np.float32(3.3387558e-05), np.float32(0.9999666)], [np.float32(0.00015834106), np.float32(0.99984163)], [np.float32(0.00062004226), np.float32(0.99937993)], [np.float32(2.124147e-05), np.float32(0.9999788)], [np.float32(1.7925005e-07), np.float32(0.9999998)], [np.float32(0.6164818), np.float32(0.38351822)], [np.float32(0.012331227), np.float32(0.98766875)], [np.float32(1.2709422e-07), np.float32(0.9999999)], [np.float32(5.6768393e-08), np.float32(0.99999994)], [np.float32(0.07878928), np.float32(0.9212107)], [np.float32(0.23293246), np.float32(0.76706755)], [np.float32(0.0002268737), np.float32(0.99977314)], [np.float32(0.00018915442), np.float32(0.9998109)], [np.float32(3.7719015e-05), np.float32(0.9999623)], [np.float32(1.2562821e-06), np.float32(0.99999875)], [np.float32(3.695992e-06), np.float32(0.9999963)], [np.float32(4.0765426e-05), np.float32(0.99995923)], [np.float32(3.9410395e-07), np.float32(0.9999996)], [np.float32(0.0012694726), np.float32(0.99873054)], [np.float32(2.0234386e-10), np.float32(1.0)], [np.float32(3.913961e-05), np.float32(0.99996084)], [np.float32(6.0825283e-07), np.float32(0.9999994)], [np.float32(0.0113783525), np.float32(0.98862165)], [np.float32(0.0020330534), np.float32(0.99796695)], [np.float32(0.023870936), np.float32(0.97612906)], [np.float32(0.9979036), np.float32(0.0020964146)], [np.float32(0.0015250325), np.float32(0.99847496)], [np.float32(0.3433355), np.float32(0.6566645)], [np.float32(2.855991e-16), np.float32(1.0)], [np.float32(4.56645e-11), np.float32(1.0)], [np.float32(0.38282633), np.float32(0.6171737)], [np.float32(0.0005571237), np.float32(0.9994429)], [np.float32(2.4966536e-07), np.float32(0.99999976)], [np.float32(0.9682898), np.float32(0.031710207)], [np.float32(1.9905992e-09), np.float32(1.0)], [np.float32(1.96733e-12), np.float32(1.0)], [np.float32(4.186284e-09), np.float32(1.0)], [np.float32(0.0030120355), np.float32(0.99698794)], [np.float32(5.560776e-20), np.float32(1.0)], [np.float32(0.9310704), np.float32(0.06892961)], [np.float32(1.4751347e-06), np.float32(0.9999985)], [np.float32(0.00034504934), np.float32(0.99965495)], [np.float32(0.0060953605), np.float32(0.99390465)], [np.float32(2.701981e-05), np.float32(0.999973)], [np.float32(0.00020161149), np.float32(0.9997984)], [np.float32(5.0181218e-12), np.float32(1.0)], [np.float32(0.00070743094), np.float32(0.99929255)], [np.float32(4.883941e-08), np.float32(0.99999994)], [np.float32(0.013210887), np.float32(0.9867891)], [np.float32(2.4972242e-12), np.float32(1.0)]]\n",
      "Unseen set\n",
      "      ID        Dx         % Mel     % Nev\n",
      "0      0  Melanoma  9.999408e-01  0.000059\n",
      "1      1  Melanoma  9.998991e-01  0.000101\n",
      "2      2  Melanoma  9.999384e-01  0.000062\n",
      "3      3  Melanoma  9.997277e-01  0.000272\n",
      "4      4  Melanoma  1.565268e-01  0.843473\n",
      "..   ...       ...           ...       ...\n",
      "395  395     Nevus  5.018122e-12  1.000000\n",
      "396  396     Nevus  7.074309e-04  0.999293\n",
      "397  397     Nevus  4.883941e-08  1.000000\n",
      "398  398     Nevus  1.321089e-02  0.986789\n",
      "399  399     Nevus  2.497224e-12  1.000000\n",
      "\n",
      "[400 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "os.chdir(home_directory)\n",
    "os.chdir('DNNorDermatologist')\n",
    "\n",
    "# Get the test dataset of 400 - 200 nevi and 200 melanoma\n",
    "test_df = pd.read_pickle('NvAndMelNoDuplicatesFullSizeTestSet.zip')\n",
    "\n",
    "# Change the idx column to be '0' where the diagnosis of the lesion was\n",
    "# nevi, and '1' when the diagnosis is diagnosis\n",
    "test_df['idx'] = np.where(test_df['id'] == 'mel', 1 , 0)\n",
    "\n",
    "# Save a new table 'features' to be test_df, without the idx column\n",
    "features=test_df.drop(columns=['idx'], axis = 1)\n",
    "# Create a new table with just the correct diagnosis (0 for melanoma (or nevi), 1 for nevi (or melanoma))\n",
    "target=test_df['idx']\n",
    "\n",
    "# Change features to be a numpy array of image pixel data ((R, G, B))\n",
    "features = np.asarray(features['image'].tolist())\n",
    "\n",
    "# I want to resize the images \n",
    "features = np.array([cv2.resize(image, (224, 224)) for image in features])\n",
    "\n",
    "# Normalise this data in an alternate table to be values from 0 ... 1\n",
    "# e.g. 255 -> 1, 0 --> 0\n",
    "# Normalises for original prediction and evaluation of model, the SHAP funciton below requires non normalised data\n",
    "# TODO: Standarise this so SHAP takes normalised\n",
    "\n",
    "features2 = features / 255\n",
    "\n",
    "# Convert the data to one-hot encoding\n",
    "target_cat = to_categorical(target, num_classes = 2)\n",
    "\n",
    "# Get predictions for image data\n",
    "# e.g.\n",
    "# Index 0 : [0.9222, 0.0778]\n",
    "# Index 1 : [0.4500, 0.5500]\n",
    "# etc..\n",
    "# This represents likelihood of melanoma and nevi respectively (according to the model)\n",
    "y_pred = model.predict(features2, verbose=1)\n",
    "y_pred = [[value[0], 1-value[0]] for value in y_pred]\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "# Create a new dataframe with entries for each element of the test set\n",
    "# Include an ID, diagnosis, and % likelihoods for each diagnosis from the model\n",
    "df = pd.DataFrame(columns=['ID', 'Dx', '% Mel', '% Nev'],index=[i for i in range(400)])\n",
    "df['ID'] = df.index\n",
    "\n",
    "# Create dictionaries to contain actual diagnosis and probabilities from the model\n",
    "dx_d = {}\n",
    "Pmel = {}\n",
    "Pnev = {}\n",
    "# Take the actual diagnoses from where we retrieved them earlier\n",
    "y_test_cat = target_cat\n",
    "\n",
    "# For each element in the test set:\n",
    "for ind in range(400):\n",
    "    # Append the diagnosis and predictions to their respective dictionaries\n",
    "    if y_test_cat[ind][1] == 1.0:\n",
    "        diagnosis = 'Melanoma'\n",
    "    elif y_test_cat[ind][0] == 1.0:\n",
    "        diagnosis = 'Nevus'\n",
    "    dx_d[ind] = diagnosis\n",
    "    Pmel[ind] = y_pred[ind][0]\n",
    "    Pnev[ind] = y_pred[ind][1]\n",
    "    \n",
    "# Take the above dictionaries and insert them into the data frame\n",
    "df['Dx'] = df['ID'].map(dx_d)\n",
    "df['% Mel'] = df['ID'].map(Pmel)\n",
    "df['% Nev'] = df['ID'].map(Pnev)\n",
    "\n",
    "# Change the prediction likelihoods to be floats \n",
    "df = df.astype({\"% Mel\": float, \"% Nev\": float})\n",
    "\n",
    "#df = df.iloc[id_list]\n",
    "\n",
    "# Print the first 5 entries in the data frame\n",
    "print('Unseen set') \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want examine the results, so I will just save them\n",
    "df.to_csv(f'predictions_model_{seed}.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
